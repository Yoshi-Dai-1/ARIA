import io
import random
import re
import time
import unicodedata
import zipfile
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
import requests
from huggingface_hub import CommitOperationAdd, CommitOperationDelete, HfApi, hf_hub_download
from huggingface_hub.utils import EntryNotFoundError, HfHubHTTPError, RepositoryNotFoundError
from loguru import logger
from models import CatalogRecord, EdinetCodeRecord, StockMasterRecord
from utils import normalize_code


class CatalogManager:
    def __init__(self, hf_repo: str, hf_token: str, data_path: Path):
        self.hf_repo = hf_repo
        self.hf_token = hf_token
        self.data_path = data_path
        self.data_path.mkdir(parents=True, exist_ok=True)

        # ã€ä¿®æ­£ã€‘é€šä¿¡å®‰å®šæ€§å‘ä¸Šã®ãŸã‚ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
        # huggingface_hub v0.20+ / 1.x ã¯ç’°å¢ƒå¤‰æ•°ã‚’å‚ç…§ã—ã¦ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’åˆ¶å¾¡ã—ã¾ã™
        if hf_repo and hf_token:
            import os

            os.environ["HF_HUB_TIMEOUT"] = "300"
            os.environ["HF_HUB_HTTP_TIMEOUT"] = "300"
            self.api = HfApi(token=hf_token)
        else:
            self.api = None

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å®šç¾©
        self.paths = {
            "catalog": "catalog/documents_index.parquet",
            "master": "meta/stocks_master.parquet",
            "listing": "meta/listing_history.parquet",
            "index": "meta/index_history.parquet",
            "name": "meta/name_history.parquet",
        }

        self.catalog_df = self._load_parquet("catalog")
        self.master_df = self._load_parquet("master")

        # ã€æœ€é‡è¦ã€‘ä¸€æ‹¬ã‚³ãƒŸãƒƒãƒˆç”¨ãƒãƒƒãƒ•ã‚¡
        self._commit_operations = {}
        self._snapshots = {}  # æ•´åˆæ€§ä¿è­·ã®ãŸã‚ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        logger.info("CatalogManager ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")

        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã¨æœ€æ–°ã‚¹ã‚­ãƒ¼ãƒã¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
        self._retrospective_cleanse()

        # ã€è¿½åŠ ã€‘èµ·å‹•æ™‚ã«EDINETã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’åŒæœŸ (å’Œè‹±å”åŒ + é›†ç´„ä¸€è¦§)
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã§åœæ­¢ã—ãªã„ã‚ˆã†ã€å†…éƒ¨ã§ä¾‹å¤–å‡¦ç†
        self.edinet_codes, self.aggregation_map = self.sync_edinet_code_lists()

        # ã€è¿½åŠ ã€‘åŒæœŸã—ãŸã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’ãƒã‚¹ã‚¿ã«åæ˜ 
        if self.edinet_codes:
            self._update_master_from_edinet_codes()

    def _update_master_from_edinet_codes(self):
        """åŒæœŸã—ãŸ edinet_codes ãŠã‚ˆã³ aggregation_map ã‚’ master_df ã«åæ˜ ã•ã›ã€å±æ€§ã‚’æœ€æ–°åŒ–ã™ã‚‹"""
        from datetime import datetime

        logger.info("EDINETã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’ãƒã‚¹ã‚¿ã«åæ˜ ä¸­ (é›†ç´„ãƒ–ãƒªãƒƒã‚¸ + JCNå¤‰æ›´æ¤œçŸ¥ + ä¸Šå ´ç”Ÿæ­»åˆ¤å®š)...")
        updated_count = 0
        listing_events = []
        today = datetime.now().strftime("%Y-%m-%d")

        # æ—¢å­˜ãƒã‚¹ã‚¿ã‚’ edinet_code ã‚’ã‚­ãƒ¼ã«ã—ãŸè¾æ›¸ã«å¤‰æ› (é«˜é€ŸåŒ–ç”¨)
        master_dict = {
            str(row["edinet_code"]): row.to_dict()
            for _, row in self.master_df.iterrows()
            if pd.notna(row.get("edinet_code"))
        }

        # ã€é›†ç´„ãƒ–ãƒªãƒƒã‚¸ã€‘æ—§ã‚³ãƒ¼ãƒ‰â†’æ–°ã‚³ãƒ¼ãƒ‰ã®ä»˜ã‘æ›¿ãˆã‚’é©ç”¨
        for old_code, new_code in self.aggregation_map.items():
            if new_code in master_dict:
                existing_former = master_dict[new_code].get("former_edinet_codes") or ""
                former_set = set(existing_former.split(",")) if existing_former else set()
                former_set.discard("")
                former_set.add(old_code)
                master_dict[new_code]["former_edinet_codes"] = ",".join(sorted(former_set))
                logger.debug(f"é›†ç´„ãƒ–ãƒªãƒƒã‚¸é©ç”¨: {old_code} â†’ {new_code} (æ—§ã‚³ãƒ¼ãƒ‰ã‚’ãƒªãƒ³ã‚¯)")

        for e_code, ed_rec in self.edinet_codes.items():
            # ã€æœ€é©åŒ–ã€‘ä¸Šå ´åˆ¤å®š: é‡‘èåºãƒªã‚¹ãƒˆã®ã€Œä¸Šå ´åŒºåˆ†ã€ãŒå®Œå…¨ã« "ä¸Šå ´" ã§ã‚ã‚‹å ´åˆã®ã¿
            is_listed_official = str(ed_rec.is_listed or "").strip() == "ä¸Šå ´"

            if e_code in master_dict:
                # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã®æ›´æ–°
                m_rec = master_dict[e_code]

                # ã€JCNå¤‰æ›´æ¤œçŸ¥ã€‘
                old_jcn = m_rec.get("jcn")
                new_jcn = ed_rec.jcn
                if old_jcn and new_jcn and str(old_jcn) != str(new_jcn):
                    logger.warning(
                        f"âš ï¸ JCNå¤‰æ›´æ¤œçŸ¥: {e_code} ({ed_rec.submitter_name}) æ—§JCN={old_jcn} â†’ æ–°JCN={new_jcn}"
                    )

                # ã€ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¤ãƒ™ãƒ³ãƒˆç”Ÿæˆ (ç”Ÿæ­»åˆ¤å®š)ã€‘
                old_is_active = bool(m_rec.get("is_active", False))
                sec_code = ed_rec.sec_code or m_rec.get("code")
                if sec_code:
                    if old_is_active is False and is_listed_official is True:
                        listing_events.append({"code": sec_code, "type": "LISTING", "event_date": today})
                        logger.info(f"ğŸŸ¢ æ–°è¦ä¸Šå ´/å†ä¸Šå ´æ¤œçŸ¥: {sec_code} ({ed_rec.submitter_name})")
                    elif old_is_active is True and is_listed_official is False:
                        listing_events.append({"code": sec_code, "type": "DELISTING", "event_date": today})
                        logger.info(f"ğŸ”´ ä¸Šå ´å»ƒæ­¢æ¤œçŸ¥: {sec_code} ({ed_rec.submitter_name})")

                # å¤‰æ›´ãŒã‚ã‚‹å ´åˆã®ã¿æ›´æ–° (èª å®ŸãªåŒæœŸ)
                updates = {
                    "jcn": ed_rec.jcn or m_rec.get("jcn"),
                    "code": sec_code,
                    "company_name": ed_rec.submitter_name,
                    "company_name_en": ed_rec.submitter_name_en or m_rec.get("company_name_en"),
                    "industry_edinet": ed_rec.industry_edinet,
                    "industry_edinet_en": ed_rec.industry_edinet_en or m_rec.get("industry_edinet_en"),
                    "is_listed_edinet": is_listed_official,
                    "is_active": is_listed_official,  # EDINETã®å®Œå…¨ç§»è­²
                }

                changed = False
                for k, v in updates.items():
                    if m_rec.get(k) != v:
                        m_rec[k] = v
                        changed = True

                if changed:
                    master_dict[e_code] = m_rec
                    updated_count += 1
            else:
                # æ–°è¦ãƒ¬ã‚³ãƒ¼ãƒ‰ã®è¿½åŠ 
                sec_code = ed_rec.sec_code
                if sec_code and is_listed_official:
                    listing_events.append({"code": sec_code, "type": "LISTING", "event_date": today})

                new_master_rec = StockMasterRecord(
                    edinet_code=e_code,
                    code=sec_code,
                    jcn=ed_rec.jcn,
                    company_name=ed_rec.submitter_name,
                    company_name_en=ed_rec.submitter_name_en,
                    industry_edinet=ed_rec.industry_edinet,
                    industry_edinet_en=ed_rec.industry_edinet_en,
                    is_listed_edinet=is_listed_official,
                    is_active=is_listed_official,  # EDINETã®å®Œå…¨ç§»è­²
                )
                master_dict[e_code] = new_master_rec.model_dump()
                updated_count += 1

        if updated_count > 0:
            self.master_df = pd.DataFrame(list(master_dict.values()))
            self.master_df = self._clean_dataframe("master", self.master_df)
            logger.success(f"ãƒã‚¹ã‚¿åŒæœŸå®Œäº†: {updated_count} ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°/è¿½åŠ ã—ã¾ã—ãŸã€‚")
            self._save_and_upload("master", self.master_df, defer=True)

        if listing_events:
            events_df = pd.DataFrame(listing_events)
            self.update_listing_history(events_df)
            logger.success(f"ä¸Šå ´å±¥æ­´åŒæœŸå®Œäº†: {len(events_df)} ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’è¿½åŠ äºˆç´„ã—ã¾ã—ãŸã€‚")

    def sync_edinet_code_lists(self) -> Tuple[Dict[str, EdinetCodeRecord], Dict[str, str]]:
        """é‡‘èåºã‹ã‚‰å’Œè‹±ä¸¡æ–¹ã®ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆãŠã‚ˆã³é›†ç´„ä¸€è¦§ã‚’å–å¾—ã—ã€å”åŒã—ã¦ãƒã‚¹ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹"""
        urls = {
            "jp": "https://disclosure2dl.edinet-fsa.go.jp/searchdocument/codelist/Edinetcode.zip",
            "en": "https://disclosure2dl.edinet-fsa.go.jp/searchdocument/codelisteng/Edinetcode.zip",
            "agg": "https://disclosure2dl.edinet-fsa.go.jp/guide/static/disclosure/download/ESE140190.csv",
        }

        results = {}
        agg_map = {}  # æ—§ã‚³ãƒ¼ãƒ‰ -> æ–°ã‚³ãƒ¼ãƒ‰
        try:
            logger.info("EDINETã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ (å’Œè‹±) ã®åŒæœŸã‚’é–‹å§‹...")

            # æ—¥æœ¬èªç‰ˆã®å–å¾—ã¨è§£æ
            res_jp = requests.get(urls["jp"], timeout=30)
            res_jp.raise_for_status()
            with zipfile.ZipFile(io.BytesIO(res_jp.content)) as z:
                csv_file = [f for f in z.namelist() if f.endswith(".csv")][0]
                df_jp = pd.read_csv(z.open(csv_file), encoding="cp932", skiprows=1)

            # è‹±èªç‰ˆã®å–å¾—ã¨è§£æ (æ¥­ç¨®ç¿»è¨³ã®æŠ½å‡ºç”¨)
            res_en = requests.get(urls["en"], timeout=30)
            res_en.raise_for_status()
            with zipfile.ZipFile(io.BytesIO(res_en.content)) as z:
                csv_file = [f for f in z.namelist() if f.endswith(".csv")][0]
                df_en = pd.read_csv(z.open(csv_file), encoding="cp932", skiprows=1)

            # ã€é‡è¦ã€‘é›†ç´„ä¸€è¦§ (ESE140190.csv) ã®å–å¾—ã¨è§£æ (ã‚³ãƒ¼ãƒ‰ä»˜ã‘æ›¿ãˆå¯¾å¿œ)
            # ç‰©ç†æ¤œè¨¼çµæœ: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯ CP932ã€1è¡Œç›®ã¯ã‚¿ã‚¤ãƒˆãƒ«è¡Œ
            try:
                res_agg = requests.get(urls["agg"], timeout=30)
                res_agg.raise_for_status()
                # CSV èª­ã¿è¾¼ã¿: [é›†ç´„å‡¦ç†æ—¥, å»ƒæ­¢EDINETã‚³ãƒ¼ãƒ‰, ç¶™ç¶šEDINETã‚³ãƒ¼ãƒ‰] å½¢å¼ã‚’æƒ³å®š
                # 1è¡Œç›®ãŒã€ŒEDINETã‚³ãƒ¼ãƒ‰é›†ç´„ä¸€è¦§,,ã€ã®ãŸã‚ skiprows=1
                df_agg = pd.read_csv(io.BytesIO(res_agg.content), encoding="cp932", skiprows=1)
                for _, agg_row in df_agg.iterrows():
                    # 0:å‡¦ç†æ—¥, 1:å»ƒæ­¢ã‚³ãƒ¼ãƒ‰, 2:ç¶™ç¶šã‚³ãƒ¼ãƒ‰
                    old_c = str(agg_row.iloc[1]).strip()
                    new_c = str(agg_row.iloc[2]).strip()
                    if old_c and new_c and old_c != new_c:
                        agg_map[old_c] = new_c
                logger.info(f"EDINETã‚³ãƒ¼ãƒ‰é›†ç´„ä¸€è¦§ã‚’ãƒ­ãƒ¼ãƒ‰: {len(agg_map)} ä»¶ã®ä»˜ã‘æ›¿ãˆã‚’ç‰¹å®š")
            except Exception as ae:
                logger.warning(f"é›†ç´„ä¸€è¦§ã®å–å¾—ãƒ»è§£æã«å¤±æ•—ã—ã¾ã—ãŸ (ç¶™ç¶šå¯èƒ½): {ae}")

            # åå¯„ã›: EDINETã‚³ãƒ¼ãƒ‰ã‚’ã‚­ãƒ¼ã«ã™ã‚‹
            # æ—¥æœ¬èªç‰ˆã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã€è‹±èªç‰ˆã‹ã‚‰æ¥­ç¨®åã‚’è£œå®Œ
            for _, row in df_jp.iterrows():
                e_code = str(row["ï¼¥ï¼¤ï¼©ï¼®ï¼¥ï¼´ã‚³ãƒ¼ãƒ‰"])

                # è‹±èªç‰ˆã‹ã‚‰å¯¾å¿œã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢
                en_row = df_en[df_en.iloc[:, 0] == e_code]
                ind_en = en_row.iloc[0]["Submitter's industry"] if not en_row.empty else None

                # æ•°å€¤å‹ã®å¯èƒ½æ€§ãŒã‚ã‚‹ã‚«ãƒ©ãƒ ã‚’å®‰å…¨ã«æ–‡å­—åˆ—åŒ– (2024.0 å›é¿)
                def safe_int_str(val):
                    if pd.isna(val):
                        return None
                    try:
                        return str(int(float(val)))
                    except Exception:
                        return str(val)

                res_dict = {
                    "edinet_code": e_code,
                    "submitter_type": row.get("æå‡ºè€…ç¨®åˆ¥"),
                    "is_listed": row.get("ä¸Šå ´åŒºåˆ†"),
                    "is_consolidated": row.get("é€£çµã®æœ‰ç„¡"),
                    "capital": float(row["è³‡æœ¬é‡‘"]) if pd.notna(row.get("è³‡æœ¬é‡‘")) else None,
                    "settlement_date": str(row.get("æ±ºç®—æ—¥")),
                    "submitter_name": str(row.get("æå‡ºè€…å")),
                    "submitter_name_en": str(row.get("æå‡ºè€…åï¼ˆè‹±å­—ï¼‰")),
                    "submitter_name_kana": str(row.get("æå‡ºè€…åï¼ˆãƒ¨ãƒŸï¼‰")),
                    "address": str(row.get("æ‰€åœ¨åœ°")),
                    "industry_edinet": str(row.get("æå‡ºè€…æ¥­ç¨®")),
                    "industry_edinet_en": ind_en,
                    "sec_code": normalize_code(str(row["è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰"]))
                    if pd.notna(row.get("è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰")) and str(row["è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰"]).strip()
                    else None,
                    "jcn": safe_int_str(row.get("æå‡ºè€…æ³•äººç•ªå·")),
                }
                results[e_code] = EdinetCodeRecord(**res_dict)

            logger.success(f"EDINETã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆåŒæœŸå®Œäº†: {len(results)} ä»¶")

        except Exception as e:
            logger.error(f"EDINETã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã®åŒæœŸã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            # å¤±æ•—ã—ãŸå ´åˆã¯æ—¢å­˜ã® master_df ã‹ã‚‰æœ€å°é™ã®æƒ…å ±ã‚’å¾©å…ƒã™ã‚‹ã“ã¨ã‚’æ¤œè¨

        return results, agg_map

    def _retrospective_cleanse(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èµ°æŸ»ã—ã€ä¸å‚™ãŒã‚ã‚Œã°è‡ªå‹•ä¿®æ­£ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        if not self.api:
            return

        logger.info("Starting integrity check for all Parquet files...")
        updated_count = 0

        # 1. å®šç¾©æ¸ˆã¿ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯
        for key in self.paths.keys():
            try:
                # æ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã® catalog_df, master_df ã¯ _load_parquet ã§ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿
                df = self.catalog_df if key == "catalog" else (self.master_df if key == "master" else None)
                if df is None:
                    df = self._load_parquet(key)

                # ã‚«ã‚¿ãƒ­ã‚°ã®å ´åˆã€18ã‚«ãƒ©ãƒ æœªæº€ãªã‚‰å¼·åˆ¶ä¿å­˜ã—ã¦ã‚¹ã‚­ãƒ¼ãƒæ‹¡å¼µ
                if key == "catalog" and len(df.columns) < 18:
                    self._save_and_upload(key, df, defer=True)
                    updated_count += 1
            except Exception:
                continue

        # 2. ãƒã‚¹ã‚¿ãƒ¼ã®å…¨Binãƒ•ã‚¡ã‚¤ãƒ«ã‚’èµ°æŸ»
        try:
            files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type="dataset")
            bin_files = [f for f in files if "master/bin/" in f and f.endswith(".parquet")]

            for b_file in bin_files:
                local_tmp = self.data_path / "temp_cleanse.parquet"
                self.api.hf_hub_download(
                    repo_id=self.hf_repo,
                    filename=b_file,
                    repo_type="dataset",
                    token=self.hf_token,
                    local_dir=str(self.data_path),
                    local_dir_use_symlinks=False,
                )
                df_bin = pd.read_parquet(self.data_path / b_file)

                # ã‚¹ã‚­ãƒ¼ãƒä¸é©åˆãŒã‚ã‚Œã°ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã—ã¦äºˆç´„
                # (å…·ä½“çš„ãª rec ãƒã‚§ãƒƒã‚¯ã§ã¯ãªãã€ãƒ¢ãƒ‡ãƒ«ã¨ã®ä¸ä¸€è‡´ã‚’åŸºæº–ã«ã™ã‚‹)
                df_clean = self._clean_dataframe("master", df_bin)
                if len(df_clean.columns) != len(df_bin.columns):
                    logger.info(f"Cleaned up bin file schema: {b_file}")
                    df_clean.to_parquet(local_tmp, index=False, compression="zstd")
                    self.add_commit_operation(b_file, local_tmp)
                    updated_count += 1
        except Exception:
            pass

    def _clean_dataframe(self, key: str, df: pd.DataFrame) -> pd.DataFrame:
        """å…¨ã¦ã®DataFrameã«å¯¾ã—ã¦å…±é€šã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚’é©ç”¨"""
        if df.empty:
            return df

        # 0. ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–ï¼ˆç©ºç™½é™¤å»ï¼‰
        df.columns = df.columns.astype(str).str.strip()

        # ã€è¿½åŠ ã€‘å…¨æ–‡å­—åˆ—ã‚«ãƒ©ãƒ ã®ç©ºæ–‡å­—ã‚’æ˜ç¤ºçš„ã« None (NULL) ã«çµ±ä¸€
        for col in df.columns:
            if df[col].dtype == "object":
                # ç©ºç™½ã®ã¿ã®æ–‡å­—åˆ—ã‚‚ NULL æ‰±ã„ã¨ã™ã‚‹
                df[col] = df[col].apply(lambda x: None if (isinstance(x, str) and not x.strip()) else x)

        # 1. ä¸è¦ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”±æ¥ã‚«ãƒ©ãƒ ã®é™¤å»
        drop_targets = ["index", "level_0", "Unnamed: 0"]
        cols_to_drop = [c for c in drop_targets if c in df.columns]

        if cols_to_drop:
            logger.debug(f"{key}: Removed unnecessary columns: {cols_to_drop}")
            df = df.drop(columns=cols_to_drop)

        # 2. ã‚«ã‚¿ãƒ­ã‚°ã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«å®šç¾©ã®ã‚«ãƒ©ãƒ æ§‹æˆã‚’å¼·åˆ¶ (ç¾åœ¨ã¯27ã‚«ãƒ©ãƒ ã«æ‹¡å¼µ)
        if key == "catalog":
            # NaN ã‚’ None ã«ç½®æ›
            df = df.replace({pd.NA: None, float("nan"): None})

            # ãƒ¢ãƒ‡ãƒ«å®šç¾©ã®å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—
            expected_cols = list(CatalogRecord.model_fields.keys())

            # æ—¢å­˜ã®ã‚«ãƒ©ãƒ ã®ã¿ã§Pydanticãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šã—ã€ä¸è¶³åˆ†ã‚’Noneã§è£œå®Œ
            validated = []
            for rec_dict in df.to_dict("records"):
                try:
                    # æ¬ è½ã—ã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã£ã¦ã‚‚ Pydantic ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è£œå®Œ
                    validated.append(CatalogRecord(**rec_dict).model_dump())
                except Exception as e:
                    # å¿…é ˆé …ç›®(doc_idç­‰)ãŒæ¬ ã‘ã¦ã„ã‚‹å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼
                    logger.warning(f"ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ä¸­ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ä¸å‚™ (doc_id: {rec_dict.get('doc_id')}): {e}")
                    # æ§‹é€ ã ã‘ã§ã‚‚ç¶­æŒã™ã‚‹ãŸã‚ã€è¾æ›¸ã¨ã—ã¦å¯èƒ½ãªé™ã‚Šä¿æŒ
                    row = {col: rec_dict.get(col) for col in expected_cols}
                    validated.append(row)

            df = pd.DataFrame(validated)
            # ã‚«ãƒ©ãƒ é †ã‚’ãƒ¢ãƒ‡ãƒ«å®šç¾©ã«åˆã‚ã›ã‚‹
            df = df[expected_cols]

            # ã€é‡è¦ã€‘ãƒ‡ãƒ¼ã‚¿å‹ã®æ­£è¦åŒ– (2024.0 å›é¿ã®ãŸã‚ã® Int64 é©ç”¨)
            # pandas ã®æµ®å‹•å°æ•°ç‚¹åŒ–ã‚’é˜»æ­¢ã—ã€æ•´æ•°ã¾ãŸã¯ NULL ã¨ã—ã¦ä¿å­˜
            if "fiscal_year" in df.columns:
                df["fiscal_year"] = pd.to_numeric(df["fiscal_year"], errors="coerce").astype("Int64")
            if "num_months" in df.columns:
                df["num_months"] = pd.to_numeric(df["num_months"], errors="coerce").astype("Int64")

        # 3. è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã®æ­£è¦åŒ– (5æ¡çµ±ä¸€)
        targets = ["master", "listing", "index", "name"]
        if key in targets and "code" in df.columns:
            df["code"] = df["code"].apply(normalize_code)

        # 4. Objectå‹ã®å®‰å®šåŒ– (None ã‚’ä¿æŒã—ã¤ã¤æ–‡å­—åˆ—åŒ–)
        for col in df.columns:
            if df[col].dtype == "object":
                # ã€æœ€é‡è¦ã€‘è«–ç†å€¤ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯æ–‡å­—åˆ—åŒ–ã‚’å›é¿
                # æ—¢ã« 'True' / 'False'ï¼ˆæ–‡å­—åˆ—ï¼‰ã«ãªã£ã¦ã—ã¾ã£ã¦ã„ã‚‹å ´åˆã®å¾©æ—§å‡¦ç½®ã‚‚å…¼ã­ã‚‹
                has_string_bools = df[col].isin(["True", "False"]).any()
                if has_string_bools:
                    # æ–‡å­—åˆ—ã® 'True'/'False' ã‚’æ­£è¦ã® Boolean ã«æˆ»ã™ (None ã¯ç¶­æŒ)
                    df[col] = df[col].map({"True": True, "False": False, True: True, False: False}, na_action="ignore")

                # æ”¹ã‚ã¦ãƒã‚§ãƒƒã‚¯ã—ã€ç´”ç²‹ãªæ–‡å­—åˆ—ã‚«ãƒ©ãƒ ã®ã¿ã‚’ as_type(str) ç›¸å½“ã®å‡¦ç†ã«ã‹ã‘ã‚‹
                is_pure_bool = df[col].isin([True, False]).any()
                if not is_pure_bool:
                    df[col] = df[col].apply(lambda x: str(x) if (x is not None and not pd.isna(x)) else None)

        return df

    def _normalize_company_name(self, name: str) -> str:
        """æ¯”è¼ƒåˆ¤å®šã®ãŸã‚ã«æ³•äººæ ¼ã‚„ç©ºç™½ã‚’é™¤å»ã—ã¦æ­£è¦åŒ–ã™ã‚‹ (NFKCå¯¾å¿œç‰ˆ)"""
        if not name or not isinstance(name, str):
            return ""

        # 1. NFKCæ­£è¦åŒ– (å…¨è§’æ•°å­—ãƒ»è‹±å­—ã‚’åŠè§’ã«ã€ãˆ± ãªã©ã‚’ (æ ª) ã«åˆ†è§£)
        n = unicodedata.normalize("NFKC", name)

        # 2. å…¨ã¦ã®ç©ºç™½é™¤å»
        n = n.replace(" ", "").replace("\u3000", "")

        # 3. ä»£è¡¨çš„ãªæ³•äººæ ¼è¡¨è¨˜ã‚’é™¤å»
        # NFKCå¾Œã® (æ ª) ã‚„ (æœ‰) ãªã©ã«å¯¾å¿œã§ãã‚‹ã‚ˆã†ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ•´ç†
        patterns = [
            r"æ ªå¼ä¼šç¤¾",
            r"æœ‰é™ä¼šç¤¾",
            r"åˆåŒä¼šç¤¾",
            r"åˆè³‡ä¼šç¤¾",
            r"åˆåä¼šç¤¾",
            r"ä¸€èˆ¬ç¤¾å›£æ³•äºº",
            r"ä¸€èˆ¬è²¡å›£æ³•äºº",
            r"å…¬ç›Šç¤¾å›£æ³•äºº",
            r"å…¬ç›Šè²¡å›£æ³•äºº",
            r"\(æ ª\)",
            r"\(æœ‰\)",
            r"\(åˆ\)",
            r"\(ç¤¾\)",
            r"\(è²¡\)",
        ]
        for p in patterns:
            n = re.sub(p, "", n)

        return n.strip()

    def add_commit_operation(self, repo_path: str, local_path: Path):
        """ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«æ“ä½œã‚’è¿½åŠ ï¼ˆé‡è¤‡ã¯æœ€æ–°ã§ä¸Šæ›¸ãï¼‰"""
        self._commit_operations[repo_path] = CommitOperationAdd(path_in_repo=repo_path, path_or_fileobj=str(local_path))
        logger.debug(f"ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {repo_path}")

    def take_snapshot(self):
        """ç¾åœ¨ã®GlobalçŠ¶æ…‹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ãƒ¡ãƒ¢ãƒªã«å–å¾— (ä¸æ•´åˆç™ºç”Ÿæ™‚ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨)"""
        # ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«ä¿å­˜
        self._snapshots = {
            "catalog": self.catalog_df.copy(),
            "master": self.master_df.copy(),
            "listing": self._load_parquet("listing").copy(),
            "index": self._load_parquet("index").copy(),
            "name": self._load_parquet("name").copy(),
        }
        logger.info("Global çŠ¶æ…‹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—ã—ã¾ã—ãŸ (å®‰å…¨æ€§ç¢ºä¿)")

    def rollback(self, message: str = "RaW-V Failure: Automated Recovery Rollback"):
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®çŠ¶æ…‹ã‚’å¼·åˆ¶çš„ã«æ›¸ãæˆ»ã—ã€Globalãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’å¾©æ—§ã™ã‚‹"""
        if not self._snapshots:
            logger.error("âŒ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ãã¾ã›ã‚“ã€‚")
            return False

        logger.warning(f"â›” ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’é–‹å§‹ã—ã¾ã™: {message}")

        # æ—¢å­˜ã®ã‚³ãƒŸãƒƒãƒˆäºˆç´„ã‚’ã™ã¹ã¦ç ´æ£„
        self._commit_operations = {}

        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®å†…å®¹ã‚’å¼·åˆ¶çš„ã«ä¸Šæ›¸ãäºˆç´„
        for key, df in self._snapshots.items():
            self._save_and_upload(key, df, defer=True)

        # ä¸€æ‹¬ã‚³ãƒŸãƒƒãƒˆã®å®Ÿè¡Œ (äº‹å®Ÿä¸Šã®å·®ã—æˆ»ã—)
        success = self.push_commit(f"ROLLBACK: {message}")
        if success:
            logger.success("âœ… ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ»ã‚³ãƒŸãƒƒãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚æ•´åˆæ€§ã¯å¾©æ—§ã•ã‚Œã¾ã—ãŸã€‚")
            # ãƒ¡ãƒ¢ãƒªä¸Šã®æœ€æ–°çŠ¶æ…‹ã‚‚ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«æˆ»ã™
            self.catalog_df = self._snapshots["catalog"]
            self.master_df = self._snapshots["master"]
        else:
            logger.critical(
                "âŒ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯è‡ªä½“ã«å¤±æ•—ã—ã¾ã—ãŸï¼"
                "Hugging Faceä¸Šã®ãƒ‡ãƒ¼ã‚¿ãŒå£Šã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ç›´ã¡ã«æ‰‹å‹•ç¢ºèªãŒå¿…è¦ã§ã™ã€‚"
            )
        return success

    def _load_parquet(self, key: str, force_download: bool = False) -> pd.DataFrame:
        filename = self.paths[key]
        try:
            local_path = hf_hub_download(
                repo_id=self.hf_repo,
                filename=filename,
                repo_type="dataset",
                token=self.hf_token,
                force_download=force_download,
            )
            df = pd.read_parquet(local_path)
            # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘èª­ã¿è¾¼ã¿ç›´å¾Œã«ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
            df = self._clean_dataframe(key, df)
            logger.debug(f"ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {filename} ({len(df)} rows)")
            return df
        except RepositoryNotFoundError:
            logger.error(f"ãƒªãƒã‚¸ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.hf_repo}")
            logger.error("ç’°å¢ƒå¤‰æ•° HF_REPO ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            raise
        except (EntryNotFoundError, requests.exceptions.HTTPError) as e:
            # EntryNotFoundError (HFãƒ©ã‚¤ãƒ–ãƒ©ãƒª) ã¾ãŸã¯ ç”Ÿã® 404 (ãƒ‘ãƒƒãƒé©ç”¨æ™‚) ã‚’ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            is_404 = isinstance(e, EntryNotFoundError) or (
                hasattr(e, "response") and e.response is not None and e.response.status_code == 404
            )

            if not is_404:
                # 404 ä»¥å¤–ãªã‚‰ä¸Šä½ã¾ãŸã¯ Exception ã¸é£›ã°ã™
                raise e

            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚æ–°è¦ä½œæˆã—ã¾ã™: {filename}")
            if key == "catalog":
                cols = list(CatalogRecord.model_fields.keys())
                return pd.DataFrame(columns=cols)
            elif key == "master":
                cols = list(StockMasterRecord.model_fields.keys())
                return pd.DataFrame(columns=cols)
            elif key == "listing":
                return pd.DataFrame(columns=["code", "type", "event_date"])
            elif key == "index":
                return pd.DataFrame(columns=["index_name", "code", "type", "event_date"])
            elif key == "name":
                return pd.DataFrame(columns=["code", "old_name", "new_name", "change_date"])
            return pd.DataFrame()
        except HfHubHTTPError as e:
            logger.error(f"HF API ã‚¨ãƒ©ãƒ¼ ({e.response.status_code}): {filename}")
            logger.error(f"è©³ç´°: {e}")
            if e.response.status_code == 401:
                logger.error("èªè¨¼ã‚¨ãƒ©ãƒ¼: HF_TOKEN ãŒç„¡åŠ¹ã¾ãŸã¯æœŸé™åˆ‡ã‚Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            elif e.response.status_code == 403:
                logger.error("ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦: ãƒªãƒã‚¸ãƒˆãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“")
            raise
        except Exception as e:
            logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {filename} - {type(e).__name__}: {e}")
            raise

    def is_processed(self, doc_id: str) -> bool:
        if self.catalog_df.empty:
            return False
        # doc_id ãŒå­˜åœ¨ã—ã€ã‹ã¤ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ 'success' ã¾ãŸã¯ 'retracted' (å–ä¸‹æ¸ˆ) ã®å ´åˆã®ã¿ã€Œå‡¦ç†æ¸ˆã¿ã€ã¨ã¿ãªã™
        # ã“ã‚Œã«ã‚ˆã‚Šã€pending ã‚„ failure ã®æ›¸é¡ã¯è‡ªå‹•çš„ã«å†å‡¦ç†ã®å¯¾è±¡ã«ãªã‚‹
        # retracted ã®æ›¸é¡ã¯å†é€ã—ã¦ã‚‚ç„¡æ„å‘³ãªãŸã‚ã€å‡¦ç†æ¸ˆã¿ã¨ã—ã¦æ‰±ã†
        processed = self.catalog_df[
            (self.catalog_df["doc_id"] == doc_id) & (self.catalog_df["processed_status"].isin(["success", "retracted"]))
        ]
        return not processed.empty

    def get_status(self, doc_id: str) -> Optional[str]:
        """æŒ‡å®šã—ãŸ doc_id ã®ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        if self.catalog_df.empty:
            return None
        match = self.catalog_df[self.catalog_df["doc_id"] == doc_id]
        if match.empty:
            return None
        return match.iloc[0]["processed_status"]

    def update_catalog(self, new_records: List[Dict]) -> bool:
        """ã‚«ã‚¿ãƒ­ã‚°ã‚’æ›´æ–° (Pydanticãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿæ–½)"""
        if not new_records:
            return True

        validated = []
        for rec in new_records:
            try:
                validated.append(CatalogRecord(**rec).model_dump())
            except Exception as e:
                logger.error(f"ã‚«ã‚¿ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•— (doc_id: {rec.get('doc_id')}): {e}")

        if not validated:
            return False

        new_df = pd.DataFrame(validated)

        # ã€ä¿®æ­£ã€‘ä¸€æ™‚çš„ã«çµåˆã—ãŸDataFrameã‚’ä½œæˆï¼ˆãƒ¡ãƒ¢ãƒªä¸Šã®çŠ¶æ…‹ã¯å¤‰æ›´ã—ãªã„ï¼‰
        temp_catalog = pd.concat([self.catalog_df, new_df], ignore_index=True).drop_duplicates(
            subset=["doc_id"], keep="last"
        )

        # ã€ä¿®æ­£ã€‘ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸæ™‚ã®ã¿ã€ãƒ¡ãƒ¢ãƒªä¸Šã®ã‚«ã‚¿ãƒ­ã‚°ã‚’æ›´æ–°
        if self._save_and_upload("catalog", temp_catalog):
            self.catalog_df = temp_catalog
            logger.success(f"âœ… ã‚«ã‚¿ãƒ­ã‚°æ›´æ–°æˆåŠŸ: {len(validated)} ä»¶")
            return True
        else:
            logger.error("ã‚«ã‚¿ãƒ­ã‚°ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ¡ãƒ¢ãƒªä¸Šã®çŠ¶æ…‹ã‚’ä¿æŒã—ã¾ã™")
            return False

    def _save_and_upload(self, key: str, df: pd.DataFrame, defer: bool = False) -> bool:
        filename = self.paths[key]
        local_file = self.data_path / Path(filename).name

        # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘ä¿å­˜ç›´å‰ã«æœ€çµ‚ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        df = self._clean_dataframe(key, df)

        df.to_parquet(local_file, index=False, compression="zstd")

        if self.api:
            if defer:
                # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ ã—ã¦çµ‚äº† (ãƒ‘ã‚¹ã‚’ã‚­ãƒ¼ã«ã—ã¦æœ€æ–°ã®ã‚‚ã®ã§ä¸Šæ›¸ã)
                self.add_commit_operation(filename, local_file)
                return True

            max_retries = 5  # å¼·åŒ–
            for attempt in range(max_retries):
                try:
                    self.api.upload_file(
                        path_or_fileobj=str(local_file),
                        path_in_repo=filename,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.success(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {filename}")
                    return True
                except Exception as e:
                    # HfHubHTTPErrorã®å‹ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã€429ã®å ´åˆã®ã¿ãƒªãƒˆãƒ©ã‚¤
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(f"Rate limit exceeded. Waiting {wait_time}s before retry ({attempt + 1}/5)...")
                        time.sleep(wait_time)
                        continue

                    # ãã®ä»–ã®HTTPã‚¨ãƒ©ãƒ¼ (5xxç­‰) ã‚‚ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã«ã™ã‚‹
                    if isinstance(e, HfHubHTTPError) and e.response.status_code >= 500:
                        wait_time = 15 * (attempt + 1)
                        logger.warning(
                            f"Master HF Server Error ({e.response.status_code}). "
                            f"Waiting {wait_time}s... ({attempt + 1}/5)"
                        )
                        time.sleep(wait_time)
                        continue

                    logger.warning(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {filename} - {e} - Retrying ({attempt + 1}/5)...")
                    time.sleep(10 * (attempt + 1))
            logger.error(f"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«æœ€çµ‚çš„ã«å¤±æ•—ã—ã¾ã—ãŸ: {filename}")
            return False
        return True

    def upload_raw(self, local_path: Path, repo_path: str, defer: bool = False) -> bool:
        """ãƒ­ãƒ¼ã‚«ãƒ«ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ Hugging Face ã® raw/ ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        if not local_path.exists():
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“: {local_path}")
            return False

        if self.api:
            if defer:
                self.add_commit_operation(repo_path, local_path)
                logger.debug(f"RAWã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {repo_path}")
                return True

            max_retries = 5  # å¼·åŒ–
            for attempt in range(max_retries):
                try:
                    self.api.upload_file(
                        path_or_fileobj=str(local_path),
                        path_in_repo=repo_path,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.debug(f"RAWã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {repo_path}")
                    return True
                except Exception as e:
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(f"Rate limit exceeded for RAW. Waiting {wait_time}s... ({attempt + 1}/5)")
                        time.sleep(wait_time)
                        continue

                    logger.warning(f"RAWã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {repo_path} - {e} - Retrying ({attempt + 1}/5)...")
                    time.sleep(10 * (attempt + 1))
            return False
        return True

    def upload_raw_folder(self, folder_path: Path, path_in_repo: str, defer: bool = False) -> bool:
        """ãƒ•ã‚©ãƒ«ãƒ€å˜ä½ã§ã®ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ãƒªãƒˆãƒ©ã‚¤ä»˜)"""
        if not folder_path.exists():
            return True  # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãªã—ã¯æˆåŠŸã¨ã¿ãªã™

        if self.api:
            if defer:
                # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                for f in folder_path.glob("**/*"):
                    if f.is_file():
                        r_path = f"{path_in_repo}/{f.relative_to(folder_path)}"
                        self._commit_operations[r_path] = CommitOperationAdd(
                            path_in_repo=r_path, path_or_fileobj=str(f)
                        )
                logger.debug(f"RAWãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {path_in_repo}")
                return True

            max_retries = 5  # 3å›ã‹ã‚‰5å›ã«å¼·åŒ–
            for attempt in range(max_retries):
                try:
                    self.api.upload_folder(
                        folder_path=str(folder_path),
                        path_in_repo=path_in_repo,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.success(f"ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {path_in_repo} (from {folder_path})")
                    return True
                except Exception as e:
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(
                            f"Folder Upload Rate limit exceeded. Waiting {wait_time}s... ({attempt + 1}/{max_retries})"
                        )
                        time.sleep(wait_time)
                        continue

                    logger.warning(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {e} - Retrying ({attempt + 1}/{max_retries})...")
                    time.sleep(10)

            logger.error(f"ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•— (Give up): {path_in_repo}")
            return False
        return True

    def update_listing_history(self, new_events: pd.DataFrame) -> bool:
        history = self._load_parquet("listing")

        # åˆå›å®Ÿè¡Œæ™‚ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã›ãšã€ã‚¤ãƒ™ãƒ³ãƒˆã‚‚ç©ºï¼‰ã®å ´åˆã§ã‚‚ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if new_events.empty:
            if history.empty:
                # ç©ºã®å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ä¿å­˜
                return self._save_and_upload("listing", history)
            return True

        history = pd.concat([history, new_events], ignore_index=True).drop_duplicates()
        return self._save_and_upload("listing", history)

    def update_index_history(self, new_events: pd.DataFrame) -> bool:
        history = self._load_parquet("index")

        # åˆå›å®Ÿè¡Œæ™‚ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã›ãšã€ã‚¤ãƒ™ãƒ³ãƒˆã‚‚ç©ºï¼‰ã®å ´åˆã§ã‚‚ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if new_events.empty:
            if history.empty:
                # ç©ºã®å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ä¿å­˜
                return self._save_and_upload("index", history)
            return True

        history = pd.concat([history, new_events], ignore_index=True).drop_duplicates()
        return self._save_and_upload("index", history)

    def get_listing_history(self) -> pd.DataFrame:
        """ç¾åœ¨ã®ä¸Šå ´å±¥æ­´ãƒã‚¹ã‚¿ã‚’å–å¾—"""
        return self._load_parquet("listing")

    def get_index_history(self) -> pd.DataFrame:
        """ç¾åœ¨ã®æŒ‡æ•°æ¡ç”¨å±¥æ­´ãƒã‚¹ã‚¿ã‚’å–å¾—"""
        return self._load_parquet("index")

    def update_stocks_master(self, incoming_data: pd.DataFrame):
        """ãƒã‚¹ã‚¿æ›´æ–° & æ™‚ç³»åˆ—ãƒªã‚³ãƒ³ã‚·ãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ (ä¸–ç•Œæœ€é«˜æ°´æº–ã®æ­´å²å†æ§‹ç¯‰ãƒ­ã‚¸ãƒƒã‚¯)"""
        if incoming_data.empty:
            return True

        # 1. ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨å‹æ­£è¦åŒ–
        records = incoming_data.to_dict("records")
        validated = []
        for rec in records:
            try:
                rec = {k: (v if not pd.isna(v) else None) for k, v in rec.items()}
                # is_active ã®å‹æ­£è¦åŒ–
                if isinstance(rec.get("is_active"), str):
                    rec["is_active"] = rec["is_active"].lower() in ["true", "1", "yes"]
                # ã€æœ€é©è§£ã€‘æƒ…å ±ã®æå¤±ã‚’ä¼´ã†åˆ‡ã‚Šæ¨ã¦ã‚’å»ƒæ­¢ã—ã€ã‚½ãƒ¼ã‚¹ã®ç²¾åº¦ã‚’ç¶­æŒã™ã‚‹
                # (Datetimeå‹ã¸ã®å¤‰æ›ã¯å¾Œç¶šã®ä¿å­˜ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¾ãŸã¯Pydanticãƒ¢ãƒ‡ãƒ«ã«å§”ã­ã‚‹)
                validated.append(StockMasterRecord(**rec).model_dump())
            except Exception as e:
                logger.error(f"éŠ˜æŸ„æƒ…å ±ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•— (code: {rec.get('code')}): {e}")

        if not validated:
            return True
        incoming_df = pd.DataFrame(validated)

        # 2. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®çµ±åˆ (ãƒªã‚³ãƒ³ã‚·ãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³)
        # æ—¢å­˜ãƒã‚¹ã‚¿ã‚’ã€Œéå»ã®çŠ¶æ…‹ã®ä¸€ã¤ã€ã¨ã—ã¦æ‰±ã„ã€å…¨ã¦ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹
        current_m = self.master_df.copy()
        # ã‚«ãƒ©ãƒ è‡ªä½“ã®å­˜åœ¨ã‚’ã‚±ã‚¢ (NULL ã¯ NULL ã®ã¾ã¾ç¶­æŒ)
        if "last_submitted_at" not in current_m.columns:
            current_m["last_submitted_at"] = None

        # å…¨ã¦ã®æ—¢çŸ¥ã®çŠ¶æ…‹ã‚’çµ±åˆ
        # ã€é‡è¦ã€‘ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦çµåˆ
        all_states = pd.concat([current_m, incoming_df], ignore_index=True)

        # é‡è¤‡æ’é™¤ (å±æ€§ã®å¤‰åŒ–ã‚‚ã€Œæ–°ã—ã„è¨¼è¨€ã€ã¨ã—ã¦å—ã‘å…¥ã‚Œã‚‹)
        # ä»¥å‰ã¯ subset=["code", "company_name", "last_submitted_at"] ã®ã¿ã ã£ãŸãŸã‚ã€
        # NULLå±æ€§ã®å¤ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒæœ€æ–°ã®JPXå±æ€§ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦ã„ãŸã€‚
        all_states.drop_duplicates(
            subset=["code", "company_name", "last_submitted_at", "is_active", "sector_jpx_33", "market"], inplace=True
        )

        # 3. ç¤¾åå¤‰æ›´ã®æ­´å²çš„å¤‰é·ã‚’è§£æ
        name_history = self._load_parquet("name")
        new_history_events = []

        processed_codes = set()

        for code, group in all_states.groupby("code"):
            processed_codes.add(code)

            # æå‡ºæ—¥æ™‚ã®æ˜‡é †ã§ã‚½ãƒ¼ãƒˆ (ã“ã‚ŒãŒãªã„ã¨ sorted_group ãŒæœªå®šç¾©ã«ãªã‚‹)
            sorted_group = group.sort_values("last_submitted_at", ascending=True)

            # --- C. æ­´å²ã®å®Œå…¨å†æ§‹ç¯‰ (Full History Rebuild) ---
            # æ—¢å­˜ã®å±¥æ­´ã€ç¾åœ¨ã®ãƒã‚¹ã‚¿ã€æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦ã€Œã‚¤ãƒ™ãƒ³ãƒˆã€ã¨ã—ã¦æ™‚ç³»åˆ—ã«ä¸¦ã¹ç›´ã™

            timeline_events = []

            # 1. æ—¢å­˜ãƒã‚¹ã‚¿ & æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º
            for _, row in sorted_group.iterrows():
                if pd.notna(row.get("last_submitted_at")):
                    timeline_events.append(
                        {"date": row["last_submitted_at"], "name": row["company_name"], "source": "master_or_incoming"}
                    )

            # 2. æ—¢å­˜å±¥æ­´(name_history)ã‹ã‚‰ã®ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º
            # ã“ã‚Œã¾ã§ã®è¨˜éŒ²ã‚‚ã€Œéå»ã®è¨¼è¨€ã€ã¨ã—ã¦æ¡ç”¨ã™ã‚‹
            if not name_history.empty:
                code_hist = name_history[name_history["code"] == code].sort_values("change_date")
                if not code_hist.empty:
                    # ã€é‡è¦: è‡ªå·±ä¿®å¾©ã‚·ãƒ¼ãƒ‰ã®æ³¨å…¥ã€‘
                    # ä¸€ç•ªæœ€åˆã®ç¤¾åå¤‰æ›´ã‚¤ãƒ™ãƒ³ãƒˆã®ã€Œold_nameã€ã‚’æ­´å²ã®å¤œæ˜ã‘ã¨ã—ã¦æ¤ãˆä»˜ã‘ã‚‹
                    first_hist = code_hist.iloc[0]
                    timeline_events.append(
                        {
                            "date": "0000-00-00",
                            "name": first_hist["old_name"],
                            "source": "history_seed",
                        }
                    )

                for _, h_row in code_hist.iterrows():
                    timeline_events.append(
                        {"date": h_row["change_date"], "name": h_row["new_name"], "source": "history"}
                    )

            # 3. æ™‚ç³»åˆ—ã‚½ãƒ¼ãƒˆ (å¤ã„é †)
            # æ—¥ä»˜å‹ã¸ã®å¤‰æ›ã¨ã‚½ãƒ¼ãƒˆ
            # (æ³¨æ„: æ–‡å­—åˆ—æ¯”è¼ƒã§ã‚‚ YYYY-MM-DD å½¢å¼ãªã‚‰æ¦‚ã­æ©Ÿèƒ½ã™ã‚‹ãŒã€pd.to_datetimeæ¨å¥¨)
            timeline_events.sort(key=lambda x: str(x["date"]))

            # 4. æ­´å²ã®å†ç”Ÿ (Replay)
            current_tracking_name = None

            # åˆæœŸå€¤ã®æ¨è«–:
            # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®æœ€åˆã®ã‚¤ãƒ™ãƒ³ãƒˆã®ã€Œå‰ã€ã®çŠ¶æ…‹ã¯åˆ†ã‹ã‚‰ãªã„ã€‚
            # ã—ã‹ã—ã€æœ€åˆã®ã‚¤ãƒ™ãƒ³ãƒˆåãŒã€Œæœ€åˆã®åå‰ã€ã§ã‚ã‚‹ã“ã¨ã¯ç¢ºå®šã§ãã‚‹ã€‚

            rebuilt_code_events = []

            for evt in timeline_events:
                evt_name = evt["name"]
                evt_date = evt["date"]

                if current_tracking_name is None:
                    current_tracking_name = evt_name
                    continue

                # æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒ
                norm_curr = self._normalize_company_name(current_tracking_name)
                norm_evt = self._normalize_company_name(evt_name)

                if norm_curr != norm_evt:
                    # å¤‰æ›´æ¤œçŸ¥
                    # éå»ã«è¨˜éŒ²ã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆã¨å…¨ãåŒã˜ã‚‚ã®(æ—¥æ™‚ãƒ»æ–°æ—§å)ã§ã‚ã‚Œã°ã€
                    # é‡è¤‡æ’é™¤ã•ã‚Œã‚‹ãŒã€ã“ã“ã§ã¯æ„å›³çš„ã«ã€Œå†ç”Ÿæˆã€ã™ã‚‹ã€‚
                    rebuilt_code_events.append(
                        {"code": code, "old_name": current_tracking_name, "new_name": evt_name, "change_date": evt_date}
                    )
                    logger.info(f"ğŸ”„ Rebuild History: {code} | {current_tracking_name} -> {evt_name} ({evt_date})")
                    current_tracking_name = evt_name

            # 5. çµæœã®æ ¼ç´ (ãƒ¡ãƒ¢ãƒªä¸Šã®æ›´æ–°)
            # ã“ã®ã‚³ãƒ¼ãƒ‰ã«é–¢ã™ã‚‹æ–°ã—ã„å±¥æ­´ã‚’ç¢ºå®šãƒªã‚¹ãƒˆã«è¿½åŠ 
            # (é‡è¤‡é™¤å¤–ã¯å¾Œç¶šã® drop_duplicates ã§è¡Œã‚ã‚Œã‚‹ãŒã€
            #  å¤ã„èª¤ã£ãŸå±¥æ­´(æœªæ¥->éå»)ã‚’æ¶ˆã™ãŸã‚ã«ã€å¾Œã§ name_history ã‹ã‚‰ã“ã®ã‚³ãƒ¼ãƒ‰åˆ†ã‚’é™¤å¤–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹)
            new_history_events.extend(rebuilt_code_events)

        # 4. å±¥æ­´ã®ä¿å­˜ (Atomic & Non-destructive)
        # ã€ä¿®æ­£ã€‘History Evaporationï¼ˆå±¥æ­´ã®è’¸ç™ºï¼‰ãƒã‚°ã‚’ä¿®æ­£ã€‚
        # ä»¥å‰ã¯ processed_codes ã«è©²å½“ã™ã‚‹å…¨å±¥æ­´ã‚’å‰Šé™¤ã—ã¦ã„ãŸãŒã€
        # ã“ã‚Œã§ã¯åˆ¥æœŸé–“ã®å®Ÿè¡Œæ™‚ã«æ—¢å­˜ã®å±¥æ­´ãŒæ¶ˆãˆã¦ã—ã¾ã†ã€‚
        # æ—¢å­˜ã®å±¥æ­´ã‚’ä¿æŒã—ãŸã¾ã¾ã€æ–°ã—ã„å¤‰é·ã®ã¿ã‚’ãƒãƒ¼ã‚¸ã—ã¦é‡è¤‡æ’é™¤ã™ã‚‹ã€‚

        if new_history_events:
            new_hist_df = pd.DataFrame(new_history_events)
            name_history = pd.concat([name_history, new_hist_df], ignore_index=True)

        if processed_codes:  # å¤‰æ›´ãŒã‚ã£ã¦ã‚‚ãªãã¦ã‚‚ã€ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ï¼ˆå‰Šé™¤ã®åæ˜ ï¼‰ã¯å¿…è¦
            name_history = name_history.drop_duplicates()
            # defer=True ã‚’æŒ‡å®šã—ã¦ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«ç©ã‚€
            self._save_and_upload("name", name_history, defer=True)
            if new_history_events:
                logger.info(f"æ™‚ç³»åˆ—ãƒªã‚³ãƒ³ã‚·ãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³: {len(new_history_events)} ä»¶ã®å¤‰é·ã‚’ç‰¹å®š (Clean Rebuild)")
            else:
                logger.info("æ™‚ç³»åˆ—ãƒªã‚³ãƒ³ã‚·ãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³: å¤‰æ›´ãªã— (å±¥æ­´ã¯ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¾ã—ãŸ)")

        # å…¨çŠ¶æ…‹ã®ä¸­ã‹ã‚‰ã€code ã”ã¨ã«æå‡ºæ—¥æ™‚ãŒæœ€æ–°ã®ã‚‚ã®ã‚’æŠ½å‡º
        sorted_all = all_states.sort_values("last_submitted_at", ascending=False)

        # ã‚»ã‚¯ã‚¿ãƒ¼ã¨å¸‚å ´æƒ…å ±ã®ã€Œå±æ€§ç¶™æ‰¿ï¼ˆInheritanceï¼‰ã€
        # æœ€æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒ NULL ã‚„ "ãã®ä»–" ã®å ´åˆã€éå»ã®æœ‰åŠ¹ãªãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆJPXç­‰ï¼‰ã‹ã‚‰å¼•ãç¶™ã
        def resolve_attr(group, col):
            # æå‡ºæ—¥ã«é–¢ã‚ã‚‰ãšã€ãã®ã‚³ãƒ¼ãƒ‰ã«ãŠã‘ã‚‹ NULL ä»¥å¤–ã®æœ€ã‚‚ç¢ºã‹ãªå€¤ã‚’æ¢ã™
            # (JPXã¯1970å¹´ã ãŒã‚»ã‚¯ã‚¿ãƒ¼æƒ…å ±ã¯ã€Œæ­£ã€ã§ã‚ã‚‹ãŸã‚ã€å…¨ä½“ã‹ã‚‰æ¤œç´¢ã—ã¦è‰¯ã„)
            if col not in group.columns:
                return None
            valid = group[col][~group[col].isin(["ãã®ä»–", None, "nan", ""])]
            return valid.iloc[0] if not valid.empty else None

        # å„ã‚³ãƒ¼ãƒ‰ã®æœ€æ–°çŠ¶æ…‹ã‚’ç‰¹å®šã—ã¤ã¤ã€å±æ€§ã‚’è£œå®Œ
        best_records = []
        for _, group in sorted_all.groupby("code", sort=False):
            # 1. ç‰©ç†çš„ãªæœ€æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾— (ç¤¾åã¨æå‡ºæ—¥æ™‚ã®æ±ºå®šç”¨)
            latest_rec = group.iloc[0].copy()

            # 2. JPXãƒ¬ã‚³ãƒ¼ãƒ‰(æ—¥ä»˜ãªã—)ã‚’ç‰¹å®š (å±æ€§ã®æ­£è§£ãƒ‡ãƒ¼ã‚¿)
            jpx_entries = group[group["last_submitted_at"].isna()]

            if not jpx_entries.empty:
                # JPXãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ä¸»è¦å±æ€§ã‚’JPXã‹ã‚‰å¼·åˆ¶å–å¾—ï¼ˆEDINETå±æ€§ã‚’æ‹’çµ¶ï¼‰
                jpx_rec = jpx_entries.iloc[0]
                latest_rec["sector_jpx_33"] = jpx_rec.get("sector_jpx_33")
                latest_rec["market"] = jpx_rec.get("market")
                # ä¸‡ãŒä¸€ JPX ã®ã‚»ã‚¯ã‚¿ãƒ¼ãŒä¸å…¨ãªå ´åˆã¯ã€éå»ã®æœ‰åŠ¹ãªå±æ€§ã‹ã‚‰æ‹¾ã†ï¼ˆãŸã ã—å„ªå…ˆåº¦ã¯JPXï¼‰
                if latest_rec.get("sector_jpx_33") in ["ãã®ä»–", None, "nan", ""]:
                    latest_rec["sector_jpx_33"] = resolve_attr(group, "sector_jpx_33")
            else:
                # JPXã«ä¸€åº¦ã‚‚ç™»éŒ²ã•ã‚ŒãŸã“ã¨ãŒãªã„(å®Œå…¨æ–°è¦ä¸Šå ´ç­‰)ã®å ´åˆ
                latest_rec["sector_jpx_33"] = None
                latest_rec["market"] = None

            best_records.append(latest_rec)

        self.master_df = pd.DataFrame(best_records)

        # defer=True ã‚’æŒ‡å®šã—ã¦ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«ç©ã‚€
        return self._save_and_upload("master", self.master_df, defer=True)

    def get_last_index_list(self, index_name: str) -> pd.DataFrame:
        """æŒ‡å®šæŒ‡æ•°ã®æ§‹æˆéŠ˜æŸ„ã‚’å–å¾— (Phase 3ç”¨)"""
        return pd.DataFrame(columns=["code"])

    def get_sector(self, code: str) -> str:
        """è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æ¥­ç¨®å–å¾—"""
        if self.master_df.empty:
            return None
        row = self.master_df[self.master_df["code"] == code]
        if not row.empty:
            col_name = "sector_jpx_33" if "sector_jpx_33" in self.master_df.columns else "sector"
            val = row.iloc[0].get(col_name)
            return str(val) if val is not None else None
        return None

    def save_delta(
        self,
        key: str,
        df: pd.DataFrame,
        run_id: str,
        chunk_id: str,
        custom_filename: str = None,
        defer: bool = False,
        local_only: bool = False,
    ) -> bool:
        """
        ãƒ‡ãƒ«ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€‚
        local_only=True ã®å ´åˆã€HFã«ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã›ãšãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã®ã¿è¡Œã† (GHA Artifactç”¨)ã€‚
        """
        if df.empty:
            return True

        if custom_filename:
            filename = custom_filename
        else:
            filename = f"{Path(self.paths[key]).stem}.parquet"

        # ãƒªãƒã‚¸ãƒˆãƒªå†…ãƒ‘ã‚¹
        delta_repo_path = f"temp/deltas/{run_id}/{chunk_id}/{filename}"

        # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆ (MergerãŒåé›†ã—ã‚„ã™ã„ã‚ˆã†ã«æ§‹é€ åŒ–)
        local_delta_dir = self.data_path / "deltas" / str(run_id) / str(chunk_id)
        local_delta_dir.mkdir(parents=True, exist_ok=True)
        local_file = local_delta_dir / filename

        # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘ä¿å­˜ç›´å‰ã«æœ€çµ‚ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        df = self._clean_dataframe(key, df)

        df.to_parquet(local_file, index=False, compression="zstd")

        if local_only:
            logger.debug(f"Delta saved locally (local_only): {local_file}")
            return True

        return self.upload_raw(local_file, delta_repo_path, defer=defer)

    def mark_chunk_success(self, run_id: str, chunk_id: str, defer: bool = False, local_only: bool = False) -> bool:
        """ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æˆåŠŸãƒ•ãƒ©ã‚° (_SUCCESS) ã‚’ä½œæˆ"""
        success_repo_path = f"temp/deltas/{run_id}/{chunk_id}/_SUCCESS"

        local_delta_dir = self.data_path / "deltas" / str(run_id) / str(chunk_id)
        local_delta_dir.mkdir(parents=True, exist_ok=True)
        local_file = local_delta_dir / "_SUCCESS"
        local_file.touch()

        if local_only:
            logger.debug(f"Chunk success marked locally: {local_file}")
            return True

        return self.upload_raw(local_file, success_repo_path, defer=defer)

    def load_deltas(self, run_id: str) -> Dict[str, pd.DataFrame]:
        """
        å…¨ãƒ‡ãƒ«ã‚¿ã‚’åé›†ã—ã¦ãƒãƒ¼ã‚¸ (Mergerç”¨)
        ãƒ­ãƒ¼ã‚«ãƒ« (data/deltas/{run_id}) ã¨ãƒªãƒ¢ãƒ¼ãƒˆ (HF) ã®ä¸¡æ–¹ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹ã€‚
        """
        deltas = {}
        processed_chunks = set()

        # --- A. ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ (GHA Artifacts ç­‰ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®å ´åˆ) ---
        local_run_dir = self.data_path / "deltas" / str(run_id)
        if local_run_dir.exists():
            logger.info(f"Checking local deltas in {local_run_dir}")
            for chunk_dir in local_run_dir.iterdir():
                if not chunk_dir.is_dir():
                    continue

                chunk_id = chunk_dir.name
                if not (chunk_dir / "_SUCCESS").exists():
                    logger.warning(f"âš ï¸ æœªå®Œäº†ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—: {chunk_id}")
                    continue

                processed_chunks.add(chunk_id)
                for p_file in chunk_dir.glob("*.parquet"):
                    key = self._get_key_from_filename(p_file.name)
                    if key:
                        try:
                            df = pd.read_parquet(p_file)
                            deltas.setdefault(key, []).append(df)
                        except Exception as e:
                            logger.error(f"âŒ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ«ã‚¿èª­ã¿è¾¼ã¿å¤±æ•— ({p_file.name}): {e}")

        # --- B. ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ã‚­ãƒ£ãƒ³ (Hugging Face Repository) ---
        if self.api:
            try:
                folder = f"temp/deltas/{run_id}"
                files = []
                # åæ˜ é…å»¶ã«å¯¾å‡¦
                for attempt in range(3):
                    files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type="dataset")
                    target_files = [f for f in files if f.startswith(folder)]
                    if target_files:
                        break
                    if attempt < 2:
                        logger.warning(f"ãƒªãƒ¢ãƒ¼ãƒˆãƒ‡ãƒ«ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å†è©¦è¡Œä¸­... ({attempt + 1}/3)")
                        time.sleep(10)

                # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                remote_chunks = {}
                for f in target_files:
                    parts = f.split("/")
                    if len(parts) < 4:
                        continue
                    chunk_id = parts[3]
                    # ã™ã§ã«ãƒ­ãƒ¼ã‚«ãƒ«ã§å‡¦ç†æ¸ˆã¿ã®ãƒãƒ£ãƒ³ã‚¯ã¯ã‚¹ã‚­ãƒƒãƒ— (é‡è¤‡é˜²æ­¢)
                    if chunk_id in processed_chunks:
                        continue
                    remote_chunks.setdefault(chunk_id, []).append(f)

                valid_remote_count = 0
                for chunk_id, file_list in remote_chunks.items():
                    if not any(f.endswith("_SUCCESS") for f in file_list):
                        logger.warning(f"âš ï¸ æœªå®Œäº†ã®ãƒªãƒ¢ãƒ¼ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—: {chunk_id}")
                        continue

                    valid_remote_count += 1
                    for remote_path in file_list:
                        if remote_path.endswith("_SUCCESS"):
                            continue

                        key = self._get_key_from_filename(Path(remote_path).name)
                        if key:
                            attempts = 2
                            for att in range(attempts):
                                try:
                                    local_path = hf_hub_download(
                                        repo_id=self.hf_repo,
                                        filename=remote_path,
                                        repo_type="dataset",
                                        token=self.hf_token,
                                    )
                                    df = pd.read_parquet(local_path)
                                    deltas.setdefault(key, []).append(df)
                                    break
                                except Exception as e:
                                    if att == attempts - 1:
                                        logger.error(f"âŒ ãƒªãƒ¢ãƒ¼ãƒˆãƒ‡ãƒ«ã‚¿èª­ã¿è¾¼ã¿å¤±æ•— ({remote_path}): {e}")
                                    else:
                                        time.sleep(5)

                logger.info(f"åé›†çµæœ: Local Chunks={len(processed_chunks)}, Remote Chunks={valid_remote_count}")

            except Exception as e:
                logger.error(f"ãƒªãƒ¢ãƒ¼ãƒˆãƒ‡ãƒ«ã‚¿åé›†å¤±æ•—: {e}")

        # --- C. æœ€çµ‚ãƒãƒ¼ã‚¸ ---
        merged = {}
        for key, df_list in deltas.items():
            if df_list:
                merged[key] = pd.concat(df_list, ignore_index=True)
            else:
                merged[key] = pd.DataFrame()
        return merged

    def _get_key_from_filename(self, fname: str) -> Optional[str]:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å†…éƒ¨ã‚­ãƒ¼ã‚’åˆ¤å®šã™ã‚‹"""
        if fname == "documents_index.parquet":
            return "catalog"
        if fname == "stocks_master.parquet":
            return "master"
        if fname == "listing_history.parquet":
            return "listing"
        if fname == "index_history.parquet":
            return "index"
        if fname == "name_history.parquet":
            return "name"
        if fname.startswith("financial_values_bin"):
            bin_id = fname.replace("financial_values_bin", "").replace(".parquet", "")
            return f"financial_bin{bin_id}"
        if fname.startswith("qualitative_text_bin"):
            bin_id = fname.replace("qualitative_text_bin", "").replace(".parquet", "")
            return f"text_bin{bin_id}"
        if fname.startswith("financial_values_"):
            sector = fname.replace("financial_values_", "").replace(".parquet", "")
            return f"financial_{sector}"
        if fname.startswith("qualitative_text_"):
            sector = fname.replace("qualitative_text_", "").replace(".parquet", "")
            return f"text_{sector}"
        return None

    def push_commit(self, message: str = "Batch update from ARIA") -> bool:
        """
        ãƒãƒƒãƒ•ã‚¡ã«æºœã¾ã£ãŸæ“ä½œã‚’ã‚³ãƒŸãƒƒãƒˆå®Ÿè¡Œã€‚
        ã€ç©¶æ¥µã®å®‰å®šåŒ–ã€‘æ“ä½œæ•°ãŒå¤šã„å ´åˆã¯ã€HFå´ã®è² è·ã¨429ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã€è‡ªå‹•çš„ã«åˆ†å‰²ã—ã¦ã‚³ãƒŸãƒƒãƒˆã™ã‚‹ã€‚
        """
        if not self.api or not self._commit_operations:
            return True

        ops_list = list(self._commit_operations.values())
        total_ops = len(ops_list)

        # 1ã‚³ãƒŸãƒƒãƒˆã‚ãŸã‚Šã®æœ€å¤§æ“ä½œæ•°
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ (128å›/æ™‚) ã‚’å›é¿ã™ã‚‹ãŸã‚ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ‹¡å¤§ã—ã¦ã‚³ãƒŸãƒƒãƒˆå›æ•°ã‚’å‰Šæ¸›ã™ã‚‹
        # HFå´ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãªã„ã‚®ãƒªã‚®ãƒªã®ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ 500ä»¶ç¨‹åº¦ãŒæœ€é©
        # ã€ä¿®æ­£ã€‘Hugging Face API åˆ¶é™ (128 req/hour) ã¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ã®ãŸã‚ã€
        # GHAä¸¦åˆ—æ•°(20) ã‚’è€ƒæ…®ã—ã¦ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ 200 ã«ç¸®å°ã—ã€åˆè¨ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’æŠ‘åˆ¶ã™ã‚‹ã€‚
        # (600 files / 200 = 3 commits * 20 jobs = 60 req < 128 req)
        batch_size = 200

        batches = [ops_list[i : i + batch_size] for i in range(0, total_ops, batch_size)]

        logger.info(f"ğŸš€ ã‚³ãƒŸãƒƒãƒˆé€ä¿¡é–‹å§‹: åˆè¨ˆ {total_ops} æ“ä½œã‚’ {len(batches)} ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦å®Ÿè¡Œã—ã¾ã™")

        for i, batch in enumerate(batches):
            batch_msg = f"{message} (part {i + 1}/{len(batches)})"
            max_retries = 12
            success = False

            for attempt in range(max_retries):
                try:
                    # ã€é‡è¦ã€‘create_commit ã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒé‡ã„ãŸã‚ã€å€‹åˆ¥ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
                    # (ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã£ã¦ã¯ç›´æ¥å¼•æ•°ã‚’å–ã‚‰ãªã„å ´åˆãŒã‚ã‚‹ãŸã‚ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³å´ã§ä¿è­·)
                    self.api.create_commit(
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        operations=batch,
                        commit_message=batch_msg,
                        token=self.hf_token,
                    )
                    success = True
                    break
                except BaseException as e:
                    if isinstance(e, Exception):
                        status_code = getattr(getattr(e, "response", None), "status_code", None)

                        # 429 ãƒ¬ãƒ¼ãƒˆåˆ¶é™ ã¾ãŸã¯ 500 ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
                        if status_code in [429, 500]:
                            # 429ã®å ´åˆã¯ã‚ˆã‚Šé•·ãå¾…æ©Ÿ (HFã®å›å¾©ã‚’å¾…ã¤)
                            wait_time = int(getattr(e.response.headers, "get", lambda x, y: y)("Retry-After", 60))
                            wait_time = max(wait_time, 60) + (attempt * 30) + random.uniform(5, 15)
                            logger.warning(
                                f"HF Server Error ({status_code}). Waiting {wait_time:.1f}s... "
                                f"(Batch {i + 1}, Attempt {attempt + 1}/{max_retries})"
                            )
                            time.sleep(wait_time)
                            continue

                        # 409 ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ ã¾ãŸã¯ 412 å‰ææ¡ä»¶å¤±æ•—
                        if status_code in [409, 412]:
                            # 20ä¸¦åˆ—ä»¥ä¸Šã®ç’°å¢ƒä¸‹ã§ã¯ã€å¾…æ©Ÿæ™‚é–“ã‚’åºƒã‚ã«åˆ†æ•£ã•ã›ã‚‹ (10ã€œ70ç§’ + æŒ‡æ•°)
                            wait_time = (2 ** (attempt + 1)) * 5 + (random.uniform(10, 60))
                            logger.warning(
                                f"Commit Conflict ({status_code}). Retrying in {wait_time:.2f}s... "
                                f"(Batch {i + 1}, Attempt {attempt + 1}/{max_retries})"
                            )
                            time.sleep(wait_time)
                            continue

                        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç­‰ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¾‹å¤–
                        wait_time = (attempt + 1) * 20 + random.uniform(5, 15)
                        logger.warning(
                            f"é€šä¿¡ã‚¨ãƒ©ãƒ¼ ({e}): {wait_time:.1f}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™... "
                            f"(Batch {i + 1}, Attempt {attempt + 1}/{max_retries})"
                        )
                        time.sleep(wait_time)
                    else:
                        # KeyboardInterrupt ã‚„ SystemExit ãªã©ã€é€šå¸¸ã®ä¾‹å¤–ä»¥å¤–ã§çµ‚äº†ã™ã‚‹å ´åˆ
                        logger.critical(
                            f"âš ï¸ ãƒ—ãƒ­ã‚»ã‚¹ãŒã‚·ã‚°ãƒŠãƒ«ã¾ãŸã¯è‡´å‘½çš„ãªä¾‹å¤–ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ: {type(e).__name__}"
                        )
                        raise e

            if not success:
                logger.error(f"âŒ ãƒãƒƒãƒ {i + 1} ã®é€ä¿¡ã«æœ€çµ‚çš„ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                return False

            # ãƒãƒƒãƒé–“ã«çŸ­ã„ä¼‘æ†©ã‚’æŒŸã‚“ã§HFå´ã®è² è·ã‚’é€ƒãŒã™
            if i < len(batches) - 1:
                time.sleep(random.uniform(3, 7))

        logger.success(f"âœ… å…¨ {total_ops} æ“ä½œã®ãƒãƒƒãƒã‚³ãƒŸãƒƒãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        self._commit_operations = {}  # ã‚¯ãƒªã‚¢
        return True

    def cleanup_deltas(self, run_id: str, cleanup_old: bool = True):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (Mergerç”¨)"""
        if not self.api:
            return

        try:
            files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type="dataset")
            delta_root = "temp/deltas"

            # å‰Šé™¤å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            delete_files = []

            if cleanup_old:
                # 24æ™‚é–“ä»¥ä¸ŠçµŒéã—ãŸã‚‚ã®ã‚’å¯¾è±¡ã¨ã™ã‚‹
                from datetime import datetime, timezone

                now = datetime.now(timezone.utc)
                expired_runs = set()

                for f in files:
                    if not f.startswith(delta_root):
                        continue
                    parts = f.split("/")
                    if len(parts) < 3:
                        continue
                    r_id = parts[2]

                    # ã€ä¿®æ­£ã€‘run_id ã¯ 'backfill-YYYY-MM-DD-NNNNNN' ç­‰ã®å½¢å¼
                    # æ—¥ä»˜éƒ¨åˆ†ã‚’æ­£è¦è¡¨ç¾ã§æŠ½å‡ºã—ã€24æ™‚é–“ä»¥ä¸ŠçµŒéã—ã¦ã„ã‚‹ã‹ã‚’åˆ¤å®š
                    try:
                        date_match = re.search(r"(\d{4}-\d{2}-\d{2})", r_id)
                        if date_match:
                            run_date = datetime.strptime(date_match.group(1), "%Y-%m-%d").replace(tzinfo=timezone.utc)
                            if (now - run_date).total_seconds() > 86400:
                                delete_files.append(f)
                                expired_runs.add(r_id)
                        else:
                            # æ—¥ä»˜ã‚’å«ã¾ãªã„run_idï¼ˆç´”ç²‹ãªæ•°å€¤ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç­‰ï¼‰ã‚‚å‡¦ç†
                            try:
                                timestamp = int(r_id)
                                if (now.timestamp() - timestamp) > 86400:
                                    delete_files.append(f)
                                    expired_runs.add(r_id)
                            except ValueError:
                                # ãƒ‘ãƒ¼ã‚¹ä¸èƒ½ãªrun_idã¯7æ—¥ä»¥ä¸ŠçµŒéã¨ã¿ãªã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                                delete_files.append(f)
                                expired_runs.add(r_id)
                    except Exception:
                        pass

                if delete_files:
                    logger.info(f"å¤ã„ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¸…æƒä¸­... (24æ™‚é–“ä»¥ä¸ŠçµŒé: {len(expired_runs)} runs)")

            else:
                # ä»Šå›ã®ãƒ©ãƒ³IDã®ã¿å¯¾è±¡
                target_prefix = f"{delta_root}/{run_id}"
                delete_files = [f for f in files if f.startswith(target_prefix)]
                if delete_files:
                    logger.info(f"ä»Šå›ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ä¸­... {run_id} ({len(delete_files)} files)")

            if not delete_files:
                return

            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ‹¡å¤§ (50 -> 500) ã—ã¦APIã‚³ãƒ¼ãƒ«æ•°ã‚’å‰Šæ¸›
            batch_size = 500
            total_batches = (len(delete_files) + batch_size - 1) // batch_size

            for i in range(0, len(delete_files), batch_size):
                batch = delete_files[i : i + batch_size]
                del_ops = [CommitOperationDelete(path_in_repo=p) for p in batch]

                batch_num = (i // batch_size) + 1
                commit_msg = f"Cleanup deltas (Batch {batch_num}/{total_batches})"

                # ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ (Backoff)
                max_retries = 10
                success = False
                for attempt in range(max_retries):
                    try:
                        self.api.create_commit(
                            repo_id=self.hf_repo,
                            repo_type="dataset",
                            operations=del_ops,
                            commit_message=commit_msg,
                            token=self.hf_token,
                        )
                        success = True
                        break
                    except Exception as e:
                        if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                            wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                            logger.warning(
                                f"Cleanup Rate limit exceeded. Waiting {wait_time}s... "
                                f"(Batch {batch_num}/{total_batches}, Attempt {attempt + 1})"
                            )
                            time.sleep(wait_time)
                            continue

                        logger.warning(
                            f"Cleanup error: {e}. Retrying... "
                            f"(Batch {batch_num}/{total_batches}, Attempt {attempt + 1})"
                        )
                        time.sleep(10 * (attempt + 1))

                if success:
                    logger.debug(f"Cleanup batch {batch_num}/{total_batches} done.")
                    if batch_num < total_batches:
                        time.sleep(2)  # ãƒãƒƒãƒé–“ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³
                else:
                    logger.error(f"âŒ Cleanup batch {batch_num} failed permanently.")

            logger.success("Cleanup sequence completed.")

        except Exception as e:
            logger.error(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å…¨ä½“å¤±æ•—: {e}")
