import random
import re
import time
import unicodedata
from pathlib import Path
from typing import Dict, List

import pandas as pd
import requests
from huggingface_hub import CommitOperationAdd, CommitOperationDelete, HfApi, hf_hub_download
from huggingface_hub.utils import EntryNotFoundError, HfHubHTTPError, RepositoryNotFoundError
from loguru import logger
from models import CatalogRecord, StockMasterRecord


class CatalogManager:
    def __init__(self, hf_repo: str, hf_token: str, data_path: Path):
        self.hf_repo = hf_repo
        self.hf_token = hf_token
        self.data_path = data_path
        self.data_path.mkdir(parents=True, exist_ok=True)

        # ã€ä¿®æ­£ã€‘é€šä¿¡å®‰å®šæ€§å‘ä¸Šã®ãŸã‚ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å»¶é•·ã—ãŸã‚«ã‚¹ã‚¿ãƒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨
        if hf_repo and hf_token:
            session = requests.Session()
            # read/connect timeout ã‚’å¤§å¹…ã«å»¶é•· (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯çŸ­ã„ãŸã‚)
            adapter = requests.adapters.HTTPAdapter(pool_connections=10, pool_maxsize=10, max_retries=3)
            session.mount("https://", adapter)
            self.api = HfApi(token=hf_token, session=session)
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä¸Šæ›¸ã (å†…éƒ¨çš„ãª requests å‘¼ã³å‡ºã—ç”¨)
            self._default_timeout = 300
        else:
            self.api = None
            self._default_timeout = 30

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å®šç¾©
        self.paths = {
            "catalog": "catalog/documents_index.parquet",
            "master": "meta/stocks_master.parquet",
            "listing": "meta/listing_history.parquet",
            "index": "meta/index_history.parquet",
            "name": "meta/name_history.parquet",
        }

        self.catalog_df = self._load_parquet("catalog")
        self.master_df = self._load_parquet("master")

        # ã€æœ€é‡è¦ã€‘ä¸€æ‹¬ã‚³ãƒŸãƒƒãƒˆç”¨ãƒãƒƒãƒ•ã‚¡
        self._commit_operations = {}
        self._snapshots = {}  # æ•´åˆæ€§ä¿è­·ã®ãŸã‚ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        logger.info("CatalogManager ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")

        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã¨æœ€æ–°ã‚¹ã‚­ãƒ¼ãƒã¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
        self._retrospective_cleanse()

    def _retrospective_cleanse(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èµ°æŸ»ã—ã€ä¸å‚™ãŒã‚ã‚Œã°è‡ªå‹•ä¿®æ­£ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        if not self.api:
            return

        logger.info("Starting integrity check for all Parquet files...")
        updated_count = 0

        # 1. å®šç¾©æ¸ˆã¿ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯
        for key in self.paths.keys():
            try:
                # æ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã® catalog_df, master_df ã¯ _load_parquet ã§ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿
                df = self.catalog_df if key == "catalog" else (self.master_df if key == "master" else None)
                if df is None:
                    df = self._load_parquet(key)

                # ã‚«ã‚¿ãƒ­ã‚°ã®å ´åˆã€18ã‚«ãƒ©ãƒ æœªæº€ãªã‚‰å¼·åˆ¶ä¿å­˜ã—ã¦ã‚¹ã‚­ãƒ¼ãƒæ‹¡å¼µ
                if key == "catalog" and len(df.columns) < 18:
                    self._save_and_upload(key, df, defer=True)
                    updated_count += 1
            except Exception:
                continue

        # 2. ãƒã‚¹ã‚¿ãƒ¼ã®å…¨Binãƒ•ã‚¡ã‚¤ãƒ«ã‚’èµ°æŸ»
        try:
            files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type="dataset")
            bin_files = [f for f in files if "master/bin/" in f and f.endswith(".parquet")]

            for b_file in bin_files:
                local_tmp = self.data_path / "temp_cleanse.parquet"
                self.api.hf_hub_download(
                    repo_id=self.hf_repo,
                    filename=b_file,
                    repo_type="dataset",
                    token=self.hf_token,
                    local_dir=str(self.data_path),
                    local_dir_use_symlinks=False,
                )
                df_bin = pd.read_parquet(self.data_path / b_file)

                # ã‚¹ã‚­ãƒ¼ãƒä¸é©åˆãŒã‚ã‚Œã°ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã—ã¦äºˆç´„
                # (å…·ä½“çš„ãª rec ãƒã‚§ãƒƒã‚¯ã§ã¯ãªãã€ãƒ¢ãƒ‡ãƒ«ã¨ã®ä¸ä¸€è‡´ã‚’åŸºæº–ã«ã™ã‚‹)
                df_clean = self._clean_dataframe("master", df_bin)
                if len(df_clean.columns) != len(df_bin.columns):
                    logger.info(f"Cleaned up bin file schema: {b_file}")
                    df_clean.to_parquet(local_tmp, index=False, compression="zstd")
                    self.add_commit_operation(b_file, local_tmp)
                    updated_count += 1
        except Exception:
            pass

    def _clean_dataframe(self, key: str, df: pd.DataFrame) -> pd.DataFrame:
        """å…¨ã¦ã®DataFrameã«å¯¾ã—ã¦å…±é€šã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚’é©ç”¨"""
        if df.empty:
            return df

        # 0. ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–ï¼ˆç©ºç™½é™¤å»ï¼‰
        df.columns = df.columns.astype(str).str.strip()

        # ã€è¿½åŠ ã€‘å…¨æ–‡å­—åˆ—ã‚«ãƒ©ãƒ ã®ç©ºæ–‡å­—ã‚’æ˜ç¤ºçš„ã« None (NULL) ã«çµ±ä¸€
        for col in df.columns:
            if df[col].dtype == "object":
                # ç©ºç™½ã®ã¿ã®æ–‡å­—åˆ—ã‚‚ NULL æ‰±ã„ã¨ã™ã‚‹
                df[col] = df[col].apply(lambda x: None if (isinstance(x, str) and not x.strip()) else x)

        # 1. ä¸è¦ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”±æ¥ã‚«ãƒ©ãƒ ã®é™¤å»
        drop_targets = ["index", "level_0", "Unnamed: 0"]
        cols_to_drop = [c for c in drop_targets if c in df.columns]

        if cols_to_drop:
            logger.debug(f"{key}: Removed unnecessary columns: {cols_to_drop}")
            df = df.drop(columns=cols_to_drop)

        # 2. ã‚«ã‚¿ãƒ­ã‚°ã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«å®šç¾©ã®ã‚«ãƒ©ãƒ æ§‹æˆã‚’å¼·åˆ¶ (18ã‚«ãƒ©ãƒ åŒ–)
        if key == "catalog":
            # NaN ã‚’ None ã«ç½®æ›
            df = df.replace({pd.NA: None, float("nan"): None})

            # ãƒ¢ãƒ‡ãƒ«å®šç¾©ã®å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—
            expected_cols = list(CatalogRecord.model_fields.keys())

            # æ—¢å­˜ã®ã‚«ãƒ©ãƒ ã®ã¿ã§Pydanticãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šã—ã€ä¸è¶³åˆ†ã‚’Noneã§è£œå®Œ
            validated = []
            for rec_dict in df.to_dict("records"):
                try:
                    # æ¬ è½ã—ã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã£ã¦ã‚‚ Pydantic ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è£œå®Œ
                    validated.append(CatalogRecord(**rec_dict).model_dump())
                except Exception as e:
                    # å¿…é ˆé …ç›®(doc_idç­‰)ãŒæ¬ ã‘ã¦ã„ã‚‹å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼
                    logger.warning(f"ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ä¸­ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ä¸å‚™ (doc_id: {rec_dict.get('doc_id')}): {e}")
                    # æ§‹é€ ã ã‘ã§ã‚‚ç¶­æŒã™ã‚‹ãŸã‚ã€è¾æ›¸ã¨ã—ã¦å¯èƒ½ãªé™ã‚Šä¿æŒ
                    row = {col: rec_dict.get(col) for col in expected_cols}
                    validated.append(row)

            df = pd.DataFrame(validated)
            # ã‚«ãƒ©ãƒ é †ã‚’ãƒ¢ãƒ‡ãƒ«å®šç¾©ã«åˆã‚ã›ã‚‹
            df = df[expected_cols]

            # ã€é‡è¦ã€‘ãƒ‡ãƒ¼ã‚¿å‹ã®æ­£è¦åŒ– (2024.0 å›é¿ã®ãŸã‚ã® Int64 é©ç”¨)
            # pandas ã®æµ®å‹•å°æ•°ç‚¹åŒ–ã‚’é˜»æ­¢ã—ã€æ•´æ•°ã¾ãŸã¯ NULL ã¨ã—ã¦ä¿å­˜
            if "fiscal_year" in df.columns:
                df["fiscal_year"] = pd.to_numeric(df["fiscal_year"], errors="coerce").astype("Int64")
            if "num_months" in df.columns:
                df["num_months"] = pd.to_numeric(df["num_months"], errors="coerce").astype("Int64")

        # 3. è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã®æ­£è¦åŒ– (5æ¡çµ±ä¸€: 4æ¡ãªã‚‰æœ«å°¾0ä»˜ä¸)
        targets = ["master", "listing", "index", "name"]
        if key in targets and "code" in df.columns:
            df["code"] = df["code"].astype(str).str.strip().apply(lambda x: x + "0" if len(x) == 4 else x)

        # 4. Objectå‹ã®å®‰å®šåŒ– (None ã‚’ä¿æŒã—ã¤ã¤æ–‡å­—åˆ—åŒ–)
        for col in df.columns:
            if df[col].dtype == "object":
                # ã€æœ€é‡è¦ã€‘è«–ç†å€¤ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯æ–‡å­—åˆ—åŒ–ã‚’å›é¿
                # æ—¢ã« 'True' / 'False'ï¼ˆæ–‡å­—åˆ—ï¼‰ã«ãªã£ã¦ã—ã¾ã£ã¦ã„ã‚‹å ´åˆã®å¾©æ—§å‡¦ç½®ã‚‚å…¼ã­ã‚‹
                has_string_bools = df[col].isin(["True", "False"]).any()
                if has_string_bools:
                    # æ–‡å­—åˆ—ã® 'True'/'False' ã‚’æ­£è¦ã® Boolean ã«æˆ»ã™ (None ã¯ç¶­æŒ)
                    df[col] = df[col].map({"True": True, "False": False, True: True, False: False}, na_action="ignore")

                # æ”¹ã‚ã¦ãƒã‚§ãƒƒã‚¯ã—ã€ç´”ç²‹ãªæ–‡å­—åˆ—ã‚«ãƒ©ãƒ ã®ã¿ã‚’ as_type(str) ç›¸å½“ã®å‡¦ç†ã«ã‹ã‘ã‚‹
                is_pure_bool = df[col].isin([True, False]).any()
                if not is_pure_bool:
                    df[col] = df[col].apply(lambda x: str(x) if (x is not None and not pd.isna(x)) else None)

        return df

    def _normalize_company_name(self, name: str) -> str:
        """æ¯”è¼ƒåˆ¤å®šã®ãŸã‚ã«æ³•äººæ ¼ã‚„ç©ºç™½ã‚’é™¤å»ã—ã¦æ­£è¦åŒ–ã™ã‚‹ (NFKCå¯¾å¿œç‰ˆ)"""
        if not name or not isinstance(name, str):
            return ""

        # 1. NFKCæ­£è¦åŒ– (å…¨è§’æ•°å­—ãƒ»è‹±å­—ã‚’åŠè§’ã«ã€ãˆ± ãªã©ã‚’ (æ ª) ã«åˆ†è§£)
        n = unicodedata.normalize("NFKC", name)

        # 2. å…¨ã¦ã®ç©ºç™½é™¤å»
        n = n.replace(" ", "").replace("\u3000", "")

        # 3. ä»£è¡¨çš„ãªæ³•äººæ ¼è¡¨è¨˜ã‚’é™¤å»
        # NFKCå¾Œã® (æ ª) ã‚„ (æœ‰) ãªã©ã«å¯¾å¿œã§ãã‚‹ã‚ˆã†ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ•´ç†
        patterns = [
            r"æ ªå¼ä¼šç¤¾",
            r"æœ‰é™ä¼šç¤¾",
            r"åˆåŒä¼šç¤¾",
            r"åˆè³‡ä¼šç¤¾",
            r"åˆåä¼šç¤¾",
            r"ä¸€èˆ¬ç¤¾å›£æ³•äºº",
            r"ä¸€èˆ¬è²¡å›£æ³•äºº",
            r"å…¬ç›Šç¤¾å›£æ³•äºº",
            r"å…¬ç›Šè²¡å›£æ³•äºº",
            r"\(æ ª\)",
            r"\(æœ‰\)",
            r"\(åˆ\)",
            r"\(ç¤¾\)",
            r"\(è²¡\)",
        ]
        for p in patterns:
            n = re.sub(p, "", n)

        return n.strip()

    def add_commit_operation(self, repo_path: str, local_path: Path):
        """ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«æ“ä½œã‚’è¿½åŠ ï¼ˆé‡è¤‡ã¯æœ€æ–°ã§ä¸Šæ›¸ãï¼‰"""
        self._commit_operations[repo_path] = CommitOperationAdd(path_in_repo=repo_path, path_or_fileobj=str(local_path))
        logger.debug(f"ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {repo_path}")

    def take_snapshot(self):
        """ç¾åœ¨ã®GlobalçŠ¶æ…‹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ãƒ¡ãƒ¢ãƒªã«å–å¾— (ä¸æ•´åˆç™ºç”Ÿæ™‚ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨)"""
        # ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«ä¿å­˜
        self._snapshots = {
            "catalog": self.catalog_df.copy(),
            "master": self.master_df.copy(),
            "listing": self._load_parquet("listing").copy(),
            "index": self._load_parquet("index").copy(),
            "name": self._load_parquet("name").copy(),
        }
        logger.info("Global çŠ¶æ…‹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—ã—ã¾ã—ãŸ (å®‰å…¨æ€§ç¢ºä¿)")

    def rollback(self, message: str = "RaW-V Failure: Automated Recovery Rollback"):
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®çŠ¶æ…‹ã‚’å¼·åˆ¶çš„ã«æ›¸ãæˆ»ã—ã€Globalãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’å¾©æ—§ã™ã‚‹"""
        if not self._snapshots:
            logger.error("âŒ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ãã¾ã›ã‚“ã€‚")
            return False

        logger.warning(f"â›” ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’é–‹å§‹ã—ã¾ã™: {message}")

        # æ—¢å­˜ã®ã‚³ãƒŸãƒƒãƒˆäºˆç´„ã‚’ã™ã¹ã¦ç ´æ£„
        self._commit_operations = {}

        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®å†…å®¹ã‚’å¼·åˆ¶çš„ã«ä¸Šæ›¸ãäºˆç´„
        for key, df in self._snapshots.items():
            self._save_and_upload(key, df, defer=True)

        # ä¸€æ‹¬ã‚³ãƒŸãƒƒãƒˆã®å®Ÿè¡Œ (äº‹å®Ÿä¸Šã®å·®ã—æˆ»ã—)
        success = self.push_commit(f"ROLLBACK: {message}")
        if success:
            logger.success("âœ… ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ»ã‚³ãƒŸãƒƒãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚æ•´åˆæ€§ã¯å¾©æ—§ã•ã‚Œã¾ã—ãŸã€‚")
            # ãƒ¡ãƒ¢ãƒªä¸Šã®æœ€æ–°çŠ¶æ…‹ã‚‚ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«æˆ»ã™
            self.catalog_df = self._snapshots["catalog"]
            self.master_df = self._snapshots["master"]
        else:
            logger.critical(
                "âŒ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯è‡ªä½“ã«å¤±æ•—ã—ã¾ã—ãŸï¼"
                "Hugging Faceä¸Šã®ãƒ‡ãƒ¼ã‚¿ãŒå£Šã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ç›´ã¡ã«æ‰‹å‹•ç¢ºèªãŒå¿…è¦ã§ã™ã€‚"
            )
        return success

    def _load_parquet(self, key: str, force_download: bool = False) -> pd.DataFrame:
        filename = self.paths[key]
        try:
            local_path = hf_hub_download(
                repo_id=self.hf_repo,
                filename=filename,
                repo_type="dataset",
                token=self.hf_token,
                force_download=force_download,
            )
            df = pd.read_parquet(local_path)
            # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘èª­ã¿è¾¼ã¿ç›´å¾Œã«ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
            df = self._clean_dataframe(key, df)
            logger.debug(f"ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {filename} ({len(df)} rows)")
            return df
        except RepositoryNotFoundError:
            logger.error(f"ãƒªãƒã‚¸ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.hf_repo}")
            logger.error("ç’°å¢ƒå¤‰æ•° HF_REPO ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            raise
        except (EntryNotFoundError, requests.exceptions.HTTPError) as e:
            # EntryNotFoundError (HFãƒ©ã‚¤ãƒ–ãƒ©ãƒª) ã¾ãŸã¯ ç”Ÿã® 404 (ãƒ‘ãƒƒãƒé©ç”¨æ™‚) ã‚’ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            is_404 = isinstance(e, EntryNotFoundError) or (
                hasattr(e, "response") and e.response is not None and e.response.status_code == 404
            )

            if not is_404:
                # 404 ä»¥å¤–ãªã‚‰ä¸Šä½ã¾ãŸã¯ Exception ã¸é£›ã°ã™
                raise e

            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚æ–°è¦ä½œæˆã—ã¾ã™: {filename}")
            if key == "catalog":
                cols = list(CatalogRecord.model_fields.keys())
                return pd.DataFrame(columns=cols)
            elif key == "master":
                cols = list(StockMasterRecord.model_fields.keys())
                return pd.DataFrame(columns=cols)
            elif key == "listing":
                return pd.DataFrame(columns=["code", "type", "event_date"])
            elif key == "index":
                return pd.DataFrame(columns=["index_name", "code", "type", "event_date"])
            elif key == "name":
                return pd.DataFrame(columns=["code", "old_name", "new_name", "change_date"])
            return pd.DataFrame()
        except HfHubHTTPError as e:
            logger.error(f"HF API ã‚¨ãƒ©ãƒ¼ ({e.response.status_code}): {filename}")
            logger.error(f"è©³ç´°: {e}")
            if e.response.status_code == 401:
                logger.error("èªè¨¼ã‚¨ãƒ©ãƒ¼: HF_TOKEN ãŒç„¡åŠ¹ã¾ãŸã¯æœŸé™åˆ‡ã‚Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            elif e.response.status_code == 403:
                logger.error("ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦: ãƒªãƒã‚¸ãƒˆãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“")
            raise
        except Exception as e:
            logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {filename} - {type(e).__name__}: {e}")
            raise

    def is_processed(self, doc_id: str) -> bool:
        if self.catalog_df.empty:
            return False
        # doc_id ãŒå­˜åœ¨ã—ã€ã‹ã¤ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ 'success' ã®å ´åˆã®ã¿ã€Œå‡¦ç†æ¸ˆã¿ã€ã¨ã¿ãªã™
        # ã“ã‚Œã«ã‚ˆã‚Šã€pending ã‚„ failure ã®æ›¸é¡ã¯è‡ªå‹•çš„ã«å†å‡¦ç†ã®å¯¾è±¡ã«ãªã‚‹
        processed = self.catalog_df[
            (self.catalog_df["doc_id"] == doc_id) & (self.catalog_df["processed_status"] == "success")
        ]
        return not processed.empty

    def update_catalog(self, new_records: List[Dict]) -> bool:
        """ã‚«ã‚¿ãƒ­ã‚°ã‚’æ›´æ–° (Pydanticãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿæ–½)"""
        if not new_records:
            return True

        validated = []
        for rec in new_records:
            try:
                validated.append(CatalogRecord(**rec).model_dump())
            except Exception as e:
                logger.error(f"ã‚«ã‚¿ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•— (doc_id: {rec.get('doc_id')}): {e}")

        if not validated:
            return False

        new_df = pd.DataFrame(validated)

        # ã€ä¿®æ­£ã€‘ä¸€æ™‚çš„ã«çµåˆã—ãŸDataFrameã‚’ä½œæˆï¼ˆãƒ¡ãƒ¢ãƒªä¸Šã®çŠ¶æ…‹ã¯å¤‰æ›´ã—ãªã„ï¼‰
        temp_catalog = pd.concat([self.catalog_df, new_df], ignore_index=True).drop_duplicates(
            subset=["doc_id"], keep="last"
        )

        # ã€ä¿®æ­£ã€‘ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸæ™‚ã®ã¿ã€ãƒ¡ãƒ¢ãƒªä¸Šã®ã‚«ã‚¿ãƒ­ã‚°ã‚’æ›´æ–°
        if self._save_and_upload("catalog", temp_catalog):
            self.catalog_df = temp_catalog
            logger.success(f"âœ… ã‚«ã‚¿ãƒ­ã‚°æ›´æ–°æˆåŠŸ: {len(validated)} ä»¶")
            return True
        else:
            logger.error("ã‚«ã‚¿ãƒ­ã‚°ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ¡ãƒ¢ãƒªä¸Šã®çŠ¶æ…‹ã‚’ä¿æŒã—ã¾ã™")
            return False

    def _save_and_upload(self, key: str, df: pd.DataFrame, defer: bool = False) -> bool:
        filename = self.paths[key]
        local_file = self.data_path / Path(filename).name

        # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘ä¿å­˜ç›´å‰ã«æœ€çµ‚ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        df = self._clean_dataframe(key, df)

        df.to_parquet(local_file, index=False, compression="zstd")

        if self.api:
            if defer:
                # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ ã—ã¦çµ‚äº† (ãƒ‘ã‚¹ã‚’ã‚­ãƒ¼ã«ã—ã¦æœ€æ–°ã®ã‚‚ã®ã§ä¸Šæ›¸ã)
                self.add_commit_operation(filename, local_file)
                return True

            max_retries = 5  # å¼·åŒ–
            for attempt in range(max_retries):
                try:
                    self.api.upload_file(
                        path_or_fileobj=str(local_file),
                        path_in_repo=filename,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.success(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {filename}")
                    return True
                except Exception as e:
                    # HfHubHTTPErrorã®å‹ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã€429ã®å ´åˆã®ã¿ãƒªãƒˆãƒ©ã‚¤
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(f"Rate limit exceeded. Waiting {wait_time}s before retry ({attempt + 1}/5)...")
                        time.sleep(wait_time)
                        continue

                    # ãã®ä»–ã®HTTPã‚¨ãƒ©ãƒ¼ (5xxç­‰) ã‚‚ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã«ã™ã‚‹
                    if isinstance(e, HfHubHTTPError) and e.response.status_code >= 500:
                        wait_time = 15 * (attempt + 1)
                        logger.warning(
                            f"Master HF Server Error ({e.response.status_code}). "
                            f"Waiting {wait_time}s... ({attempt + 1}/5)"
                        )
                        time.sleep(wait_time)
                        continue

                    logger.warning(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {filename} - {e} - Retrying ({attempt + 1}/5)...")
                    time.sleep(10 * (attempt + 1))
            logger.error(f"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«æœ€çµ‚çš„ã«å¤±æ•—ã—ã¾ã—ãŸ: {filename}")
            return False
        return True

    def upload_raw(self, local_path: Path, repo_path: str, defer: bool = False) -> bool:
        """ãƒ­ãƒ¼ã‚«ãƒ«ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ Hugging Face ã® raw/ ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        if not local_path.exists():
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“: {local_path}")
            return False

        if self.api:
            if defer:
                self.add_commit_operation(repo_path, local_path)
                logger.debug(f"RAWã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {repo_path}")
                return True

            max_retries = 5  # å¼·åŒ–
            for attempt in range(max_retries):
                try:
                    self.api.upload_file(
                        path_or_fileobj=str(local_path),
                        path_in_repo=repo_path,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.debug(f"RAWã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {repo_path}")
                    return True
                except Exception as e:
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(f"Rate limit exceeded for RAW. Waiting {wait_time}s... ({attempt + 1}/5)")
                        time.sleep(wait_time)
                        continue

                    logger.warning(f"RAWã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {repo_path} - {e} - Retrying ({attempt + 1}/5)...")
                    time.sleep(10 * (attempt + 1))
            return False
        return True

    def upload_raw_folder(self, folder_path: Path, path_in_repo: str, defer: bool = False) -> bool:
        """ãƒ•ã‚©ãƒ«ãƒ€å˜ä½ã§ã®ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ãƒªãƒˆãƒ©ã‚¤ä»˜)"""
        if not folder_path.exists():
            return True  # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãªã—ã¯æˆåŠŸã¨ã¿ãªã™

        if self.api:
            if defer:
                # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                for f in folder_path.glob("**/*"):
                    if f.is_file():
                        r_path = f"{path_in_repo}/{f.relative_to(folder_path)}"
                        self._commit_operations[r_path] = CommitOperationAdd(
                            path_in_repo=r_path, path_or_fileobj=str(f)
                        )
                logger.debug(f"RAWãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {path_in_repo}")
                return True

            max_retries = 5  # 3å›ã‹ã‚‰5å›ã«å¼·åŒ–
            for attempt in range(max_retries):
                try:
                    self.api.upload_folder(
                        folder_path=str(folder_path),
                        path_in_repo=path_in_repo,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.success(f"ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {path_in_repo} (from {folder_path})")
                    return True
                except Exception as e:
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(
                            f"Folder Upload Rate limit exceeded. Waiting {wait_time}s... ({attempt + 1}/{max_retries})"
                        )
                        time.sleep(wait_time)
                        continue

                    logger.warning(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {e} - Retrying ({attempt + 1}/{max_retries})...")
                    time.sleep(10)

            logger.error(f"ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•— (Give up): {path_in_repo}")
            return False
        return True

    def update_listing_history(self, new_events: pd.DataFrame) -> bool:
        history = self._load_parquet("listing")

        # åˆå›å®Ÿè¡Œæ™‚ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã›ãšã€ã‚¤ãƒ™ãƒ³ãƒˆã‚‚ç©ºï¼‰ã®å ´åˆã§ã‚‚ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if new_events.empty:
            if history.empty:
                # ç©ºã®å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ä¿å­˜
                return self._save_and_upload("listing", history)
            return True

        history = pd.concat([history, new_events], ignore_index=True).drop_duplicates()
        return self._save_and_upload("listing", history)

    def update_index_history(self, new_events: pd.DataFrame) -> bool:
        history = self._load_parquet("index")

        # åˆå›å®Ÿè¡Œæ™‚ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã›ãšã€ã‚¤ãƒ™ãƒ³ãƒˆã‚‚ç©ºï¼‰ã®å ´åˆã§ã‚‚ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if new_events.empty:
            if history.empty:
                # ç©ºã®å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ä¿å­˜
                return self._save_and_upload("index", history)
            return True

        history = pd.concat([history, new_events], ignore_index=True).drop_duplicates()
        return self._save_and_upload("index", history)

    def get_listing_history(self) -> pd.DataFrame:
        """ç¾åœ¨ã®ä¸Šå ´å±¥æ­´ãƒã‚¹ã‚¿ã‚’å–å¾—"""
        return self._load_parquet("listing")

    def get_index_history(self) -> pd.DataFrame:
        """ç¾åœ¨ã®æŒ‡æ•°æ¡ç”¨å±¥æ­´ãƒã‚¹ã‚¿ã‚’å–å¾—"""
        return self._load_parquet("index")

    def update_stocks_master(self, incoming_data: pd.DataFrame):
        """ãƒã‚¹ã‚¿æ›´æ–° & æ™‚ç³»åˆ—ãƒªã‚³ãƒ³ã‚·ãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ (ä¸–ç•Œæœ€é«˜æ°´æº–ã®æ­´å²å†æ§‹ç¯‰ãƒ­ã‚¸ãƒƒã‚¯)"""
        if incoming_data.empty:
            return True

        # 1. ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨å‹æ­£è¦åŒ–
        records = incoming_data.to_dict("records")
        validated = []
        for rec in records:
            try:
                rec = {k: (v if not pd.isna(v) else None) for k, v in rec.items()}
                # is_active ã®å‹æ­£è¦åŒ–
                if isinstance(rec.get("is_active"), str):
                    rec["is_active"] = rec["is_active"].lower() in ["true", "1", "yes"]
                # ã€æœ€é©è§£ã€‘æƒ…å ±ã®æå¤±ã‚’ä¼´ã†åˆ‡ã‚Šæ¨ã¦ã‚’å»ƒæ­¢ã—ã€ã‚½ãƒ¼ã‚¹ã®ç²¾åº¦ã‚’ç¶­æŒã™ã‚‹
                # (Datetimeå‹ã¸ã®å¤‰æ›ã¯å¾Œç¶šã®ä¿å­˜ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¾ãŸã¯Pydanticãƒ¢ãƒ‡ãƒ«ã«å§”ã­ã‚‹)
                validated.append(StockMasterRecord(**rec).model_dump())
            except Exception as e:
                logger.error(f"éŠ˜æŸ„æƒ…å ±ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•— (code: {rec.get('code')}): {e}")

        if not validated:
            return True
        incoming_df = pd.DataFrame(validated)

        # 2. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®çµ±åˆ (ãƒªã‚³ãƒ³ã‚·ãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³)
        # æ—¢å­˜ãƒã‚¹ã‚¿ã‚’ã€Œéå»ã®çŠ¶æ…‹ã®ä¸€ã¤ã€ã¨ã—ã¦æ‰±ã„ã€å…¨ã¦ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹
        current_m = self.master_df.copy()
        # ã‚«ãƒ©ãƒ è‡ªä½“ã®å­˜åœ¨ã‚’ã‚±ã‚¢ (NULL ã¯ NULL ã®ã¾ã¾ç¶­æŒ)
        if "last_submitted_at" not in current_m.columns:
            current_m["last_submitted_at"] = None

        # å…¨ã¦ã®æ—¢çŸ¥ã®çŠ¶æ…‹ã‚’çµ±åˆ
        # ã€é‡è¦ã€‘ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦çµåˆ
        all_states = pd.concat([current_m, incoming_df], ignore_index=True)

        # é‡è¤‡æ’é™¤ (å±æ€§ã®å¤‰åŒ–ã‚‚ã€Œæ–°ã—ã„è¨¼è¨€ã€ã¨ã—ã¦å—ã‘å…¥ã‚Œã‚‹)
        # ä»¥å‰ã¯ subset=["code", "company_name", "last_submitted_at"] ã®ã¿ã ã£ãŸãŸã‚ã€
        # NULLå±æ€§ã®å¤ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒæœ€æ–°ã®JPXå±æ€§ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦ã„ãŸã€‚
        all_states.drop_duplicates(
            subset=["code", "company_name", "last_submitted_at", "is_active", "sector", "market"], inplace=True
        )

        # 3. ç¤¾åå¤‰æ›´ã®æ­´å²çš„å¤‰é·ã‚’è§£æ
        name_history = self._load_parquet("name")
        new_history_events = []

        processed_codes = set()

        for code, group in all_states.groupby("code"):
            processed_codes.add(code)

            # æå‡ºæ—¥æ™‚ã®æ˜‡é †ã§ã‚½ãƒ¼ãƒˆ (ã“ã‚ŒãŒãªã„ã¨ sorted_group ãŒæœªå®šç¾©ã«ãªã‚‹)
            sorted_group = group.sort_values("last_submitted_at", ascending=True)

            # --- C. æ­´å²ã®å®Œå…¨å†æ§‹ç¯‰ (Full History Rebuild) ---
            # æ—¢å­˜ã®å±¥æ­´ã€ç¾åœ¨ã®ãƒã‚¹ã‚¿ã€æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦ã€Œã‚¤ãƒ™ãƒ³ãƒˆã€ã¨ã—ã¦æ™‚ç³»åˆ—ã«ä¸¦ã¹ç›´ã™

            timeline_events = []

            # 1. æ—¢å­˜ãƒã‚¹ã‚¿ & æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º
            for _, row in sorted_group.iterrows():
                if pd.notna(row.get("last_submitted_at")):
                    timeline_events.append(
                        {"date": row["last_submitted_at"], "name": row["company_name"], "source": "master_or_incoming"}
                    )

            # 2. æ—¢å­˜å±¥æ­´(name_history)ã‹ã‚‰ã®ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º
            # ã“ã‚Œã¾ã§ã®è¨˜éŒ²ã‚‚ã€Œéå»ã®è¨¼è¨€ã€ã¨ã—ã¦æ¡ç”¨ã™ã‚‹
            if not name_history.empty:
                code_hist = name_history[name_history["code"] == code]
                for _, h_row in code_hist.iterrows():
                    timeline_events.append(
                        {"date": h_row["change_date"], "name": h_row["new_name"], "source": "history"}
                    )

            # 3. æ™‚ç³»åˆ—ã‚½ãƒ¼ãƒˆ (å¤ã„é †)
            # æ—¥ä»˜å‹ã¸ã®å¤‰æ›ã¨ã‚½ãƒ¼ãƒˆ
            # (æ³¨æ„: æ–‡å­—åˆ—æ¯”è¼ƒã§ã‚‚ YYYY-MM-DD å½¢å¼ãªã‚‰æ¦‚ã­æ©Ÿèƒ½ã™ã‚‹ãŒã€pd.to_datetimeæ¨å¥¨)
            timeline_events.sort(key=lambda x: str(x["date"]))

            # 4. æ­´å²ã®å†ç”Ÿ (Replay)
            current_tracking_name = None

            # åˆæœŸå€¤ã®æ¨è«–:
            # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®æœ€åˆã®ã‚¤ãƒ™ãƒ³ãƒˆã®ã€Œå‰ã€ã®çŠ¶æ…‹ã¯åˆ†ã‹ã‚‰ãªã„ã€‚
            # ã—ã‹ã—ã€æœ€åˆã®ã‚¤ãƒ™ãƒ³ãƒˆåãŒã€Œæœ€åˆã®åå‰ã€ã§ã‚ã‚‹ã“ã¨ã¯ç¢ºå®šã§ãã‚‹ã€‚

            rebuilt_code_events = []

            for evt in timeline_events:
                evt_name = evt["name"]
                evt_date = evt["date"]

                if current_tracking_name is None:
                    current_tracking_name = evt_name
                    continue

                # æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒ
                norm_curr = self._normalize_company_name(current_tracking_name)
                norm_evt = self._normalize_company_name(evt_name)

                if norm_curr != norm_evt:
                    # å¤‰æ›´æ¤œçŸ¥
                    # éå»ã«è¨˜éŒ²ã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆã¨å…¨ãåŒã˜ã‚‚ã®(æ—¥æ™‚ãƒ»æ–°æ—§å)ã§ã‚ã‚Œã°ã€
                    # é‡è¤‡æ’é™¤ã•ã‚Œã‚‹ãŒã€ã“ã“ã§ã¯æ„å›³çš„ã«ã€Œå†ç”Ÿæˆã€ã™ã‚‹ã€‚
                    rebuilt_code_events.append(
                        {"code": code, "old_name": current_tracking_name, "new_name": evt_name, "change_date": evt_date}
                    )
                    logger.info(f"ğŸ”„ Rebuild History: {code} | {current_tracking_name} -> {evt_name} ({evt_date})")
                    current_tracking_name = evt_name

            # 5. çµæœã®æ ¼ç´ (ãƒ¡ãƒ¢ãƒªä¸Šã®æ›´æ–°)
            # ã“ã®ã‚³ãƒ¼ãƒ‰ã«é–¢ã™ã‚‹æ–°ã—ã„å±¥æ­´ã‚’ç¢ºå®šãƒªã‚¹ãƒˆã«è¿½åŠ 
            # (é‡è¤‡é™¤å¤–ã¯å¾Œç¶šã® drop_duplicates ã§è¡Œã‚ã‚Œã‚‹ãŒã€
            #  å¤ã„èª¤ã£ãŸå±¥æ­´(æœªæ¥->éå»)ã‚’æ¶ˆã™ãŸã‚ã«ã€å¾Œã§ name_history ã‹ã‚‰ã“ã®ã‚³ãƒ¼ãƒ‰åˆ†ã‚’é™¤å¤–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹)
            new_history_events.extend(rebuilt_code_events)

        # 4. å±¥æ­´ã®ä¿å­˜ (Atomic & Non-destructive)
        # å‡¦ç†å¯¾è±¡ã¨ãªã£ãŸã‚³ãƒ¼ãƒ‰(processed_codes)ã«ã¤ã„ã¦ã¯ã€
        # "ã‚¤ãƒ™ãƒ³ãƒˆãªã—" (=ãšã£ã¨åŒã˜åå‰) ã‚‚å«ã‚ã¦ã€ã“ã‚ŒãŒã€Œæœ€æ–°ã®æ­£è§£ã€ã§ã‚ã‚‹ã€‚
        # ã—ãŸãŒã£ã¦ã€æ—¢å­˜ã®å±¥æ­´ã‹ã‚‰ processed_codes ã«è©²å½“ã™ã‚‹ã‚‚ã®ã¯å…¨ã¦å‰Šé™¤ã—ã€
        # ä»Šå›ç”Ÿæˆã•ã‚ŒãŸ new_history_events (ã‚ã‚Œã°) ã§ç½®ãæ›ãˆã‚‹ã€‚

        if processed_codes and not name_history.empty:
            name_history = name_history[~name_history["code"].isin(processed_codes)]

        if new_history_events:
            new_hist_df = pd.DataFrame(new_history_events)
            name_history = pd.concat([name_history, new_hist_df], ignore_index=True)

        if processed_codes:  # å¤‰æ›´ãŒã‚ã£ã¦ã‚‚ãªãã¦ã‚‚ã€ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ï¼ˆå‰Šé™¤ã®åæ˜ ï¼‰ã¯å¿…è¦
            name_history = name_history.drop_duplicates()
            # defer=True ã‚’æŒ‡å®šã—ã¦ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«ç©ã‚€
            self._save_and_upload("name", name_history, defer=True)
            if new_history_events:
                logger.info(f"æ™‚ç³»åˆ—ãƒªã‚³ãƒ³ã‚·ãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³: {len(new_history_events)} ä»¶ã®å¤‰é·ã‚’ç‰¹å®š (Clean Rebuild)")
            else:
                logger.info("æ™‚ç³»åˆ—ãƒªã‚³ãƒ³ã‚·ãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³: å¤‰æ›´ãªã— (å±¥æ­´ã¯ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¾ã—ãŸ)")

        # å…¨çŠ¶æ…‹ã®ä¸­ã‹ã‚‰ã€code ã”ã¨ã«æå‡ºæ—¥æ™‚ãŒæœ€æ–°ã®ã‚‚ã®ã‚’æŠ½å‡º
        sorted_all = all_states.sort_values("last_submitted_at", ascending=False)

        # ã‚»ã‚¯ã‚¿ãƒ¼ã¨å¸‚å ´æƒ…å ±ã®ã€Œå±æ€§ç¶™æ‰¿ï¼ˆInheritanceï¼‰ã€
        # æœ€æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒ NULL ã‚„ "ãã®ä»–" ã®å ´åˆã€éå»ã®æœ‰åŠ¹ãªãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆJPXç­‰ï¼‰ã‹ã‚‰å¼•ãç¶™ã
        def resolve_attr(group, col):
            # æå‡ºæ—¥ã«é–¢ã‚ã‚‰ãšã€ãã®ã‚³ãƒ¼ãƒ‰ã«ãŠã‘ã‚‹ NULL ä»¥å¤–ã®æœ€ã‚‚ç¢ºã‹ãªå€¤ã‚’æ¢ã™
            # (JPXã¯1970å¹´ã ãŒã‚»ã‚¯ã‚¿ãƒ¼æƒ…å ±ã¯ã€Œæ­£ã€ã§ã‚ã‚‹ãŸã‚ã€å…¨ä½“ã‹ã‚‰æ¤œç´¢ã—ã¦è‰¯ã„)
            valid = group[col][~group[col].isin(["ãã®ä»–", None, "nan", ""])]
            return valid.iloc[0] if not valid.empty else None

        # å„ã‚³ãƒ¼ãƒ‰ã®æœ€æ–°çŠ¶æ…‹ã‚’ç‰¹å®šã—ã¤ã¤ã€å±æ€§ã‚’è£œå®Œ
        best_records = []
        for _, group in sorted_all.groupby("code", sort=False):
            # 1. ç‰©ç†çš„ãªæœ€æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾— (ç¤¾åã¨æå‡ºæ—¥æ™‚ã®æ±ºå®šç”¨)
            latest_rec = group.iloc[0].copy()

            # 2. JPXãƒ¬ã‚³ãƒ¼ãƒ‰(æ—¥ä»˜ãªã—)ã‚’ç‰¹å®š (å±æ€§ã®æ­£è§£ãƒ‡ãƒ¼ã‚¿)
            jpx_entries = group[group["last_submitted_at"].isna()]

            if not jpx_entries.empty:
                # JPXãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ä¸»è¦å±æ€§ã‚’JPXã‹ã‚‰å¼·åˆ¶å–å¾—ï¼ˆEDINETå±æ€§ã‚’æ‹’çµ¶ï¼‰
                jpx_rec = jpx_entries.iloc[0]
                latest_rec["sector"] = jpx_rec["sector"]
                latest_rec["market"] = jpx_rec["market"]
                latest_rec["is_active"] = jpx_rec["is_active"]
                # ä¸‡ãŒä¸€ JPX ã®ã‚»ã‚¯ã‚¿ãƒ¼ãŒä¸å…¨ãªå ´åˆã¯ã€éå»ã®æœ‰åŠ¹ãªå±æ€§ã‹ã‚‰æ‹¾ã†ï¼ˆãŸã ã—å„ªå…ˆåº¦ã¯JPXï¼‰
                if latest_rec["sector"] in ["ãã®ä»–", None, "nan", ""]:
                    latest_rec["sector"] = resolve_attr(group, "sector")
            else:
                # JPXã«ä¸€åº¦ã‚‚ç™»éŒ²ã•ã‚ŒãŸã“ã¨ãŒãªã„(å®Œå…¨æ–°è¦ä¸Šå ´ç­‰)ã®å ´åˆ
                # JPXã«ã‚ˆã‚‹æ‰¿èª(åŒæœŸ)ãŒã‚ã‚‹ã¾ã§ã¯ã€Unknown (None) çŠ¶æ…‹ã§éš”é›¢ã™ã‚‹
                latest_rec["is_active"] = None
                latest_rec["sector"] = None
                latest_rec["market"] = None

            best_records.append(latest_rec)

        self.master_df = pd.DataFrame(best_records)

        # defer=True ã‚’æŒ‡å®šã—ã¦ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«ç©ã‚€
        return self._save_and_upload("master", self.master_df, defer=True)

    def get_last_index_list(self, index_name: str) -> pd.DataFrame:
        """æŒ‡å®šæŒ‡æ•°ã®æ§‹æˆéŠ˜æŸ„ã‚’å–å¾— (Phase 3ç”¨)"""
        return pd.DataFrame(columns=["code"])

    def get_sector(self, code: str) -> str:
        """è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æ¥­ç¨®å–å¾—"""
        if self.master_df.empty:
            return None
        row = self.master_df[self.master_df["code"] == code]
        if not row.empty:
            val = row.iloc[0]["sector"]
            return str(val) if val is not None else None
        return None

    def save_delta(
        self, key: str, df: pd.DataFrame, run_id: str, chunk_id: str, custom_filename: str = None, defer: bool = False
    ) -> bool:
        """ãƒ‡ãƒ«ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (Workerç”¨)"""
        if df.empty:
            return True

        if custom_filename:
            filename = custom_filename
        else:
            filename = f"{Path(self.paths[key]).stem}.parquet"

        delta_path = f"temp/deltas/{run_id}/{chunk_id}/{filename}"
        local_file = self.data_path / f"delta_{run_id}_{chunk_id}_{filename}"

        # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘ä¿å­˜ç›´å‰ã«æœ€çµ‚ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        df = self._clean_dataframe(key, df)

        df.to_parquet(local_file, index=False, compression="zstd")

        return self.upload_raw(local_file, delta_path, defer=defer)

    def mark_chunk_success(self, run_id: str, chunk_id: str, defer: bool = False) -> bool:
        """ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æˆåŠŸãƒ•ãƒ©ã‚° (_SUCCESS) ã‚’ä½œæˆ (Workerç”¨)"""
        success_path = f"temp/deltas/{run_id}/{chunk_id}/_SUCCESS"
        local_file = self.data_path / f"SUCCESS_{run_id}_{chunk_id}"
        local_file.touch()

        return self.upload_raw(local_file, success_path, defer=defer)

    def load_deltas(self, run_id: str) -> Dict[str, pd.DataFrame]:
        """å…¨ãƒ‡ãƒ«ã‚¿ã‚’åé›†ã—ã¦ãƒãƒ¼ã‚¸ (Mergerç”¨)"""
        if not self.api:
            logger.warning("APIåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ãƒ‡ãƒ«ã‚¿åé›†ä¸å¯")
            return {}

        deltas = {}

        try:
            # ã€æ•´åˆæ€§å¼·åŒ–ã€‘HF Hub ã®ãƒªã‚¹ãƒˆå–å¾—è‡ªä½“ã‚’ãƒªãƒˆãƒ©ã‚¤ã—ã€åæ˜ é…å»¶ã«å¯¾å‡¦
            folder = f"temp/deltas/{run_id}"
            files = []
            for attempt in range(3):
                files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type="dataset")
                target_files = [f for f in files if f.startswith(folder)]
                if target_files:
                    break
                logger.warning(f"ãƒ‡ãƒ«ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å†è©¦è¡Œä¸­... ({attempt + 1}/3)")
                time.sleep(10)

            # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            chunks = {}
            for f in target_files:
                parts = f.split("/")
                if len(parts) < 4:
                    continue
                chunk_id = parts[3]
                if chunk_id not in chunks:
                    chunks[chunk_id] = []
                chunks[chunk_id].append(f)

            # _SUCCESS ãŒã‚ã‚‹ãƒãƒ£ãƒ³ã‚¯ã®ã¿å‡¦ç†
            valid_chunks = 0
            for chunk_id, file_list in chunks.items():
                if not any(f.endswith("_SUCCESS") for f in file_list):
                    # ã€æ•´åˆæ€§å¼·åŒ–ã€‘HF Hubã®çµæœæ•´åˆæ€§ã‚’è€ƒæ…®ã—ã€1å›è¦‹ã¤ã‹ã‚‰ãªãã¦ã‚‚
                    # åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—ã‚’è©¦ã¿ã‚‹ã“ã¨ãŒæœ›ã¾ã—ã„ãŒã€ã“ã“ã§ã¯ä¸€æ—¦è­¦å‘Šã«ç•™ã‚ã‚‹
                    logger.warning(f"âš ï¸ æœªå®Œäº†ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—: {chunk_id}")
                    continue

                valid_chunks += 1
                for remote_path in file_list:
                    if remote_path.endswith("_SUCCESS"):
                        continue

                    # ã‚­ãƒ¼åˆ¤åˆ¥
                    fname = Path(remote_path).name
                    key = None
                    if fname == "documents_index.parquet":
                        key = "catalog"
                    elif fname == "stocks_master.parquet":
                        key = "master"
                    elif fname == "listing_history.parquet":
                        key = "listing"
                    elif fname == "index_history.parquet":
                        key = "index"
                    elif fname == "name_history.parquet":
                        key = "name"
                    elif fname.startswith("financial_values_bin"):
                        bin_id = fname.replace("financial_values_bin", "").replace(".parquet", "")
                        key = f"financial_bin{bin_id}"
                    elif fname.startswith("qualitative_text_bin"):
                        bin_id = fname.replace("qualitative_text_bin", "").replace(".parquet", "")
                        key = f"text_bin{bin_id}"
                    elif fname.startswith("financial_values_"):
                        sector = fname.replace("financial_values_", "").replace(".parquet", "")
                        key = f"financial_{sector}"
                    elif fname.startswith("qualitative_text_"):
                        sector = fname.replace("qualitative_text_", "").replace(".parquet", "")
                        key = f"text_{sector}"

                    if key:
                        attempts = 2
                        for att in range(attempts):
                            try:
                                local_path = hf_hub_download(
                                    repo_id=self.hf_repo, filename=remote_path, repo_type="dataset", token=self.hf_token
                                )
                                df = pd.read_parquet(local_path)
                                if key not in deltas:
                                    deltas[key] = []
                                deltas[key].append(df)
                                break
                            except Exception as e:
                                if att == attempts - 1:
                                    logger.error(f"âŒ ãƒ‡ãƒ«ã‚¿èª­ã¿è¾¼ã¿å¤±æ•— ({remote_path}): {e}")
                                    raise
                                logger.warning(f"ãƒ‡ãƒ«ã‚¿èª­ã¿è¾¼ã¿å†è©¦è¡Œä¸­... ({att + 1}) {remote_path}")
                                time.sleep(5)

            logger.info(f"æœ‰åŠ¹ãªãƒãƒ£ãƒ³ã‚¯æ•°: {valid_chunks} / {len(chunks)}")

            # ãƒãƒ¼ã‚¸çµæœã‚’è¿”ã™
            merged = {}
            for key, df_list in deltas.items():
                if df_list:
                    # å…¨ã¦ã®DFã®ã‚«ãƒ©ãƒ ã‚’å…±é€šåŒ–ï¼ˆå‹ä¸æ•´åˆå¯¾ç­–ï¼‰
                    merged[key] = pd.concat(df_list, ignore_index=True)
                else:
                    merged[key] = pd.DataFrame()
            return merged

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ«ã‚¿åé›†å¤±æ•—: {e}")
            return {}

    def push_commit(self, message: str = "Batch update from ARIA") -> bool:
        """
        ãƒãƒƒãƒ•ã‚¡ã«æºœã¾ã£ãŸæ“ä½œã‚’ã‚³ãƒŸãƒƒãƒˆå®Ÿè¡Œã€‚
        ã€ç©¶æ¥µã®å®‰å®šåŒ–ã€‘æ“ä½œæ•°ãŒå¤šã„å ´åˆã¯ã€HFå´ã®è² è·ã¨429ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã€è‡ªå‹•çš„ã«åˆ†å‰²ã—ã¦ã‚³ãƒŸãƒƒãƒˆã™ã‚‹ã€‚
        """
        if not self.api or not self._commit_operations:
            return True

        ops_list = list(self._commit_operations.values())
        total_ops = len(ops_list)

        # 1ã‚³ãƒŸãƒƒãƒˆã‚ãŸã‚Šã®æœ€å¤§æ“ä½œæ•°
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ (128å›/æ™‚) ã‚’å›é¿ã™ã‚‹ãŸã‚ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ‹¡å¤§ã—ã¦ã‚³ãƒŸãƒƒãƒˆå›æ•°ã‚’å‰Šæ¸›ã™ã‚‹
        # HFå´ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãªã„ã‚®ãƒªã‚®ãƒªã®ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ 500ä»¶ç¨‹åº¦ãŒæœ€é©
        # ã€ä¿®æ­£ã€‘Hugging Face API åˆ¶é™ (128 req/hour) ã¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ã®ãŸã‚ã€
        # GHAä¸¦åˆ—æ•°(20) ã‚’è€ƒæ…®ã—ã¦ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ 200 ã«ç¸®å°ã—ã€åˆè¨ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’æŠ‘åˆ¶ã™ã‚‹ã€‚
        # (600 files / 200 = 3 commits * 20 jobs = 60 req < 128 req)
        batch_size = 200

        batches = [ops_list[i : i + batch_size] for i in range(0, total_ops, batch_size)]

        logger.info(f"ğŸš€ ã‚³ãƒŸãƒƒãƒˆé€ä¿¡é–‹å§‹: åˆè¨ˆ {total_ops} æ“ä½œã‚’ {len(batches)} ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦å®Ÿè¡Œã—ã¾ã™")

        for i, batch in enumerate(batches):
            batch_msg = f"{message} (part {i + 1}/{len(batches)})"
            max_retries = 12
            success = False

            for attempt in range(max_retries):
                try:
                    # ã€é‡è¦ã€‘create_commit ã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒé‡ã„ãŸã‚ã€å€‹åˆ¥ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
                    # (ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã£ã¦ã¯ç›´æ¥å¼•æ•°ã‚’å–ã‚‰ãªã„å ´åˆãŒã‚ã‚‹ãŸã‚ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³å´ã§ä¿è­·)
                    self.api.create_commit(
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        operations=batch,
                        commit_message=batch_msg,
                        token=self.hf_token,
                    )
                    success = True
                    break
                except Exception as e:
                    status_code = getattr(getattr(e, "response", None), "status_code", None)

                    # 429 ãƒ¬ãƒ¼ãƒˆåˆ¶é™ ã¾ãŸã¯ 500 ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
                    if status_code in [429, 500]:
                        # 429ã®å ´åˆã¯ã‚ˆã‚Šé•·ãå¾…æ©Ÿ (HFã®å›å¾©ã‚’å¾…ã¤)
                        wait_time = int(getattr(e.response.headers, "get", lambda x, y: y)("Retry-After", 60))
                        wait_time = max(wait_time, 60) + (attempt * 30) + random.uniform(5, 15)
                        logger.warning(
                            f"HF Server Error ({status_code}). Waiting {wait_time:.1f}s... "
                            f"(Batch {i + 1}, Attempt {attempt + 1}/{max_retries})"
                        )
                        time.sleep(wait_time)
                        continue

                    # 409 ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ ã¾ãŸã¯ 412 å‰ææ¡ä»¶å¤±æ•—
                    if status_code in [409, 412]:
                        wait_time = (2 ** (attempt + 2)) + (random.uniform(10, 30))
                        logger.warning(
                            f"Commit Conflict ({status_code}). Retrying in {wait_time:.2f}s... "
                            f"(Batch {i + 1}, Attempt {attempt + 1}/{max_retries})"
                        )
                        time.sleep(wait_time)
                        continue

                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç­‰ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¾‹å¤–
                    wait_time = (attempt + 1) * 20 + random.uniform(5, 15)
                    logger.warning(
                        f"é€šä¿¡ã‚¨ãƒ©ãƒ¼ ({e}): {wait_time:.1f}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™... "
                        f"(Batch {i + 1}, Attempt {attempt + 1}/{max_retries})"
                    )
                    time.sleep(wait_time)

            if not success:
                logger.error(f"âŒ ãƒãƒƒãƒ {i + 1} ã®é€ä¿¡ã«æœ€çµ‚çš„ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                return False

            # ãƒãƒƒãƒé–“ã«çŸ­ã„ä¼‘æ†©ã‚’æŒŸã‚“ã§HFå´ã®è² è·ã‚’é€ƒãŒã™
            if i < len(batches) - 1:
                time.sleep(random.uniform(3, 7))

        logger.success(f"âœ… å…¨ {total_ops} æ“ä½œã®ãƒãƒƒãƒã‚³ãƒŸãƒƒãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        self._commit_operations = {}  # ã‚¯ãƒªã‚¢
        return True

    def cleanup_deltas(self, run_id: str, cleanup_old: bool = True):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (Mergerç”¨)"""
        if not self.api:
            return

        try:
            files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type="dataset")
            delta_root = "temp/deltas"

            # å‰Šé™¤å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            delete_files = []

            if cleanup_old:
                # 24æ™‚é–“ä»¥ä¸ŠçµŒéã—ãŸã‚‚ã®ã‚’å¯¾è±¡ã¨ã™ã‚‹
                now = time.time()
                expired_runs = set()

                for f in files:
                    if not f.startswith(delta_root):
                        continue
                    parts = f.split("/")
                    if len(parts) < 3:
                        continue
                    r_id = parts[2]

                    try:
                        timestamp = int(r_id)
                        if (now - timestamp) > 86400:  # 24æ™‚é–“ä»¥ä¸Š
                            delete_files.append(f)
                            expired_runs.add(r_id)
                    except ValueError:
                        pass

                if delete_files:
                    logger.info(f"å¤ã„ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¸…æƒä¸­... (24æ™‚é–“ä»¥ä¸ŠçµŒé: {len(expired_runs)} runs)")

            else:
                # ä»Šå›ã®ãƒ©ãƒ³IDã®ã¿å¯¾è±¡
                target_prefix = f"{delta_root}/{run_id}"
                delete_files = [f for f in files if f.startswith(target_prefix)]
                if delete_files:
                    logger.info(f"ä»Šå›ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ä¸­... {run_id} ({len(delete_files)} files)")

            if not delete_files:
                return

            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ‹¡å¤§ (50 -> 500) ã—ã¦APIã‚³ãƒ¼ãƒ«æ•°ã‚’å‰Šæ¸›
            batch_size = 500
            total_batches = (len(delete_files) + batch_size - 1) // batch_size

            for i in range(0, len(delete_files), batch_size):
                batch = delete_files[i : i + batch_size]
                del_ops = [CommitOperationDelete(path_in_repo=p) for p in batch]

                batch_num = (i // batch_size) + 1
                commit_msg = f"Cleanup deltas (Batch {batch_num}/{total_batches})"

                # ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ (Backoff)
                max_retries = 10
                success = False
                for attempt in range(max_retries):
                    try:
                        self.api.create_commit(
                            repo_id=self.hf_repo,
                            repo_type="dataset",
                            operations=del_ops,
                            commit_message=commit_msg,
                            token=self.hf_token,
                        )
                        success = True
                        break
                    except Exception as e:
                        if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                            wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                            logger.warning(
                                f"Cleanup Rate limit exceeded. Waiting {wait_time}s... "
                                f"(Batch {batch_num}/{total_batches}, Attempt {attempt + 1})"
                            )
                            time.sleep(wait_time)
                            continue

                        logger.warning(
                            f"Cleanup error: {e}. Retrying... "
                            f"(Batch {batch_num}/{total_batches}, Attempt {attempt + 1})"
                        )
                        time.sleep(10 * (attempt + 1))

                if success:
                    logger.debug(f"Cleanup batch {batch_num}/{total_batches} done.")
                    if batch_num < total_batches:
                        time.sleep(2)  # ãƒãƒƒãƒé–“ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³
                else:
                    logger.error(f"âŒ Cleanup batch {batch_num} failed permanently.")

            logger.success("Cleanup sequence completed.")

        except Exception as e:
            logger.error(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å…¨ä½“å¤±æ•—: {e}")
