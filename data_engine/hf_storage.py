"""
Hugging Face Storage Layer â€” ARIA ãƒ‡ãƒ¼ã‚¿ã®æ°¸ç¶šåŒ–ãƒ»èª­ã¿è¾¼ã¿ãƒ»ã‚³ãƒŸãƒƒãƒˆã‚’ä¸€å…ƒç®¡ç†ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚

CatalogManager ã‹ã‚‰åˆ†é›¢ã•ã‚ŒãŸç´”ç²‹ãª I/O é–¢å¿ƒäº‹:
- Parquet ã®èª­ã¿è¾¼ã¿ / ä¿å­˜ / ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
- RAW ãƒ•ã‚¡ã‚¤ãƒ« (ZIP/PDF) ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
- ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã¨ãƒãƒƒãƒã‚³ãƒŸãƒƒãƒˆ
"""

import random
import time
from pathlib import Path
from typing import Dict

import pandas as pd
import requests
from huggingface_hub import CommitOperationAdd, HfApi, hf_hub_download
from huggingface_hub.utils import EntryNotFoundError, HfHubHTTPError, RepositoryNotFoundError
from loguru import logger
from models import ARIA_SCHEMAS, CatalogRecord, IndexEvent, ListingEvent, StockMasterRecord


class HfStorage:
    """Hugging Face Hub ã¨ã®é€šä¿¡ãƒ»æ°¸ç¶šåŒ–ã‚’æ‹…å½“ã™ã‚‹ I/O å±¤"""

    def __init__(self, hf_repo: str, hf_token: str, data_path: Path, paths: Dict[str, str]):
        """
        Args:
            hf_repo: Hugging Face ãƒªãƒã‚¸ãƒˆãƒª ID
            hf_token: Hugging Face API ãƒˆãƒ¼ã‚¯ãƒ³
            data_path: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            paths: å†…éƒ¨ã‚­ãƒ¼ã‹ã‚‰ãƒªãƒã‚¸ãƒˆãƒªãƒ‘ã‚¹ã¸ã®å¯¾å¿œè¡¨ (e.g., {"catalog": "catalog/documents_index.parquet"})
        """
        self.hf_repo = hf_repo
        self.hf_token = hf_token
        self.data_path = data_path
        self.paths = paths

        # ã€ä¿®æ­£ã€‘é€šä¿¡å®‰å®šæ€§å‘ä¸Šã®ãŸã‚ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
        import os

        if hf_repo and hf_token:
            os.environ["HF_HUB_TIMEOUT"] = "300"
            os.environ["HF_HUB_HTTP_TIMEOUT"] = "300"
            self.api = HfApi(token=hf_token)
        else:
            self.api = None

        # ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡: {repo_path: CommitOperationAdd or (DataFrame, local_path)}
        self._commit_operations: Dict = {}

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Parquet èª­ã¿è¾¼ã¿
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def load_parquet(self, key: str, clean_fn=None, force_download: bool = False) -> pd.DataFrame:
        """
        HF ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ Parquet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã€‚
        ãƒãƒƒãƒ•ã‚¡ã«ä¿ç•™ä¸­ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’å„ªå…ˆã™ã‚‹ (Read-Your-Writes)ã€‚

        Args:
            key: å†…éƒ¨ã‚­ãƒ¼ ("catalog", "master" ç­‰)
            clean_fn: DataFrame ã«é©ç”¨ã™ã‚‹ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°é–¢æ•° (optional)
            force_download: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹
        """
        filename = self.paths[key]

        # ã€é‡è¦: Lost Update é˜²æ­¢ã€‘ä¿ç•™ä¸­ã®ã‚³ãƒŸãƒƒãƒˆï¼ˆãƒ¡ãƒ¢ãƒªä¸Šï¼‰ãŒã‚ã‚Œã°ã€ãƒªãƒ¢ãƒ¼ãƒˆã‚ˆã‚Šå„ªå…ˆã™ã‚‹
        if filename in self._commit_operations:
            data = self._commit_operations[filename]
            logger.debug(f"ãƒ¡ãƒ¢ãƒªä¸Šã®ä¿ç•™ä¸­ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã«ä½¿ç”¨ã—ã¾ã™: {filename}")
            return data[0] if isinstance(data, tuple) else data

        try:
            local_path = hf_hub_download(
                repo_id=self.hf_repo,
                filename=filename,
                repo_type="dataset",
                token=self.hf_token,
                force_download=force_download,
            )
            df = pd.read_parquet(local_path)
            # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘èª­ã¿è¾¼ã¿ç›´å¾Œã«ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
            if clean_fn:
                df = clean_fn(key, df)
            logger.debug(f"ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {filename} ({len(df)} rows)")
            return df
        except RepositoryNotFoundError:
            logger.error(f"ãƒªãƒã‚¸ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.hf_repo}")
            logger.error("ç’°å¢ƒå¤‰æ•° HF_REPO ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            raise
        except (EntryNotFoundError, requests.exceptions.HTTPError) as e:
            is_404 = isinstance(e, EntryNotFoundError) or (
                hasattr(e, "response") and e.response is not None and e.response.status_code == 404
            )

            if not is_404:
                raise e

            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚æ–°è¦ä½œæˆã—ã¾ã™: {filename}")
            if key == "catalog":
                cols = list(CatalogRecord.model_fields.keys())
                return pd.DataFrame(columns=cols)
            elif key == "master":
                cols = list(StockMasterRecord.model_fields.keys())
                return pd.DataFrame(columns=cols)
            elif key == "listing":
                cols = list(ListingEvent.model_fields.keys())
                return pd.DataFrame(columns=cols)
            elif key == "index":
                cols = list(IndexEvent.model_fields.keys())
                return pd.DataFrame(columns=cols)
            elif key == "name":
                return pd.DataFrame(columns=["code", "old_name", "new_name", "change_date"])
            return pd.DataFrame()
        except HfHubHTTPError as e:
            logger.error(f"HF API ã‚¨ãƒ©ãƒ¼ ({e.response.status_code}): {filename}")
            logger.error(f"è©³ç´°: {e}")
            if e.response.status_code == 401:
                logger.error("èªè¨¼ã‚¨ãƒ©ãƒ¼: HF_TOKEN ãŒç„¡åŠ¹ã¾ãŸã¯æœŸé™åˆ‡ã‚Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            elif e.response.status_code == 403:
                logger.error("ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦: ãƒªãƒã‚¸ãƒˆãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“")
            raise
        except Exception as e:
            logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {filename} - {type(e).__name__}: {e}")
            raise

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Parquet ä¿å­˜ & ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def save_and_upload(self, key: str, df: pd.DataFrame, clean_fn=None, defer: bool = False) -> bool:
        """Parquet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ã—ã€HF ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚"""
        filename = self.paths[key]
        local_file = self.data_path / filename
        local_file.parent.mkdir(parents=True, exist_ok=True)

        # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘ä¿å­˜ç›´å‰ã«æœ€çµ‚ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        if clean_fn:
            df = clean_fn(key, df)

        # ã€Phase 3: é‡‘å‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‘æ˜ç¤ºã‚¹ã‚­ãƒ¼ãƒã§å‹ãƒ–ãƒ¬ã‚’ç‰©ç†çš„ã«æ’é™¤
        schema = ARIA_SCHEMAS.get(key)
        df.to_parquet(local_file, index=False, compression="zstd", schema=schema)

        if self.api:
            if defer:
                # ã€é‡è¦ã€‘ãƒãƒƒãƒ•ã‚¡ã«ã¯ (DataFrame, ç‰©ç†ãƒ‘ã‚¹) ã‚’ä¿æŒã™ã‚‹
                # ã“ã‚Œã«ã‚ˆã‚Š load_parquet ã§ã®å†åˆ©ç”¨ (Read-Your-Writes) ã‚’å¯èƒ½ã«ã™ã‚‹
                self._commit_operations[filename] = (df, local_file)
                logger.debug(f"ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {filename}")
                return True

            return self._upload_with_retry(str(local_file), filename)
        return True

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # RAW ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def upload_raw(self, local_path: Path, repo_path: str, defer: bool = False) -> bool:
        """ãƒ­ãƒ¼ã‚«ãƒ«ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ Hugging Face ã® raw/ ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        if not local_path.exists():
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“: {local_path}")
            return False

        if self.api:
            if defer:
                self.add_commit_operation(repo_path, local_path)
                logger.debug(f"RAWã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {repo_path}")
                return True

            return self._upload_with_retry(str(local_path), repo_path)
        return True

    def upload_raw_folder(self, folder_path: Path, path_in_repo: str, defer: bool = False) -> bool:
        """ãƒ•ã‚©ãƒ«ãƒ€å˜ä½ã§ã®ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ãƒªãƒˆãƒ©ã‚¤ä»˜)"""
        if not folder_path.exists():
            return True  # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãªã—ã¯æˆåŠŸã¨ã¿ãªã™

        if self.api:
            if defer:
                for f in folder_path.glob("**/*"):
                    if f.is_file():
                        r_path = f"{path_in_repo}/{f.relative_to(folder_path)}"
                        self._commit_operations[r_path] = CommitOperationAdd(
                            path_in_repo=r_path, path_or_fileobj=str(f)
                        )
                logger.debug(f"RAWãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {path_in_repo}")
                return True

            max_retries = 5
            for attempt in range(max_retries):
                try:
                    self.api.upload_folder(
                        folder_path=str(folder_path),
                        path_in_repo=path_in_repo,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.success(f"ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {path_in_repo} (from {folder_path})")
                    return True
                except Exception as e:
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(
                            f"Folder Upload Rate limit exceeded. Waiting {wait_time}s... ({attempt + 1}/{max_retries})"
                        )
                        time.sleep(wait_time)
                        continue
                    logger.warning(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {e} - Retrying ({attempt + 1}/{max_retries})...")
                    time.sleep(10)

            logger.error(f"ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•— (Give up): {path_in_repo}")
            return False
        return True

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡æ“ä½œ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def add_commit_operation(self, repo_path: str, local_path: Path):
        """ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«æ“ä½œã‚’è¿½åŠ ï¼ˆé‡è¤‡ã¯æœ€æ–°ã§ä¸Šæ›¸ãï¼‰"""
        self._commit_operations[repo_path] = CommitOperationAdd(path_in_repo=repo_path, path_or_fileobj=str(local_path))
        logger.debug(f"ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {repo_path}")

    @property
    def has_pending_operations(self) -> bool:
        """ä¿ç•™ä¸­ã®ã‚³ãƒŸãƒƒãƒˆæ“ä½œãŒã‚ã‚‹ã‹ã©ã†ã‹"""
        return bool(self._commit_operations)

    def clear_operations(self):
        """ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹"""
        self._commit_operations = {}

    def push_commit(self, message: str = "Batch update from ARIA") -> bool:
        """
        ãƒãƒƒãƒ•ã‚¡ã«æºœã¾ã£ãŸæ“ä½œã‚’ã‚³ãƒŸãƒƒãƒˆå®Ÿè¡Œã€‚
        æ“ä½œæ•°ãŒå¤šã„å ´åˆã¯ã€HFå´ã®è² è·ã¨429ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã€è‡ªå‹•çš„ã«åˆ†å‰²ã—ã¦ã‚³ãƒŸãƒƒãƒˆã™ã‚‹ã€‚
        """
        if not self.api or not self._commit_operations:
            return True

        # ãƒãƒƒãƒ•ã‚¡å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’ CommitOperationAdd ã«å¤‰æ›
        ops_list = []
        for repo_path, data in self._commit_operations.items():
            if isinstance(data, tuple):
                _, local_path = data
                ops_list.append(CommitOperationAdd(path_in_repo=repo_path, path_or_fileobj=str(local_path)))
            else:
                ops_list.append(data)

        total_ops = len(ops_list)

        # 1ã‚³ãƒŸãƒƒãƒˆã‚ãŸã‚Šã®æœ€å¤§æ“ä½œæ•°
        # HF APIåˆ¶é™ (128 req/hour) ã¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ã®ãŸã‚ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’200ã«è¨­å®š
        # (600 files / 200 = 3 commits * 20 jobs = 60 req < 128 req)
        batch_size = 200

        batches = [ops_list[i : i + batch_size] for i in range(0, total_ops, batch_size)]

        logger.info(f"ğŸš€ ã‚³ãƒŸãƒƒãƒˆé€ä¿¡é–‹å§‹: åˆè¨ˆ {total_ops} æ“ä½œã‚’ {len(batches)} ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦å®Ÿè¡Œã—ã¾ã™")

        for i, batch in enumerate(batches):
            batch_msg = f"{message} (part {i + 1}/{len(batches)})"
            max_retries = 12
            success = False

            for attempt in range(max_retries):
                try:
                    self.api.create_commit(
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        operations=batch,
                        commit_message=batch_msg,
                        token=self.hf_token,
                    )
                    success = True
                    break
                except BaseException as e:
                    if isinstance(e, Exception):
                        status_code = getattr(getattr(e, "response", None), "status_code", None)

                        # 429 ãƒ¬ãƒ¼ãƒˆåˆ¶é™ ã¾ãŸã¯ 500 ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
                        if status_code in [429, 500]:
                            wait_time = int(getattr(e.response.headers, "get", lambda x, y: y)("Retry-After", 60))
                            wait_time = max(wait_time, 60) + (attempt * 30) + random.uniform(5, 15)
                            logger.warning(
                                f"HF Server Error ({status_code}). Waiting {wait_time:.1f}s... "
                                f"(Batch {i + 1}, Attempt {attempt + 1}/{max_retries})"
                            )
                            time.sleep(wait_time)
                            continue

                        # 409 ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ ã¾ãŸã¯ 412 å‰ææ¡ä»¶å¤±æ•—
                        if status_code in [409, 412]:
                            wait_time = (2 ** (attempt + 1)) * 5 + (random.uniform(10, 60))
                            logger.warning(
                                f"Commit Conflict ({status_code}). Retrying in {wait_time:.2f}s... "
                                f"(Batch {i + 1}, Attempt {attempt + 1}/{max_retries})"
                            )
                            time.sleep(wait_time)
                            continue

                        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç­‰ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¾‹å¤–
                        wait_time = (attempt + 1) * 20 + random.uniform(5, 15)
                        logger.warning(
                            f"é€šä¿¡ã‚¨ãƒ©ãƒ¼ ({e}): {wait_time:.1f}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™... "
                            f"(Batch {i + 1}, Attempt {attempt + 1}/{max_retries})"
                        )
                        time.sleep(wait_time)
                    else:
                        logger.critical(
                            f"âš ï¸ ãƒ—ãƒ­ã‚»ã‚¹ãŒã‚·ã‚°ãƒŠãƒ«ã¾ãŸã¯è‡´å‘½çš„ãªä¾‹å¤–ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ: {type(e).__name__}"
                        )
                        raise e

            if not success:
                logger.error(f"âŒ ãƒãƒƒãƒ {i + 1} ã®é€ä¿¡ã«æœ€çµ‚çš„ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                return False

            # ãƒãƒƒãƒé–“ã«çŸ­ã„ä¼‘æ†©ã‚’æŒŸã‚“ã§HFå´ã®è² è·ã‚’é€ƒãŒã™
            if i < len(batches) - 1:
                time.sleep(random.uniform(3, 7))

        logger.success(f"âœ… å…¨ {total_ops} æ“ä½œã®ãƒãƒƒãƒã‚³ãƒŸãƒƒãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        self._commit_operations = {}  # ã‚¯ãƒªã‚¢
        return True

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # å†…éƒ¨ãƒ˜ãƒ«ãƒ‘ãƒ¼
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _upload_with_retry(self, local_path_str: str, repo_path: str, max_retries: int = 5) -> bool:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
        for attempt in range(max_retries):
            try:
                self.api.upload_file(
                    path_or_fileobj=local_path_str,
                    path_in_repo=repo_path,
                    repo_id=self.hf_repo,
                    repo_type="dataset",
                    token=self.hf_token,
                )
                logger.success(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {repo_path}")
                return True
            except Exception as e:
                if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                    wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                    logger.warning(f"Rate limit exceeded. Waiting {wait_time}s... ({attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue

                if isinstance(e, HfHubHTTPError) and e.response.status_code >= 500:
                    wait_time = 15 * (attempt + 1)
                    logger.warning(
                        f"HF Server Error ({e.response.status_code}). "
                        f"Waiting {wait_time}s... ({attempt + 1}/{max_retries})"
                    )
                    time.sleep(wait_time)
                    continue

                logger.warning(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {repo_path} - {e} - Retrying ({attempt + 1}/{max_retries})...")
                time.sleep(10 * (attempt + 1))

        logger.error(f"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«æœ€çµ‚çš„ã«å¤±æ•—ã—ã¾ã—ãŸ: {repo_path}")
        return False
