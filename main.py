import argparse
import json
import os
import signal
import sys
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime, timedelta
from pathlib import Path

# HF Hub ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’éè¡¨ç¤ºã«ã™ã‚‹ (GHAãƒ­ã‚°ã®è¦–èªæ€§å‘ä¸Šã®ãŸã‚)
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"

import pandas as pd
from dotenv import load_dotenv
from loguru import logger

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from catalog_manager import CatalogManager
from edinet_engine import EdinetEngine

# ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (å‹•çš„ãƒ‘ã‚¹è¿½åŠ ã‚’å»ƒæ­¢ã—ã€æ­£è¦ã®éšå±¤ã§æŒ‡å®š)
from edinet_xbrl_prep.edinet_xbrl_prep.fs_tbl import get_fs_tbl
from master_merger import MasterMerger

# è¨­å®š
DATA_PATH = Path("data").resolve()
RAW_BASE_DIR = DATA_PATH / "raw"
TEMP_DIR = DATA_PATH / "temp"
PARALLEL_WORKERS = 4
BATCH_PARALLEL_SIZE = 8

is_shutting_down = False


def signal_handler(sig, frame):
    global is_shutting_down
    logger.warning("ä¸­æ–­ä¿¡å·ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã—ã¦ã„ã¾ã™...")
    is_shutting_down = True


signal.signal(signal.SIGINT, signal_handler)


def parse_worker(args):
    """ä¸¦åˆ—å‡¦ç†ç”¨ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°"""
    docid, row, acc_obj, raw_zip, role_kws, task_type = args
    # ã€ä¿®æ­£ã€‘ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã”ã¨ã«å€‹åˆ¥ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã€ç«¶åˆï¼ˆRace Conditionï¼‰ã‚’å›é¿
    extract_dir = TEMP_DIR / f"{docid}_{task_type}"
    try:
        if acc_obj is None:
            return docid, None, "Account list not loaded"

        logger.debug(f"è§£æé–‹å§‹: {docid} (Path: {raw_zip})")

        # é–‹ç™ºè€…ãƒ–ãƒ­ã‚°æ¨å¥¨ã® get_fs_tbl ã‚’å‘¼ã³å‡ºã—
        df = get_fs_tbl(
            account_list_common_obj=acc_obj,
            docid=docid,
            zip_file_str=str(raw_zip),
            temp_path_str=str(extract_dir),
            role_keyward_list=role_kws,
        )

        if df is not None and not df.empty:
            df["docid"] = docid
            df["code"] = str(row.get("secCode", ""))[:4]
            df["submitDateTime"] = row.get("submitDateTime", "")
            for col in df.columns:
                if df[col].dtype == "object":
                    df[col] = df[col].astype(str)
            logger.debug(f"è§£ææˆåŠŸ: {docid} ({task_type}) | æŠ½å‡ºãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df)}")
            return docid, df, None, task_type

        msg = "No objects to concatenate" if (df is None or df.empty) else "Empty Results"
        return docid, None, msg, task_type

    except Exception as e:
        import traceback

        err_detail = traceback.format_exc()
        logger.error(f"è§£æä¾‹å¤–: {docid} ({task_type})\n{err_detail}")
        return docid, None, f"{str(e)}", task_type
    finally:
        if extract_dir.exists():
            import shutil

            shutil.rmtree(extract_dir)


def run_merger(catalog, merger, run_id):
    """Mergerãƒ¢ãƒ¼ãƒ‰: ãƒ‡ãƒ«ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®é›†ç´„ã¨Globalæ›´æ–° (Master First æˆ¦ç•¥)"""
    logger.info(f"=== Merger Process Started (Run ID: {run_id}) ===")

    # 1. ãƒã‚¦ã‚¹ã‚­ãƒ¼ãƒ”ãƒ³ã‚° (å¤ã„ãƒ‡ãƒ«ã‚¿ã®å‰Šé™¤ - å¸¸ã«å®Ÿè¡Œã—ã¦OK)
    catalog.cleanup_deltas(run_id, cleanup_old=True)

    # 2. æœ‰åŠ¹ãªãƒ‡ãƒ«ã‚¿ã®åé›†
    deltas = catalog.load_deltas(run_id)
    if not deltas:
        logger.warning("æœ‰åŠ¹ãªãƒ‡ãƒ«ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ (No valid chunks found).")
        # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦çµ‚äº†ï¼ˆä½•ã‚‚æ›´æ–°ã—ã¦ã„ãªã„ã®ã§å®‰å…¨ï¼‰
        catalog.cleanup_deltas(run_id, cleanup_old=False)
        # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã§ã‚‚ã€å¾Œç¶šã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ­ã‚¸ãƒƒã‚¯ã§å‡¦ç†ã•ã‚Œã‚‹ã‚ˆã†ã« return ã‚’å‰Šé™¤
        # catalog.cleanup_deltas(run_id, cleanup_old=False) # ã“ã®è¡Œã¯å‰Šé™¤

    # 3. ãƒãƒ¼ã‚¸ã¨Globalæ›´æ–° (Strategy: Master/Data First -> Catalog Last)
    # ã“ã‚Œã«ã‚ˆã‚Šã€CatalogãŒé€²ã‚“ã§MasterãŒæ›´æ–°ã•ã‚Œãªã„ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ­ã‚¹ãƒˆï¼‰ã‚’é˜²ãã€‚

    all_masters_success = True
    processed_count = 0

    # --- A. Stock Master ---
    if "master" in deltas and not deltas["master"].empty:
        processed_count += 1
        new_master = deltas["master"]
        # JPXæœ€æ–°ãƒªã‚¹ãƒˆã«ã‚ã‚‹ã‚³ãƒ¼ãƒ‰ã®ã‚»ãƒƒãƒˆ
        active_codes = set(new_master["code"])

        # æ—¢å­˜ãƒã‚¹ã‚¿ã® is_active ã‚’æ›´æ–°
        current_master = catalog.master_df.copy()
        if not current_master.empty:
            if "is_active" not in current_master.columns:
                current_master["is_active"] = True  # ã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯åˆæœŸåŒ–

            # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
            current_master["is_active"] = current_master["code"].isin(active_codes)

        # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã® is_active ã¯å½“ç„¶ True
        new_master["is_active"] = True

        # ãƒãƒ¼ã‚¸ (æ—¢å­˜ã®çŠ¶æ…‹æ›´æ–°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ + æ–°è¦ãƒ‡ãƒ¼ã‚¿)
        merged_master = pd.concat([current_master, new_master], ignore_index=True).drop_duplicates(
            subset=["code"], keep="last"
        )

        # recã‚«ãƒ©ãƒ ãªã©ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
        if "rec" in merged_master.columns:
            merged_master.drop(columns=["rec"], inplace=True)

        # å‹ã‚’å¼·åˆ¶ï¼ˆæ–‡å­—åˆ—åŒ–ã‚’é˜²ãï¼‰
        if merged_master["is_active"].dtype == "object":
            # 'False' (æ–‡å­—åˆ—) ã‚’ç¢ºå®Ÿã« False (bool) ã«ã™ã‚‹ãŸã‚ã« lowercase ã§åˆ¤å®š
            merged_master["is_active"] = (
                merged_master["is_active"]
                .astype(str)
                .str.lower()
                .map(
                    {
                        "true": True,
                        "false": False,
                        "1": True,
                        "0": False,
                        "1.0": True,
                        "0.0": False,
                        "none": True,
                        "nan": True,
                    }
                )
                .fillna(True)
                .astype(bool)
            )
        else:
            merged_master["is_active"] = merged_master["is_active"].astype(bool)

        if catalog._save_and_upload("master", merged_master):
            logger.success(
                f"Global Stock Master Updated (Active: {merged_master['is_active'].astype(int).sum()} / "
                f"Total: {len(merged_master)})"
            )
        else:
            logger.error("âŒ Failed to update Global Stock Master")
            all_masters_success = False

    # --- B. History (listing, index, name) ---
    for key in ["listing", "index", "name"]:
        if key in deltas and not deltas[key].empty:
            processed_count += 1
            new_hist = deltas[key]
            current_hist = catalog._load_parquet(key)
            merged_hist = pd.concat([current_hist, new_hist], ignore_index=True).drop_duplicates()
            if catalog._save_and_upload(key, merged_hist):
                logger.success(f"Global {key} History Updated")
            else:
                logger.error(f"âŒ Failed to update Global {key} History")
                all_masters_success = False

    # --- C. Financial / Text (Sector based) ---
    for key, sector_df in deltas.items():
        if (key.startswith("financial_") or key.startswith("text_")) and not sector_df.empty:
            processed_count += 1
            # ã‚­ãƒ¼ã‹ã‚‰è­˜åˆ¥å­(bin ã¾ãŸã¯ sector)ã‚’å¾©å…ƒ
            identifier = key.replace("financial_", "").replace("text_", "")
            m_type = "financial_values" if key.startswith("financial_") else "qualitative_text"

            # MasterMerger (Standard Mode) ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            # ã‚»ã‚¯ã‚¿ãƒ¼å¼•æ•°ã¯ bin è­˜åˆ¥å­ã‚’æ¸¡ã™ãŒã€Mergerå†…éƒ¨ã§ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰binã‚’å†è¨ˆç®—ã™ã‚‹ãŸã‚æ•´åˆæ€§ã¯ç¶­æŒã•ã‚Œã‚‹
            if not merger.merge_and_upload(identifier, m_type, sector_df, worker_mode=False):
                logger.error(f"âŒ Failed to update {m_type} for {identifier}")
                all_masters_success = False

    # --- D. Catalog Update (Commit Log) ---
    catalog_updated = False
    if all_masters_success:
        if "catalog" in deltas and not deltas["catalog"].empty:
            processed_count += 1
            new_df = deltas["catalog"]
            merged_catalog = pd.concat([catalog.catalog_df, new_df], ignore_index=True).drop_duplicates(
                subset=["doc_id"], keep="last"
            )
            if catalog._save_and_upload("catalog", merged_catalog):
                logger.success(f"Global Catalog Updated (Total: {len(merged_catalog)} rows)")
                catalog_updated = True
            else:
                logger.error("âŒ Failed to update Global Catalog")
        else:
            catalog_updated = True  # Catalogè‡ªä½“ã®æ›´æ–°ãŒãªã„å ´åˆã¯æˆåŠŸã¨ã¿ãªã™
    else:
        logger.error("â›” Masteræ›´æ–°ã«å¤±æ•—ãŒã‚ã‚‹ãŸã‚ã€Catalogæ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ (ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ä¿è­·)")

    if processed_count == 0:
        logger.info("å‡¦ç†å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®ã¿å®Ÿè¡Œã—ã¾ã™ã€‚")

    # 4. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    # æ›´æ–°ã«æˆåŠŸã—ãŸã‹ã€ã‚ã‚‹ã„ã¯ãã‚‚ãã‚‚å‡¦ç†å¯¾è±¡ãŒãªã‹ã£ãŸå ´åˆã«å®Ÿè¡Œ
    if all_masters_success and catalog_updated:
        catalog.cleanup_deltas(run_id, cleanup_old=False)
        logger.info("=== Merger Process Completed Successfully ===")
    else:
        logger.warning("âš ï¸ å‡¦ç†ã®å¤±æ•—ï¼ˆã¾ãŸã¯æ•´åˆæ€§ä¸å‚™ï¼‰ã‚’æ¤œçŸ¥ã—ãŸãŸã‚ã€ä»Šå›ã®Deltaãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿æŒã—ã¾ã™ã€‚")
        logger.info("=== Merger Process Completed with Errors ===")


def main():
    # .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    load_dotenv()

    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š
    log_level = os.getenv("LOG_LEVEL", "INFO").upper()
    logger.remove()
    logger.add(sys.stderr, level=log_level)

    # åŸå› è¿½è·¡ã®ãŸã‚ã€å—ã‘å–ã£ãŸç”Ÿã®å¼•æ•°ã‚’ãƒ­ã‚°ã«å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    logger.debug(f"èµ·å‹•å¼•æ•°: {sys.argv}")

    parser = argparse.ArgumentParser(description="Integrated Disclosure Data Lakehouse 2.0")
    parser.add_argument("--start", type=str, help="YYYY-MM-DD")
    parser.add_argument("--end", type=str, help="YYYY-MM-DD")
    # ãƒã‚¤ãƒ•ãƒ³å½¢å¼ã¨ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢å½¢å¼ã®ä¸¡æ–¹ã‚’å—ã‘å…¥ã‚Œã€destã‚’çµ±ä¸€
    parser.add_argument("--id-list", "--id_list", type=str, dest="id_list", help="Comma separated docIDs", default=None)
    parser.add_argument("--list-only", action="store_true", help="Output metadata as JSON for GHA matrix")

    # ã€è¿½åŠ ã€‘Worker/Mergerãƒ¢ãƒ¼ãƒ‰åˆ¶å¾¡ç”¨
    parser.add_argument("--mode", type=str, default="worker", choices=["worker", "merger"], help="Execution mode")
    parser.add_argument("--run-id", type=str, dest="run_id", help="Execution ID for delta isolation")
    parser.add_argument(
        "--chunk-id", type=str, dest="chunk_id", default="default", help="Chunk ID for parallel workers"
    )

    try:
        args = parser.parse_args()
    except SystemExit as e:
        if e.code != 0:
            logger.error(f"å¼•æ•°è§£æã‚¨ãƒ©ãƒ¼ (exit code {e.code}): æ¸¡ã•ã‚ŒãŸå¼•æ•°ãŒä¸æ­£ã§ã™ã€‚ sys.argv={sys.argv}")
        raise e

    api_key = os.getenv("EDINET_API_KEY")
    hf_token = os.getenv("HF_TOKEN")
    hf_repo = os.getenv("HF_REPO")

    # å®Ÿè¡ŒIDã®è‡ªå‹•ç”Ÿæˆ (æŒ‡å®šãŒãªã„å ´åˆ)
    run_id = args.run_id or datetime.now().strftime("%Y%m%d_%H%M%S")
    chunk_id = args.chunk_id

    if not api_key:
        logger.critical("EDINET_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    if not args.start:
        args.start = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
    if not args.end:
        args.end = datetime.now().strftime("%Y-%m-%d")

    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    logger.add(log_dir / "pipeline_{time}.log", rotation="10 MB", level="INFO")

    # ã‚¿ã‚¯ã‚½ãƒãƒŸURLå®šç¾©ã®èª­ã¿è¾¼ã¿
    taxonomy_urls_path = Path("taxonomy_urls.json")
    taxonomy_urls = {}
    if taxonomy_urls_path.exists():
        try:
            with open(taxonomy_urls_path, "r", encoding="utf-8") as f:
                taxonomy_urls = json.load(f)
            logger.info(f"ã‚¿ã‚¯ã‚½ãƒãƒŸURLå®šç¾©ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(taxonomy_urls)} ä»¶")
        except Exception as e:
            logger.error(f"ã‚¿ã‚¯ã‚½ãƒãƒŸURLå®šç¾©ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    else:
        logger.warning(f"ã‚¿ã‚¯ã‚½ãƒãƒŸURLå®šç¾©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {taxonomy_urls_path}")

    edinet = EdinetEngine(api_key, DATA_PATH, taxonomy_urls=taxonomy_urls)
    catalog = CatalogManager(hf_repo, hf_token, DATA_PATH)
    merger = MasterMerger(hf_repo, hf_token, DATA_PATH)

    # ã€è¿½åŠ ã€‘Mergerãƒ¢ãƒ¼ãƒ‰åˆ†å²
    if args.mode == "merger":
        run_merger(catalog, merger, run_id)
        return

    # Workerãƒ¢ãƒ¼ãƒ‰ï¼ˆä»¥ä¸‹ã€æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã ãŒDeltaä¿å­˜ã«å¤‰æ›´ï¼‰

    # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
    all_meta = edinet.fetch_metadata(args.start, args.end)
    if not all_meta:
        if args.list_only:
            print("JSON_MATRIX_DATA: []")
        return

    # ã€ãƒ‡ãƒãƒƒã‚°ã€‘fetch_metadata ã‹ã‚‰è¿”ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
    # if all_meta:
    #     first_meta = all_meta[0]
    #     logger.info(f"ğŸ” main.py ã§å—ã‘å–ã£ãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰ã®ã‚­ãƒ¼æ•°: {len(first_meta)}")
    #     logger.info(f"ğŸ” main.py ã§å—ã‘å–ã£ãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ä¸€è¦§: {list(first_meta.keys())}")
    #     logger.info(f"ğŸ” main.py ã§å—ã‘å–ã£ãŸ periodStart: {first_meta.get('periodStart')}")
    #     logger.info(f"ğŸ” main.py ã§å—ã‘å–ã£ãŸ periodEnd: {first_meta.get('periodEnd')}")

    # ã€æŠ•è³‡ç‰¹åŒ–ã€‘è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ãŒãªã„ï¼ˆéä¸Šå ´ä¼æ¥­ï¼‰ã‚’å³åº§ã«é™¤å¤–
    initial_count = len(all_meta)

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç†ç”±ã®è¿½è·¡ãƒ­ã‚°
    filtered_meta = []
    skipped_reasons = {"no_sec_code": 0, "invalid_length": 0}

    for row in all_meta:
        sec_code = str(row.get("secCode", "")).strip()
        if not sec_code:
            skipped_reasons["no_sec_code"] += 1
            continue
        if len(sec_code) < 5:
            skipped_reasons["invalid_length"] += 1
            # 56ä»¶ã®æ›¸é¡æ¼ã‚Œãªã©ã®è¿½è·¡ç”¨
            logger.debug(f"æ›¸é¡ã‚¹ã‚­ãƒƒãƒ— (ã‚³ãƒ¼ãƒ‰çŸ­ç¸®): {row.get('docID')} - {sec_code}")
            continue
        filtered_meta.append(row)

    all_meta = filtered_meta
    if initial_count > len(all_meta) and not args.id_list:
        logger.info(
            f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ: åˆæœŸ {initial_count} ä»¶ -> ä¿æŒ {len(all_meta)} ä»¶ "
            f"(è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ãªã—: {skipped_reasons['no_sec_code']} ä»¶, "
            f"ã‚³ãƒ¼ãƒ‰ä¸æ­£/çŸ­ç¸®: {skipped_reasons['invalid_length']} ä»¶)"
        )

    # 2. GHAãƒãƒˆãƒªãƒƒã‚¯ã‚¹ç”¨å‡ºåŠ›
    if args.list_only:
        matrix_data = []
        for row in all_meta:
            docid = row["docID"]
            if catalog.is_processed(docid):
                continue

            # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿ã®è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
            raw_sec_code = str(row.get("secCode", "")).strip()[:4]
            # è§£æå¯¾è±¡ã®å³å¯†åˆ¤å®šæ¡ä»¶ã‚’ãƒãƒˆãƒªãƒƒã‚¯ã‚¹å´ã§ã‚‚æä¾›
            matrix_data.append(
                {
                    "id": docid,
                    "code": raw_sec_code,
                    "xbrl": row.get("xbrlFlag") == "1",
                    "type": row.get("docTypeCode"),
                    "ord": row.get("ordinanceCode"),
                    "form": row.get("formCode"),
                }
            )
        print(f"JSON_MATRIX_DATA: {json.dumps(matrix_data)}")
        return

    logger.info("=== Data Lakehouse 2.0 å®Ÿè¡Œé–‹å§‹ ===")

    # 4. å‡¦ç†å¯¾è±¡ã®é¸å®š
    tasks = []
    new_catalog_records = []  # ãƒãƒƒãƒä¿å­˜ç”¨ï¼ˆè§£æå¯¾è±¡å¤–ã®ã¿ï¼‰
    # ã€ã‚«ã‚¿ãƒ­ã‚°æ•´åˆæ€§ã€‘ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç›´å¾Œã§ã¯ãªãã€è§£æçµæœã«åŸºã¥ãç™»éŒ²ã™ã‚‹ã‚ˆã†è¨­è¨ˆå¤‰æ›´
    potential_catalog_records = {}  # docid -> record_base (å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ä¿æŒ)
    parsing_target_ids = set()  # è§£æå¯¾è±¡ã®docidã‚»ãƒƒãƒˆ

    # ã€RAWæ•´åˆæ€§ã€‘ãƒãƒƒãƒå‡¦ç†ç”¨
    current_batch_docids = []

    loaded_acc = {}
    skipped_types = {}  # æ–°ãŸã«è¿½åŠ 

    target_ids = args.id_list.split(",") if args.id_list else None

    # è§£æã‚¿ã‚¹ã‚¯ã®è¿½åŠ  (XBRL ãŒã‚ã‚‹ Yuho/Shihanki ã®ã¿)
    # é–‹ç™ºè€…ãƒ–ãƒ­ã‚°ã®æŒ‡å®š + è¿½åŠ ãƒ­ãƒ¼ãƒ« (CF, SS, Notes)
    # ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆè€…ã®è¨˜äº‹ã«åŸºã¥ããƒ­ãƒ¼ãƒ«å®šç¾©ã®é©æ­£åŒ–
    fs_dict = {
        "BS": ["_BalanceSheet", "_ConsolidatedBalanceSheet"],
        "PL": ["_StatementOfIncome", "_ConsolidatedStatementOfIncome"],
        "CF": ["_StatementOfCashFlows", "_ConsolidatedStatementOfCashFlows"],
        "SS": ["_StatementOfChangesInEquity", "_ConsolidatedStatementOfChangesInEquity"],
        "notes": ["_Notes", "_ConsolidatedNotes"],
        "report": ["_CabinetOfficeOrdinanceOnDisclosure"],
    }

    # ã€ä¿®æ­£ã€‘å®šé‡ãƒ‡ãƒ¼ã‚¿ã¨å®šæ€§ãƒ‡ãƒ¼ã‚¿ã®åˆ†é›¢ã‚’é©æ­£åŒ–
    # å®šæ€§æ³¨è¨˜ï¼ˆnotesï¼‰ã¯ qualitative_text ã¸ã€è²¡å‹™è«¸è¡¨æœ¬ä½“ã¯ financial_values ã¸
    quant_roles = fs_dict["BS"] + fs_dict["PL"] + fs_dict["CF"] + fs_dict["SS"]
    text_roles = fs_dict["report"] + fs_dict["notes"]

    for row in all_meta:
        docid = row["docID"]
        title = row.get("docDescription", "åç§°ä¸æ˜")

        if target_ids and docid not in target_ids:
            continue
        if not target_ids and catalog.is_processed(docid):
            continue

        y, m = row["submitDateTime"][:4], row["submitDateTime"][5:7]
        # ã€ä¿®æ­£ã€‘ãƒ‘ã‚¹æœ€é©åŒ–: raw/edinet/year=YYYY/month=MM/
        raw_dir = RAW_BASE_DIR / "edinet" / f"year={y}" / f"month={m}"
        raw_dir.mkdir(parents=True, exist_ok=True)
        raw_zip = raw_dir / f"{docid}.zip"
        raw_pdf = raw_dir / f"{docid}.pdf"

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ (ãƒ•ãƒ©ã‚°ã«åŸºã¥ãæ­£ç¢ºã«è©¦è¡Œ)
        has_xbrl = row.get("xbrlFlag") == "1"
        has_pdf = row.get("pdfFlag") == "1"

        zip_ok = False
        if has_xbrl:
            zip_ok = edinet.download_doc(docid, raw_zip, 1)
            if zip_ok:
                pass
            else:
                logger.error(f"XBRLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {docid} | {title}")

        pdf_ok = False
        if has_pdf:
            pdf_ok = edinet.download_doc(docid, raw_pdf, 2)
            if pdf_ok:
                pass
            else:
                logger.error(f"PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {docid} | {title}")

        file_status = []
        if has_xbrl:
            file_status.append("XBRLã‚ã‚Š" if zip_ok else "XBRL(DLå¤±æ•—)")
        if has_pdf:
            file_status.append("PDFã‚ã‚Š" if pdf_ok else "PDF(DLå¤±æ•—)")
        status_str = " + ".join(file_status) if file_status else "ãƒ•ã‚¡ã‚¤ãƒ«ãªã—"

        # è§£æã‚¿ã‚¹ã‚¯åˆ¤å®šç”¨ã®ã‚³ãƒ¼ãƒ‰ã‚’å…ˆã«å–å¾—
        dtc = row.get("docTypeCode")
        ord_c = row.get("ordinanceCode")
        form_c = row.get("formCode")

        # æœŸæœ«æ—¥ãƒ»æ±ºç®—å¹´åº¦ãƒ»æ±ºç®—æœŸé–“ï¼ˆæœˆæ•°ï¼‰ã®æŠ½å‡º
        period_start = row.get("periodStart")
        period_end = row.get("periodEnd")

        # ã€ãƒ‡ãƒãƒƒã‚°ã€‘ã‚«ã‚¿ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆæ™‚ã® period ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªï¼ˆæœ€åˆã®1ä»¶ã®ã¿ï¼‰
        # if len(potential_catalog_records) == 0:
        #     logger.info(f"ğŸ” ã‚«ã‚¿ãƒ­ã‚°ä½œæˆæ™‚ã® row ã‚­ãƒ¼ä¸€è¦§: {list(row.keys())}")
        #     logger.info(f"ğŸ” ã‚«ã‚¿ãƒ­ã‚°ä½œæˆæ™‚ã® periodStart: {period_start}")
        #     logger.info(f"ğŸ” ã‚«ã‚¿ãƒ­ã‚°ä½œæˆæ™‚ã® periodEnd: {period_end}")
        #     logger.info(f"ğŸ” ã‚«ã‚¿ãƒ­ã‚°ä½œæˆæ™‚ã® docID: {docid}")

        fiscal_year = int(period_end[:4]) if period_end else None

        # æ±ºç®—æœŸã®æœˆæ•°ã‚’ç®—å‡º (å¤‰å‰‡æ±ºç®—å¯¾å¿œ)
        num_months = 12
        if period_start and period_end:
            try:
                d1 = datetime.strptime(period_start, "%Y-%m-%d")
                d2 = datetime.strptime(period_end, "%Y-%m-%d")
                # æ—¥æ•°å·®ã‚’æœˆæ•°ã«æ›ç®— (+1æ—¥ã—ã¦ç«¯æ•°ã‚’ä¸¸ã‚ã‚‹)
                diff_days = (d2 - d1).days + 1
                num_months = round(diff_days / 30.4375)  # 365.25 / 12
                # é€šå¸¸ã¯ 12, 9, 6, 3 ãªã©ã«åæŸã€‚å¢ƒç•Œå€¤ã‚¬ãƒ¼ãƒ‰
                num_months = max(1, min(24, num_months))
            except Exception:
                pass

        sec_code = row.get("secCode", "")[:4]
        # è¨‚æ­£ãƒ•ãƒ©ã‚°
        is_amendment = row.get("withdrawalStatus") != "0" or "è¨‚æ­£" in title

        # ã‚«ã‚¿ãƒ­ã‚°æƒ…å ±ã®ãƒ™ãƒ¼ã‚¹ã‚’ä¿æŒ (models.py ã®å®šç¾©é †ã«æº–æ‹ )
        record = {
            "doc_id": docid,
            "code": sec_code,
            "company_name": row.get("filerName", "Unknown"),
            "fiscal_year": fiscal_year,
            "period_start": period_start,
            "period_end": period_end,
            "num_months": num_months,
            "is_amendment": is_amendment,
            "doc_type": dtc or "",
            "form_code": form_c,
            "ordinance_code": ord_c,
            "submit_at": row.get("submitDateTime", ""),
            "title": title,
            "edinet_code": row.get("edinetCode", ""),
            "jcn": row.get("JCN"),
            "fund_code": row.get("fundCode"),
            "issuer_edinet_code": row.get("issuerEdinetCode"),
            "subject_edinet_code": row.get("subjectEdinetCode"),
            "subsidiary_edinet_code": row.get("subsidiaryEdinetCode"),
            "current_report_reason": row.get("currentReportReason"),
            "parent_doc_id": row.get("parentDocID"),
            "ope_date_time": row.get("opeDateTime"),
            "withdrawal_status": row.get("withdrawalStatus", "0"),
            "doc_info_edit_status": row.get("docInfoEditStatus", "0"),
            "disclosure_status": row.get("disclosureStatus", "0"),
            "xbrl_flag": row.get("xbrlFlag", "0"),
            "pdf_flag": row.get("pdfFlag", "0"),
            "attach_doc_flag": row.get("attachDocFlag", "0"),
            "english_doc_flag": row.get("englishDocFlag", "0"),
            "csv_flag": row.get("csvFlag", "0"),
            "legal_status": row.get("legalStatus", "0"),
            "raw_zip_path": f"raw/edinet/year={y}/month={m}/{docid}.zip" if zip_ok else "",
            "pdf_path": f"raw/edinet/year={y}/month={m}/{docid}.pdf" if pdf_ok else "",
            "processed_status": "success" if (zip_ok or pdf_ok) else "failure",
            "source": "EDINET",
        }
        potential_catalog_records[docid] = record

        # è§£æå¯¾è±¡ã®å³å¯†åˆ¤å®š: æœ‰å ±(120) / ä¸€èˆ¬äº‹æ¥­ç”¨(010) / æ¨™æº–æ§˜å¼(030000)
        is_target_yuho = dtc == "120" and ord_c == "010" and form_c == "030000"

        if is_target_yuho and zip_ok:
            try:
                # ã€é‡è¦ã€‘æå‡ºæ—¥ã§ã¯ãªãã€XBRLå†…éƒ¨ã®åå‰ç©ºé–“ã‹ã‚‰ã€Œäº‹æ¥­å¹´åº¦ã€åŸºæº–ã§å¹´ã‚’ç‰¹å®š
                import shutil

                from edinet_xbrl_prep.edinet_xbrl_prep.fs_tbl import linkbasefile

                # åˆ¤å®šç”¨ã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆè‡ªå‹•å‰Šé™¤ç”¨ï¼‰
                detect_dir = TEMP_DIR / f"detect_{docid}"
                lb = linkbasefile(zip_file_str=str(raw_zip), temp_path_str=str(detect_dir))
                lb.read_linkbase_file()
                ty = lb.detect_account_list_year()

                if ty == "-":
                    raise ValueError(f"XBRLåå‰ç©ºé–“ã‹ã‚‰äº‹æ¥­å¹´åº¦ï¼ˆã‚¿ã‚¯ã‚½ãƒãƒŸç‰ˆï¼‰ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚({docid})")

                if ty not in loaded_acc:
                    acc = edinet.get_account_list(ty)
                    if not acc:
                        raise ValueError(
                            f"ã‚¿ã‚¯ã‚½ãƒãƒŸç‰ˆ '{ty}' ã¯ taxonomy_urls.json ã«æœªå®šç¾©ã€ã¾ãŸã¯å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                        )
                    loaded_acc[ty] = acc

                # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ã‚¿ã‚¹ã‚¯
                tasks.append((docid, row, loaded_acc[ty], raw_zip, quant_roles, "financial_values"))
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¿ã‚¹ã‚¯
                tasks.append((docid, row, loaded_acc[ty], raw_zip, text_roles, "qualitative_text"))

                parsing_target_ids.add(docid)
                logger.info(f"ã€è§£æå¯¾è±¡ã€‘: {docid} | äº‹æ¥­å¹´åº¦: {ty} | {title}")
            except Exception as e:
                logger.error(f"ã€è§£æä¸­æ­¢ã€‘ã‚¿ã‚¯ã‚½ãƒãƒŸåˆ¤å®šå¤±æ•— ({docid}): {e}")
                record["processed_status"] = "failure"
                new_catalog_records.append(record)
                continue
            finally:
                if detect_dir.exists():
                    shutil.rmtree(detect_dir)
        else:
            # ã€æ”¹å–„ã€‘ã‚¹ã‚­ãƒƒãƒ—ç†ç”±ã‚’ã‚ˆã‚Šæ˜ç¢ºã«
            form_name = "ç‰¹å®šæœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸" if form_c == "080000" else "éè§£æå¯¾è±¡"
            codes_info = f"[Type:{dtc}, Ord:{ord_c}, Form:{form_c}]"
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæœ‰å ±ãªã®ã«XBRLãŒãªã„å ´åˆã€ã¾ãŸã¯å˜ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤–ã§ã‚ã‚‹å ´åˆã‚’åŒºåˆ¥
            reason = f"XBRLãªã— {codes_info}" if is_target_yuho else f"{form_name} {codes_info}"

            logger.info(f"ã€ã‚¹ã‚­ãƒƒãƒ—æ¸ˆã¨ã—ã¦è¨˜éŒ²ã€‘: {docid} | {title} | {status_str} | ç†ç”±: {reason}")
            skipped_types[dtc] = skipped_types.get(dtc, 0) + 1
            new_catalog_records.append(record)

        # ãƒãƒƒãƒç®¡ç†ç”¨ã«IDã‚’è¿½åŠ 
        current_batch_docids.append(docid)

        # 50ä»¶ã”ã¨ã«é€²æ—ã‚’å ±å‘Š
        processed_count = len(potential_catalog_records)
        if not args.id_list and processed_count % 50 == 0:
            logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—: {processed_count} / {len(all_meta)} ä»¶å®Œäº†")

            # ã€Smart Batchingã€‘ä»¥å‰ã¯50ä»¶ã”ã¨ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã—ãŸãŒã€
            # HFã®ã‚³ãƒŸãƒƒãƒˆåˆ¶é™(128å›/æ™‚)ã‚’å›é¿ã™ã‚‹ãŸã‚ã€Workerã§ã¯æœ€å¾Œã«ä¸€åº¦ã ã‘è¡Œã„ã¾ã™ã€‚
            pass

    # ã€ä¿®æ­£ã€‘ã‚«ã‚¿ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ã®åé›†ã‚’ä¸€æœ¬åŒ–ã—ã€ä¸Šæ›¸ããƒ­ã‚¹ãƒˆã‚’é˜²æ­¢
    # ä»¥å‰ã¯ã€Œè§£æå¯¾è±¡å¤–ã€ã¨ã€Œè§£æå¯¾è±¡ã€ã§åˆ¥ã€…ã« save_delta ã‚’å‘¼ã‚“ã§ãŠã‚Šã€å¾Œè€…ãŒå‰è€…ã‚’ä¸Šæ›¸ãã—ã¦ã„ãŸ
    final_catalog_records = []
    final_catalog_records.extend(new_catalog_records)

    # 5. ä¸¦åˆ—è§£æ
    all_quant_dfs = []
    all_text_dfs = []  # ãƒ†ã‚­ã‚¹ãƒˆç”¨
    processed_infos = []

    if tasks:
        logger.info(f"è§£æå¯¾è±¡: {len(tasks) // 2} æ›¸é¡ (Taskæ•°: {len(tasks)})")

        # ã€ä¿®æ­£ã€‘è§£æã‚¹ã‚­ãƒƒãƒ—ã®å†…è¨³ã‚’ç²¾æŸ»ã€‚
        # å³å¯†ã« ã€Œæœ‰å ±(120)/ä¸€èˆ¬äº‹æ¥­ä¼šç¤¾(010)/æ¨™æº–æ§˜å¼(030000)ã€ ãŒæ¼ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è­¦å‘Šã€‚
        unexpected_skips = []
        for did in potential_catalog_records:
            rec = potential_catalog_records[did]
            # é‡è¦åˆ¤å®š: 120 / 010 / 030000 ãªã®ã« tasks ã«å…¥ã£ã¦ã„ãªã„ã‚‚ã®ã‚’è­¦å‘Šå¯¾è±¡ã¨ã™ã‚‹
            is_missed = (
                rec["doc_type"] == "120"
                and rec["ordinance_code"] == "010"
                and rec["form_code"] == "030000"
                and did not in parsing_target_ids
            )
            if is_missed:
                unexpected_skips.append(did)

        if unexpected_skips:
            logger.warning(f"æ³¨æ„: æœ€å„ªå…ˆè§£æå¯¾è±¡(120/010/030000)ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¦ã„ã¾ã™: {unexpected_skips}")

        TEMP_DIR.mkdir(parents=True, exist_ok=True)
        with ProcessPoolExecutor(max_workers=PARALLEL_WORKERS) as executor:
            for i in range(0, len(tasks), BATCH_PARALLEL_SIZE):
                if is_shutting_down:
                    break
                batch = tasks[i : i + BATCH_PARALLEL_SIZE]
                futures = [executor.submit(parse_worker, t) for t in batch]

                for f in as_completed(futures):
                    did, res_df, err, t_type = f.result()

                    # è§£æå®Œäº†å¾Œã« parsing_target_ids ã«ã‚ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
                    if did in potential_catalog_records:
                        target_rec = potential_catalog_records[did]

                        if err:
                            logger.error(f"è§£æçµæœ({t_type}): {did} - {err}")
                            # ä¸¡æ–¹ã®ã‚¿ã‚¹ã‚¯(financial/text)ãŒå¤±æ•—ã—ãŸå ´åˆã®ã¿failureã¨ã™ã‚‹ç­‰ã®å³å¯†ã•ã¯ä¸€æ—¦ç½®ããŒã€
                            # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°è­¦å‘Šãƒ¬ãƒ™ãƒ«ã‚’å¼•ãä¸Šã’ã‚‹
                            if "No objects to concatenate" not in err:
                                target_rec["processed_status"] = "failure"
                        elif res_df is not None:
                            # å°‘ãªãã¨ã‚‚ä¸€æ–¹ã®è§£æã«æˆåŠŸã—ã¦ã„ã‚Œã°æˆåŠŸ
                            target_rec["processed_status"] = "success"

                            if t_type == "financial_values":
                                quant_only = res_df[res_df["isTextBlock_flg"] == 0]
                                if not quant_only.empty:
                                    all_quant_dfs.append(quant_only)
                            elif t_type == "qualitative_text":
                                txt_only = res_df[res_df["isTextBlock_flg"] == 1]
                                if not txt_only.empty:
                                    all_text_dfs.append(txt_only)

                            # ã‚»ã‚¯ã‚¿ãƒ¼åˆ¤åˆ¥ç”¨
                            meta_row = next(m for m in all_meta if m["docID"] == did)
                            processed_infos.append(
                                {"docID": did, "sector": catalog.get_sector(meta_row.get("secCode", "")[:4])}
                            )

                # ãƒãƒƒãƒã”ã¨ã«ç™»éŒ²å¯èƒ½ãªæœªå®šè¨˜éŒ²ã‚’ç™»éŒ²
                # è§£æå¯¾è±¡ã®ã‚«ã‚¿ãƒ­ã‚°ç™»éŒ²ã¯æœ€å¾Œã«ã¾ã¨ã‚ã¦è¡Œã†ãŸã‚ã€ã“ã“ã§ã¯ä½•ã‚‚ã—ãªã„
                pass

                done_count = i + len(batch)
                logger.info(
                    f"è§£æé€²æ—: {min(done_count, len(tasks))} / {len(tasks)} tasks å®Œäº† "
                    f"(Quant: {len(all_quant_dfs)}, Text: {len(all_text_dfs)})"
                )
    new_catalog_records = []

    # 6. ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ã‚¸ & ã‚«ã‚¿ãƒ­ã‚°ç¢ºå®š
    all_success = True
    # binãƒªã‚¹ãƒˆ (é‡è¤‡æ’é™¤)
    processed_df = pd.DataFrame(processed_infos)
    if not processed_df.empty:
        # docid ã”ã¨ã« code ã‚’å–å¾—ã—ã€bin (=ä¸Š2æ¡) ã‚’ä½œæˆ
        processed_df["bin"] = processed_df["docID"].apply(
            lambda x: str(next(m for m in all_meta if m["docID"] == x).get("secCode", ""))[:2]
        )
    bins = processed_df["bin"].unique() if not processed_df.empty else []

    if all_quant_dfs:
        logger.info("æ•°å€¤ãƒ‡ãƒ¼ã‚¿(financial_values)ã®ãƒãƒ¼ã‚¸ã‚’é–‹å§‹ã—ã¾ã™...")
        try:
            full_quant_df = pd.concat(all_quant_dfs, ignore_index=True)
            for b_val in bins:
                # å½“è©² bin ã«å±ã™ã‚‹ docid ã‚’æŠ½å‡º
                bin_docids = processed_df[processed_df["bin"] == b_val]["docID"].tolist()
                sec_quant = full_quant_df[full_quant_df["docid"].isin(bin_docids)]
                if sec_quant.empty:
                    continue
                # ã€ä¿®æ­£ã€‘Masteræ›´æ–°ã¯ bin å˜ä½ã§å®Ÿè¡Œ
                merger.merge_and_upload(
                    b_val,
                    "financial_values",
                    sec_quant,
                    worker_mode=True,
                    catalog_manager=catalog,
                    run_id=run_id,
                    chunk_id=chunk_id,
                    defer=True,
                )
        except Exception as e:
            logger.error(f"æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸å¤±æ•—: {e}")
            all_success = False

    if all_text_dfs:
        logger.info("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(qualitative_text)ã®ãƒãƒ¼ã‚¸ã‚’é–‹å§‹ã—ã¾ã™...")
        try:
            full_text_df = pd.concat(all_text_dfs, ignore_index=True)
            for b_val in bins:
                bin_docids = processed_df[processed_df["bin"] == b_val]["docID"].tolist()
                sec_text = full_text_df[full_text_df["docid"].isin(bin_docids)]
                if sec_text.empty:
                    continue
                if not merger.merge_and_upload(
                    b_val,
                    "qualitative_text",
                    sec_text,
                    worker_mode=True,
                    catalog_manager=catalog,
                    run_id=run_id,
                    chunk_id=chunk_id,
                    defer=True,
                ):
                    all_success = False
                    logger.error(f"âŒ Masteræ›´æ–°å¤±æ•—: bin={b_val} (qualitative_text)")
        except Exception as e:
            logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸å¤±æ•—: {e}")
            all_success = False

    # ã€ä¿®æ­£ã€‘å…¨ã‚«ã‚¿ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ã®åé›†ã‚’ä¸€æœ¬åŒ– (å¯¾è±¡å†…ãƒ»å¯¾è±¡å¤–ã™ã¹ã¦)
    # 824ä»¶ã™ã¹ã¦ãŒ potential_catalog_records ã«æ ¼ç´ã•ã‚Œã¦ã„ã¾ã™
    final_catalog_records = list(potential_catalog_records.values())

    if final_catalog_records:
        df_cat = pd.DataFrame(final_catalog_records)

        # ã€é€æ˜æ€§ã€‘IDé‡è¤‡ã«ã‚ˆã‚‹ä»¶æ•°ä¸ä¸€è‡´ (824 vs 822 ç­‰) ã‚’äº‹å‰ã«ãƒ­ã‚°å‡ºåŠ›
        initial_len = len(df_cat)
        df_cat = df_cat.drop_duplicates(subset=["doc_id"], keep="last")
        final_len = len(df_cat)

        if initial_len > final_len:
            logger.info(
                f"ğŸ’¡ IDé‡è¤‡ã‚’æ’é™¤ã—ã¾ã—ãŸ: {initial_len} ä»¶ -> {final_len} ä»¶ (æ¸›å°‘: {initial_len - final_len} ä»¶)"
            )

        logger.info(f"å…¨æ›¸é¡ã® Catalog Delta ã‚’ä¿å­˜ã—ã¾ã™ ({final_len} ä»¶)")
        catalog.save_delta("catalog", df_cat, run_id, chunk_id, defer=True)

    # ã€ä¿®æ­£ã€‘all_success ãŒ False ã®å ´åˆã®å‡¦ç†ã‚’è¿½åŠ 
    if not all_success:
        logger.warning("âš ï¸ ä¸€éƒ¨ã®Masteræ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ¬¡å›å®Ÿè¡Œæ™‚ã«å†è©¦è¡Œã•ã‚Œã¾ã™ã€‚")

    # ã‚«ã‚¿ãƒ­ã‚°æ›´æ–°ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†å¾Œï¼‰
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ãŸdocid (Quant/Textå•ã‚ãšã€ä½•ã‚‰ã‹ã®ãƒ‡ãƒ¼ã‚¿ãŒä¿å­˜ã§ããŸã‚‚ã®)
    # å³å¯†ãªåˆ¤å®šã¯é›£ã—ã„ãŒã€ã“ã“ã§ã¯mergerã®æˆ»ã‚Šå€¤ãƒ™ãƒ¼ã‚¹ã§åˆ¤å®š
    if all_quant_dfs or all_text_dfs:
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®ã‚‚ã®ã¯ potential_catalog_records ã«ã‚ã‚‹
        # ã“ã“ã§ã¯ã€Œãƒ‡ãƒ¼ã‚¿ä¿å­˜ã¾ã§å®Œé‚ã—ãŸã€ã¨ã„ã†æ„å‘³ã§ã®æ›´æ–°ã¯ä¸è¦ã‹ã‚‚ã—ã‚Œãªã„
        # ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ™‚ã« processed_status=success ã§ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆæ¸ˆã¿ã§ã€update_catalogã‚‚å‘¼ã°ã‚Œã¦ã„ã‚‹ãŸã‚ï¼‰
        # ãŸã ã—ã€main.py ã®è¨­è¨ˆä¸Šã€æœ€å¾Œã«ã¾ã¨ã‚ã¦ update_catalog ã‚’å‘¼ã‚“ã§ã„ãŸç®‡æ‰€ã€‚
        # 360è¡Œç›®ã§éƒ½åº¦å‘¼ã‚“ã§ã„ã‚‹ã®ã§ã€ã“ã“ã¯ã€Œæœ€çµ‚çš„ãªå®Œäº†ãƒ­ã‚°ã€ã ã‘ã§è‰¯ã„å¯èƒ½æ€§ã€‚
        pass

    if all_success:
        # RAWãƒ•ã‚©ãƒ«ãƒ€ã®ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ã“ã“ã§ 1 ã‚³ãƒŸãƒƒãƒˆ)
        logger.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™...")
        catalog.upload_raw_folder(RAW_BASE_DIR, path_in_repo="raw", defer=True)

        # ãƒ‡ãƒ«ã‚¿ã¨æˆåŠŸãƒ•ãƒ©ã‚°ã‚’ä¸€æ‹¬ã‚³ãƒŸãƒƒãƒˆ (ã“ã“ã§ 1 ã‚³ãƒŸãƒƒãƒˆ)
        catalog.mark_chunk_success(run_id, chunk_id, defer=True)
        if catalog.push_commit(f"Worker Success: {run_id}/{chunk_id}"):
            logger.success(f"=== Workerå®Œäº†: å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ ({run_id}/{chunk_id}) ===")
        else:
            logger.error("âŒ æœ€çµ‚ã‚³ãƒŸãƒƒãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
    else:
        logger.error("=== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åœæ­¢ (Masterä¿å­˜ã‚¨ãƒ©ãƒ¼ç­‰) ===")
        sys.exit(1)


if __name__ == "__main__":
    main()
