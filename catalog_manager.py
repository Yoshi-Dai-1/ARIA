import random
import re
import time
import unicodedata
from pathlib import Path
from typing import Dict, List

import pandas as pd
import requests
from huggingface_hub import CommitOperationAdd, CommitOperationDelete, HfApi, hf_hub_download
from huggingface_hub.utils import EntryNotFoundError, HfHubHTTPError, RepositoryNotFoundError
from loguru import logger

from models import CatalogRecord, StockMasterRecord


class CatalogManager:
    def __init__(self, hf_repo: str, hf_token: str, data_path: Path):
        self.hf_repo = hf_repo
        self.hf_token = hf_token
        self.data_path = data_path
        self.data_path.mkdir(parents=True, exist_ok=True)
        self.api = HfApi() if hf_repo and hf_token else None

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å®šç¾©
        self.paths = {
            "catalog": "catalog/documents_index.parquet",
            "master": "meta/stocks_master.parquet",
            "listing": "meta/listing_history.parquet",
            "index": "meta/index_history.parquet",
            "name": "meta/name_history.parquet",
        }

        self.catalog_df = self._load_parquet("catalog")
        self.master_df = self._load_parquet("master")

        # ã€æœ€é‡è¦ã€‘ä¸€æ‹¬ã‚³ãƒŸãƒƒãƒˆç”¨ãƒãƒƒãƒ•ã‚¡
        self._commit_operations = {}
        self._snapshots = {}  # æ•´åˆæ€§ä¿è­·ã®ãŸã‚ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        logger.info("CatalogManager ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")

        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã¨æœ€æ–°ã‚¹ã‚­ãƒ¼ãƒã¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
        self._retrospective_cleanse()

    def _retrospective_cleanse(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èµ°æŸ»ã—ã€ä¸å‚™ãŒã‚ã‚Œã°è‡ªå‹•ä¿®æ­£ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        if not self.api:
            return

        logger.info("Starting integrity check for all Parquet files...")
        updated_count = 0

        # 1. å®šç¾©æ¸ˆã¿ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯
        for key in self.paths.keys():
            try:
                # æ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã® catalog_df, master_df ã¯ _load_parquet ã§ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿
                df = self.catalog_df if key == "catalog" else (self.master_df if key == "master" else None)
                if df is None:
                    df = self._load_parquet(key)

                # ã‚«ã‚¿ãƒ­ã‚°ã®å ´åˆã€18ã‚«ãƒ©ãƒ æœªæº€ãªã‚‰å¼·åˆ¶ä¿å­˜ã—ã¦ã‚¹ã‚­ãƒ¼ãƒæ‹¡å¼µ
                if key == "catalog" and len(df.columns) < 18:
                    self._save_and_upload(key, df, defer=True)
                    updated_count += 1
            except Exception:
                continue

        # 2. ãƒã‚¹ã‚¿ãƒ¼ã®å…¨Binãƒ•ã‚¡ã‚¤ãƒ«ã‚’èµ°æŸ»
        try:
            files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type="dataset")
            bin_files = [f for f in files if "master/bin/" in f and f.endswith(".parquet")]

            for b_file in bin_files:
                local_tmp = self.data_path / "temp_cleanse.parquet"
                self.api.hf_hub_download(
                    repo_id=self.hf_repo,
                    filename=b_file,
                    repo_type="dataset",
                    token=self.hf_token,
                    local_dir=str(self.data_path),
                    local_dir_use_symlinks=False,
                )
                df_bin = pd.read_parquet(self.data_path / b_file)

                # ã‚¹ã‚­ãƒ¼ãƒä¸é©åˆãŒã‚ã‚Œã°ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã—ã¦äºˆç´„
                # (å…·ä½“çš„ãª rec ãƒã‚§ãƒƒã‚¯ã§ã¯ãªãã€ãƒ¢ãƒ‡ãƒ«ã¨ã®ä¸ä¸€è‡´ã‚’åŸºæº–ã«ã™ã‚‹)
                df_clean = self._clean_dataframe("master", df_bin)
                if len(df_clean.columns) != len(df_bin.columns):
                    logger.info(f"Cleaned up bin file schema: {b_file}")
                    df_clean.to_parquet(local_tmp, index=False, compression="zstd")
                    self.add_commit_operation(b_file, local_tmp)
                    updated_count += 1
        except Exception:
            pass

    def _clean_dataframe(self, key: str, df: pd.DataFrame) -> pd.DataFrame:
        """å…¨ã¦ã®DataFrameã«å¯¾ã—ã¦å…±é€šã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚’é©ç”¨"""
        if df.empty:
            return df

        # 0. ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–ï¼ˆç©ºç™½é™¤å»ï¼‰
        df.columns = df.columns.astype(str).str.strip()

        # ã€è¿½åŠ ã€‘å…¨æ–‡å­—åˆ—ã‚«ãƒ©ãƒ ã®ç©ºæ–‡å­—ã‚’æ˜ç¤ºçš„ã« None (NULL) ã«çµ±ä¸€
        for col in df.columns:
            if df[col].dtype == "object":
                # ç©ºç™½ã®ã¿ã®æ–‡å­—åˆ—ã‚‚ NULL æ‰±ã„ã¨ã™ã‚‹
                df[col] = df[col].apply(lambda x: None if (isinstance(x, str) and not x.strip()) else x)

        # 1. ä¸è¦ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”±æ¥ã‚«ãƒ©ãƒ ã®é™¤å»
        drop_targets = ["index", "level_0", "Unnamed: 0"]
        cols_to_drop = [c for c in drop_targets if c in df.columns]

        if cols_to_drop:
            logger.debug(f"{key}: Removed unnecessary columns: {cols_to_drop}")
            df = df.drop(columns=cols_to_drop)

        # 2. ã‚«ã‚¿ãƒ­ã‚°ã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«å®šç¾©ã®ã‚«ãƒ©ãƒ æ§‹æˆã‚’å¼·åˆ¶ (18ã‚«ãƒ©ãƒ åŒ–)
        if key == "catalog":
            # NaN ã‚’ None ã«ç½®æ›
            df = df.replace({pd.NA: None, float("nan"): None})

            # ãƒ¢ãƒ‡ãƒ«å®šç¾©ã®å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—
            expected_cols = list(CatalogRecord.model_fields.keys())

            # æ—¢å­˜ã®ã‚«ãƒ©ãƒ ã®ã¿ã§Pydanticãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šã—ã€ä¸è¶³åˆ†ã‚’Noneã§è£œå®Œ
            validated = []
            for rec_dict in df.to_dict("records"):
                try:
                    # æ¬ è½ã—ã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã£ã¦ã‚‚ Pydantic ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è£œå®Œ
                    validated.append(CatalogRecord(**rec_dict).model_dump())
                except Exception as e:
                    # å¿…é ˆé …ç›®(doc_idç­‰)ãŒæ¬ ã‘ã¦ã„ã‚‹å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼
                    logger.warning(f"ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ä¸­ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ä¸å‚™ (doc_id: {rec_dict.get('doc_id')}): {e}")
                    # æ§‹é€ ã ã‘ã§ã‚‚ç¶­æŒã™ã‚‹ãŸã‚ã€è¾æ›¸ã¨ã—ã¦å¯èƒ½ãªé™ã‚Šä¿æŒ
                    row = {col: rec_dict.get(col) for col in expected_cols}
                    validated.append(row)

            df = pd.DataFrame(validated)
            # ã‚«ãƒ©ãƒ é †ã‚’ãƒ¢ãƒ‡ãƒ«å®šç¾©ã«åˆã‚ã›ã‚‹
            df = df[expected_cols]

            # ã€é‡è¦ã€‘ãƒ‡ãƒ¼ã‚¿å‹ã®æ­£è¦åŒ– (2024.0 å›é¿ã®ãŸã‚ã® Int64 é©ç”¨)
            # pandas ã®æµ®å‹•å°æ•°ç‚¹åŒ–ã‚’é˜»æ­¢ã—ã€æ•´æ•°ã¾ãŸã¯ NULL ã¨ã—ã¦ä¿å­˜
            if "fiscal_year" in df.columns:
                df["fiscal_year"] = pd.to_numeric(df["fiscal_year"], errors="coerce").astype("Int64")
            if "num_months" in df.columns:
                df["num_months"] = pd.to_numeric(df["num_months"], errors="coerce").astype("Int64")

        # 3. ãƒã‚¹ã‚¿ã®å ´åˆã€codeã‚’ç¢ºå®Ÿã«æ–‡å­—åˆ—åŒ–
        if key == "master" and "code" in df.columns:
            df["code"] = df["code"].astype(str).str.strip()

        # 4. Objectå‹ã®å®‰å®šåŒ– (None ã‚’ä¿æŒã—ã¤ã¤æ–‡å­—åˆ—åŒ–)
        for col in df.columns:
            if df[col].dtype == "object":
                df[col] = df[col].apply(lambda x: str(x) if (x is not None and not pd.isna(x)) else None)

        return df

    def _normalize_company_name(self, name: str) -> str:
        """æ¯”è¼ƒåˆ¤å®šã®ãŸã‚ã«æ³•äººæ ¼ã‚„ç©ºç™½ã‚’é™¤å»ã—ã¦æ­£è¦åŒ–ã™ã‚‹ (NFKCå¯¾å¿œç‰ˆ)"""
        if not name or not isinstance(name, str):
            return ""

        # 1. NFKCæ­£è¦åŒ– (å…¨è§’æ•°å­—ãƒ»è‹±å­—ã‚’åŠè§’ã«ã€ãˆ± ãªã©ã‚’ (æ ª) ã«åˆ†è§£)
        n = unicodedata.normalize("NFKC", name)

        # 2. å…¨ã¦ã®ç©ºç™½é™¤å»
        n = n.replace(" ", "").replace("\u3000", "")

        # 3. ä»£è¡¨çš„ãªæ³•äººæ ¼è¡¨è¨˜ã‚’é™¤å»
        # NFKCå¾Œã® (æ ª) ã‚„ (æœ‰) ãªã©ã«å¯¾å¿œã§ãã‚‹ã‚ˆã†ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ•´ç†
        patterns = [
            r"æ ªå¼ä¼šç¤¾",
            r"æœ‰é™ä¼šç¤¾",
            r"åˆåŒä¼šç¤¾",
            r"åˆè³‡ä¼šç¤¾",
            r"åˆåä¼šç¤¾",
            r"ä¸€èˆ¬ç¤¾å›£æ³•äºº",
            r"ä¸€èˆ¬è²¡å›£æ³•äºº",
            r"å…¬ç›Šç¤¾å›£æ³•äºº",
            r"å…¬ç›Šè²¡å›£æ³•äºº",
            r"\(æ ª\)",
            r"\(æœ‰\)",
            r"\(åˆ\)",
            r"\(ç¤¾\)",
            r"\(è²¡\)",
        ]
        for p in patterns:
            n = re.sub(p, "", n)

        return n.strip()

    def add_commit_operation(self, repo_path: str, local_path: Path):
        """ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«æ“ä½œã‚’è¿½åŠ ï¼ˆé‡è¤‡ã¯æœ€æ–°ã§ä¸Šæ›¸ãï¼‰"""
        self._commit_operations[repo_path] = CommitOperationAdd(path_in_repo=repo_path, path_or_fileobj=str(local_path))
        logger.debug(f"ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {repo_path}")

    def take_snapshot(self):
        """ç¾åœ¨ã®GlobalçŠ¶æ…‹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ãƒ¡ãƒ¢ãƒªã«å–å¾— (ä¸æ•´åˆç™ºç”Ÿæ™‚ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨)"""
        # ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«ä¿å­˜
        self._snapshots = {
            "catalog": self.catalog_df.copy(),
            "master": self.master_df.copy(),
            "listing": self._load_parquet("listing").copy(),
            "index": self._load_parquet("index").copy(),
            "name": self._load_parquet("name").copy(),
        }
        logger.info("Global çŠ¶æ…‹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—ã—ã¾ã—ãŸ (å®‰å…¨æ€§ç¢ºä¿)")

    def rollback(self, message: str = "RaW-V Failure: Automated Recovery Rollback"):
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®çŠ¶æ…‹ã‚’å¼·åˆ¶çš„ã«æ›¸ãæˆ»ã—ã€Globalãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’å¾©æ—§ã™ã‚‹"""
        if not self._snapshots:
            logger.error("âŒ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ãã¾ã›ã‚“ã€‚")
            return False

        logger.warning(f"â›” ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’é–‹å§‹ã—ã¾ã™: {message}")

        # æ—¢å­˜ã®ã‚³ãƒŸãƒƒãƒˆäºˆç´„ã‚’ã™ã¹ã¦ç ´æ£„
        self._commit_operations = {}

        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®å†…å®¹ã‚’å¼·åˆ¶çš„ã«ä¸Šæ›¸ãäºˆç´„
        for key, df in self._snapshots.items():
            self._save_and_upload(key, df, defer=True)

        # ä¸€æ‹¬ã‚³ãƒŸãƒƒãƒˆã®å®Ÿè¡Œ (äº‹å®Ÿä¸Šã®å·®ã—æˆ»ã—)
        success = self.push_commit(f"ROLLBACK: {message}")
        if success:
            logger.success("âœ… ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ»ã‚³ãƒŸãƒƒãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚æ•´åˆæ€§ã¯å¾©æ—§ã•ã‚Œã¾ã—ãŸã€‚")
            # ãƒ¡ãƒ¢ãƒªä¸Šã®æœ€æ–°çŠ¶æ…‹ã‚‚ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«æˆ»ã™
            self.catalog_df = self._snapshots["catalog"]
            self.master_df = self._snapshots["master"]
        else:
            logger.critical(
                "âŒ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯è‡ªä½“ã«å¤±æ•—ã—ã¾ã—ãŸï¼"
                "Hugging Faceä¸Šã®ãƒ‡ãƒ¼ã‚¿ãŒå£Šã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ç›´ã¡ã«æ‰‹å‹•ç¢ºèªãŒå¿…è¦ã§ã™ã€‚"
            )
        return success

    def _load_parquet(self, key: str, force_download: bool = False) -> pd.DataFrame:
        filename = self.paths[key]
        try:
            local_path = hf_hub_download(
                repo_id=self.hf_repo,
                filename=filename,
                repo_type="dataset",
                token=self.hf_token,
                force_download=force_download,
            )
            df = pd.read_parquet(local_path)
            # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘èª­ã¿è¾¼ã¿ç›´å¾Œã«ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
            df = self._clean_dataframe(key, df)
            logger.debug(f"ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {filename} ({len(df)} rows)")
            return df
        except RepositoryNotFoundError:
            logger.error(f"ãƒªãƒã‚¸ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.hf_repo}")
            logger.error("ç’°å¢ƒå¤‰æ•° HF_REPO ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            raise
        except (EntryNotFoundError, requests.exceptions.HTTPError) as e:
            # EntryNotFoundError (HFãƒ©ã‚¤ãƒ–ãƒ©ãƒª) ã¾ãŸã¯ ç”Ÿã® 404 (ãƒ‘ãƒƒãƒé©ç”¨æ™‚) ã‚’ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            is_404 = isinstance(e, EntryNotFoundError) or (
                hasattr(e, "response") and e.response is not None and e.response.status_code == 404
            )

            if not is_404:
                # 404 ä»¥å¤–ãªã‚‰ä¸Šä½ã¾ãŸã¯ Exception ã¸é£›ã°ã™
                raise e

            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚æ–°è¦ä½œæˆã—ã¾ã™: {filename}")
            if key == "catalog":
                cols = list(CatalogRecord.model_fields.keys())
                return pd.DataFrame(columns=cols)
            elif key == "master":
                cols = list(StockMasterRecord.model_fields.keys())
                return pd.DataFrame(columns=cols)
            elif key == "listing":
                return pd.DataFrame(columns=["code", "type", "event_date"])
            elif key == "index":
                return pd.DataFrame(columns=["index_name", "code", "type", "event_date"])
            elif key == "name":
                return pd.DataFrame(columns=["code", "old_name", "new_name", "change_date"])
            return pd.DataFrame()
        except HfHubHTTPError as e:
            logger.error(f"HF API ã‚¨ãƒ©ãƒ¼ ({e.response.status_code}): {filename}")
            logger.error(f"è©³ç´°: {e}")
            if e.response.status_code == 401:
                logger.error("èªè¨¼ã‚¨ãƒ©ãƒ¼: HF_TOKEN ãŒç„¡åŠ¹ã¾ãŸã¯æœŸé™åˆ‡ã‚Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            elif e.response.status_code == 403:
                logger.error("ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦: ãƒªãƒã‚¸ãƒˆãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“")
            raise
        except Exception as e:
            logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {filename} - {type(e).__name__}: {e}")
            raise

    def is_processed(self, doc_id: str) -> bool:
        if self.catalog_df.empty:
            return False
        return doc_id in self.catalog_df["doc_id"].values

    def update_catalog(self, new_records: List[Dict]) -> bool:
        """ã‚«ã‚¿ãƒ­ã‚°ã‚’æ›´æ–° (Pydanticãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿæ–½)"""
        if not new_records:
            return True

        validated = []
        for rec in new_records:
            try:
                validated.append(CatalogRecord(**rec).model_dump())
            except Exception as e:
                logger.error(f"ã‚«ã‚¿ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•— (doc_id: {rec.get('doc_id')}): {e}")

        if not validated:
            return False

        new_df = pd.DataFrame(validated)

        # ã€ä¿®æ­£ã€‘ä¸€æ™‚çš„ã«çµåˆã—ãŸDataFrameã‚’ä½œæˆï¼ˆãƒ¡ãƒ¢ãƒªä¸Šã®çŠ¶æ…‹ã¯å¤‰æ›´ã—ãªã„ï¼‰
        temp_catalog = pd.concat([self.catalog_df, new_df], ignore_index=True).drop_duplicates(
            subset=["doc_id"], keep="last"
        )

        # ã€ä¿®æ­£ã€‘ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸæ™‚ã®ã¿ã€ãƒ¡ãƒ¢ãƒªä¸Šã®ã‚«ã‚¿ãƒ­ã‚°ã‚’æ›´æ–°
        if self._save_and_upload("catalog", temp_catalog):
            self.catalog_df = temp_catalog
            logger.success(f"âœ… ã‚«ã‚¿ãƒ­ã‚°æ›´æ–°æˆåŠŸ: {len(validated)} ä»¶")
            return True
        else:
            logger.error("ã‚«ã‚¿ãƒ­ã‚°ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ¡ãƒ¢ãƒªä¸Šã®çŠ¶æ…‹ã‚’ä¿æŒã—ã¾ã™")
            return False

    def _save_and_upload(self, key: str, df: pd.DataFrame, defer: bool = False) -> bool:
        filename = self.paths[key]
        local_file = self.data_path / Path(filename).name

        # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘ä¿å­˜ç›´å‰ã«æœ€çµ‚ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        df = self._clean_dataframe(key, df)

        df.to_parquet(local_file, index=False, compression="zstd")

        if self.api:
            if defer:
                # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ ã—ã¦çµ‚äº† (ãƒ‘ã‚¹ã‚’ã‚­ãƒ¼ã«ã—ã¦æœ€æ–°ã®ã‚‚ã®ã§ä¸Šæ›¸ã)
                self.add_commit_operation(filename, local_file)
                return True

            max_retries = 5  # å¼·åŒ–
            for attempt in range(max_retries):
                try:
                    self.api.upload_file(
                        path_or_fileobj=str(local_file),
                        path_in_repo=filename,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.success(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {filename}")
                    return True
                except Exception as e:
                    # HfHubHTTPErrorã®å‹ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã€429ã®å ´åˆã®ã¿ãƒªãƒˆãƒ©ã‚¤
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(f"Rate limit exceeded. Waiting {wait_time}s before retry ({attempt + 1}/5)...")
                        time.sleep(wait_time)
                        continue

                    # ãã®ä»–ã®HTTPã‚¨ãƒ©ãƒ¼ (5xxç­‰) ã‚‚ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã«ã™ã‚‹
                    if isinstance(e, HfHubHTTPError) and e.response.status_code >= 500:
                        wait_time = 15 * (attempt + 1)
                        logger.warning(
                            f"Master HF Server Error ({e.response.status_code}). "
                            f"Waiting {wait_time}s... ({attempt + 1}/5)"
                        )
                        time.sleep(wait_time)
                        continue

                    logger.warning(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {filename} - {e} - Retrying ({attempt + 1}/5)...")
                    time.sleep(10 * (attempt + 1))
            logger.error(f"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«æœ€çµ‚çš„ã«å¤±æ•—ã—ã¾ã—ãŸ: {filename}")
            return False
        return True

    def upload_raw(self, local_path: Path, repo_path: str, defer: bool = False) -> bool:
        """ãƒ­ãƒ¼ã‚«ãƒ«ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ Hugging Face ã® raw/ ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        if not local_path.exists():
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“: {local_path}")
            return False

        if self.api:
            if defer:
                self.add_commit_operation(repo_path, local_path)
                logger.debug(f"RAWã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {repo_path}")
                return True

            max_retries = 5  # å¼·åŒ–
            for attempt in range(max_retries):
                try:
                    self.api.upload_file(
                        path_or_fileobj=str(local_path),
                        path_in_repo=repo_path,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.debug(f"RAWã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {repo_path}")
                    return True
                except Exception as e:
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(f"Rate limit exceeded for RAW. Waiting {wait_time}s... ({attempt + 1}/5)")
                        time.sleep(wait_time)
                        continue

                    logger.warning(f"RAWã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {repo_path} - {e} - Retrying ({attempt + 1}/5)...")
                    time.sleep(10 * (attempt + 1))
            return False
        return True

    def upload_raw_folder(self, folder_path: Path, path_in_repo: str, defer: bool = False) -> bool:
        """ãƒ•ã‚©ãƒ«ãƒ€å˜ä½ã§ã®ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ãƒªãƒˆãƒ©ã‚¤ä»˜)"""
        if not folder_path.exists():
            return True  # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãªã—ã¯æˆåŠŸã¨ã¿ãªã™

        if self.api:
            if defer:
                # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                for f in folder_path.glob("**/*"):
                    if f.is_file():
                        r_path = f"{path_in_repo}/{f.relative_to(folder_path)}"
                        self._commit_operations[r_path] = CommitOperationAdd(
                            path_in_repo=r_path, path_or_fileobj=str(f)
                        )
                logger.debug(f"RAWãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {path_in_repo}")
                return True

            max_retries = 5  # 3å›ã‹ã‚‰5å›ã«å¼·åŒ–
            for attempt in range(max_retries):
                try:
                    self.api.upload_folder(
                        folder_path=str(folder_path),
                        path_in_repo=path_in_repo,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.success(f"ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {path_in_repo} (from {folder_path})")
                    return True
                except Exception as e:
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(
                            f"Folder Upload Rate limit exceeded. Waiting {wait_time}s... ({attempt + 1}/{max_retries})"
                        )
                        time.sleep(wait_time)
                        continue

                    logger.warning(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {e} - Retrying ({attempt + 1}/{max_retries})...")
                    time.sleep(10)

            logger.error(f"ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•— (Give up): {path_in_repo}")
            return False
        return True

    def update_listing_history(self, new_events: pd.DataFrame) -> bool:
        history = self._load_parquet("listing")

        # åˆå›å®Ÿè¡Œæ™‚ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã›ãšã€ã‚¤ãƒ™ãƒ³ãƒˆã‚‚ç©ºï¼‰ã®å ´åˆã§ã‚‚ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if new_events.empty:
            if history.empty:
                # ç©ºã®å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ä¿å­˜
                return self._save_and_upload("listing", history)
            return True

        history = pd.concat([history, new_events], ignore_index=True).drop_duplicates()
        return self._save_and_upload("listing", history)

    def update_index_history(self, new_events: pd.DataFrame) -> bool:
        history = self._load_parquet("index")

        # åˆå›å®Ÿè¡Œæ™‚ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã›ãšã€ã‚¤ãƒ™ãƒ³ãƒˆã‚‚ç©ºï¼‰ã®å ´åˆã§ã‚‚ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if new_events.empty:
            if history.empty:
                # ç©ºã®å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ä¿å­˜
                return self._save_and_upload("index", history)
            return True

        history = pd.concat([history, new_events], ignore_index=True).drop_duplicates()
        return self._save_and_upload("index", history)

    def get_listing_history(self) -> pd.DataFrame:
        """ç¾åœ¨ã®ä¸Šå ´å±¥æ­´ãƒã‚¹ã‚¿ã‚’å–å¾—"""
        return self._load_parquet("listing")

    def get_index_history(self) -> pd.DataFrame:
        """ç¾åœ¨ã®æŒ‡æ•°æ¡ç”¨å±¥æ­´ãƒã‚¹ã‚¿ã‚’å–å¾—"""
        return self._load_parquet("index")

    def update_stocks_master(self, incoming_data: pd.DataFrame):
        """ãƒã‚¹ã‚¿æ›´æ–° & æ™‚ç³»åˆ—ãƒªã‚³ãƒ³ã‚·ãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ (ä¸–ç•Œæœ€é«˜æ°´æº–ã®æ­´å²å†æ§‹ç¯‰ãƒ­ã‚¸ãƒƒã‚¯)"""
        if incoming_data.empty:
            return True

        # 1. ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨å‹æ­£è¦åŒ–
        records = incoming_data.to_dict("records")
        validated = []
        for rec in records:
            try:
                rec = {k: (v if not pd.isna(v) else None) for k, v in rec.items()}
                # is_active ã®å‹æ­£è¦åŒ–
                if isinstance(rec.get("is_active"), str):
                    rec["is_active"] = rec["is_active"].lower() in ["true", "1", "yes"]
                # æ—¥ä»˜ã®æ­£è¦åŒ– (10æ–‡å­—åˆ¶é™)
                if rec.get("last_submitted_at"):
                    rec["last_submitted_at"] = str(rec["last_submitted_at"])[:10]
                validated.append(StockMasterRecord(**rec).model_dump())
            except Exception as e:
                logger.error(f"éŠ˜æŸ„æƒ…å ±ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•— (code: {rec.get('code')}): {e}")

        if not validated:
            return True
        incoming_df = pd.DataFrame(validated)

        # 2. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®çµ±åˆ (ãƒªã‚³ãƒ³ã‚·ãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³)
        # æ—¢å­˜ãƒã‚¹ã‚¿ã‚’ã€Œéå»ã®çŠ¶æ…‹ã®ä¸€ã¤ã€ã¨ã—ã¦æ‰±ã„ã€å…¨ã¦ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹
        current_m = self.master_df.copy()
        # ã‚«ãƒ©ãƒ è‡ªä½“ã®å­˜åœ¨ã‚’ã‚±ã‚¢ (NULL ã¯ NULL ã®ã¾ã¾ç¶­æŒ)
        if "last_submitted_at" not in current_m.columns:
            current_m["last_submitted_at"] = None

        # å…¨ã¦ã®æ—¢çŸ¥ã®çŠ¶æ…‹ã‚’çµ±åˆ
        all_states = pd.concat([current_m, incoming_df], ignore_index=True)

        # é‡è¤‡æ’é™¤ (åŒã˜ code, company_name, last_submitted_at ã¯ä¸è¦)
        all_states.drop_duplicates(subset=["code", "company_name", "last_submitted_at"], inplace=True)

        # 3. ç¤¾åå¤‰æ›´ã®æ­´å²çš„å¤‰é·ã‚’è§£æ
        name_history = self._load_parquet("name")
        new_history_events = []

        for code, group in all_states.groupby("code"):
            # æå‡ºæ—¥æ™‚ã®æ˜‡é †ã§ã‚½ãƒ¼ãƒˆ
            sorted_group = group.sort_values("last_submitted_at", ascending=True)

            # --- A. åŸºç‚¹(Baseline)ã®æ±ºå®š ---
            # 1. ã¾ãšã¯æ—¢ã«ç¢ºå®šã—ãŸå±¥æ­´(name_history)ã‹ã‚‰æœ€æ–°ã®åå‰ã‚’æ¢ã™ (æœ€ã‚‚ä¿¡é ¼ã§ãã‚‹éå»)
            prev_name = None
            if not name_history.empty:
                code_history = name_history[name_history["code"] == code]
                if not code_history.empty:
                    prev_name = code_history.iloc[-1]["new_name"]

            # 2. å±¥æ­´ãŒãªã„å ´åˆã€æ—¢å­˜ãƒã‚¹ã‚¿(stocks_master)ã®åå‰ã‚’æš«å®šåŸºç‚¹ã¨ã™ã‚‹
            # ãŸã ã—ã€ãƒã‚¹ã‚¿åã¯ JPX ç”±æ¥(ç•¥ç§°)ã®å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
            is_baseline_from_jpx = False
            if prev_name is None:
                master_entry = current_m[current_m["code"] == code]
                if not master_entry.empty:
                    prev_name = master_entry.iloc[0]["company_name"]
                    # æå‡ºæ—¥ãŒ NULL ãªã‚‰ JPX ç”±æ¥ã®æš«å®šåã¨åˆ¤æ–­
                    last_at = master_entry.iloc[0].get("last_submitted_at")
                    if pd.isna(last_at):
                        is_baseline_from_jpx = True

            # --- B. é€æ¬¡æ¯”è¼ƒã¨å±¥æ­´ç”Ÿæˆ ---
            for _, curr_state in sorted_group.iterrows():
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿(JPXç­‰ã®æ—¥ä»˜ãªã—ãƒã‚¹ã‚¿çŠ¶æ…‹)ã¯æ¯”è¼ƒã®ã€Œå¯¾è±¡ã€ã§ã¯ãªãã€ŒåŸºç‚¹ã€ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
                last_at = curr_state.get("last_submitted_at")
                if pd.isna(last_at):
                    continue

                curr_name = curr_state["company_name"]

                # åŸºç‚¹ãŒãªã„å ´åˆ (ARIAã§å®Œå…¨æ–°è¦ã«ç™ºè¦‹ã•ã‚ŒãŸéŠ˜æŸ„)
                if prev_name is None:
                    prev_name = curr_name
                    continue

                # æ­£è¦åŒ–æ¯”è¼ƒã‚’è¡Œã„ã€å®Ÿè³ªçš„ãªå·®ç•°ãŒã‚ã‚‹å ´åˆã®ã¿å±¥æ­´ã‚’ä½œæˆ
                normalized_prev = self._normalize_company_name(prev_name)
                normalized_curr = self._normalize_company_name(curr_name)

                if normalized_prev != normalized_curr:
                    # æœ¬ç‰©ã®ç¤¾åå¤‰æ›´ã¨ã—ã¦è¨˜éŒ²
                    event = {
                        "code": code,
                        "old_name": prev_name,
                        "new_name": curr_name,
                        "change_date": last_at[:10],
                    }

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    exists = False
                    if not name_history.empty:
                        exists = not name_history[
                            (name_history["code"] == code)
                            & (name_history["old_name"] == event["old_name"])
                            & (name_history["new_name"] == event["new_name"])
                        ].empty

                    if not exists:
                        new_history_events.append(event)
                        logger.info(f"âœ¨ ç¤¾åå¤‰æ›´ã‚’æ¤œçŸ¥: {code} | {prev_name} -> {curr_name}")

                    # åŸºç‚¹ã‚’æ›´æ–°
                    prev_name = curr_name
                    is_baseline_from_jpx = False  # EDINETç”±æ¥ã«ãªã£ãŸã®ã§ãƒ•ãƒ©ã‚°ã‚’è½ã¨ã™
                else:
                    # ã€Œå½¢å¼çš„ãªå·®ç•°(ç•¥ç§°â†’æ­£å¼åç§°)ã€ã¾ãŸã¯ã€ŒåŒä¸€åç§°ã€ã®å ´åˆ
                    # å±¥æ­´ã«ã¯æ®‹ã•ãªã„ãŒã€ä»¥é™ã®æ¯”è¼ƒã®ãŸã‚ã«åŸºç‚¹ã ã‘ã¯æ›´æ–°(æ­£å¼åã¸æ˜‡æ ¼)
                    if prev_name != curr_name:
                        if is_baseline_from_jpx:
                            logger.debug(
                                f"â„¹ï¸ åŸºç‚¹ã‚’ç•¥ç§°ã‹ã‚‰æ­£å¼åç§°ã¸æ˜‡æ ¼ (å±¥æ­´ã«ã¯æ®‹ã—ã¾ã›ã‚“): "
                                f"{code} | {prev_name} -> {curr_name}"
                            )
                        prev_name = curr_name
                        is_baseline_from_jpx = False
        # 4. å±¥æ­´ã®ä¿å­˜ (Atomic & Non-destructive)
        if new_history_events:
            new_hist_df = pd.DataFrame(new_history_events)
            name_history = pd.concat([name_history, new_hist_df], ignore_index=True).drop_duplicates()
            # defer=True ã‚’æŒ‡å®šã—ã¦ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«ç©ã‚€
            self._save_and_upload("name", name_history, defer=True)
            logger.info(f"æ™‚ç³»åˆ—ãƒªã‚³ãƒ³ã‚·ãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Š {len(new_history_events)} ä»¶ã®å¤‰é·ã‚’ç‰¹å®šã—ã¾ã—ãŸã€‚")
        elif name_history.empty:
            # åˆå›å®Ÿè¡Œæ™‚ãªã©ã§å±¥æ­´ãŒç©ºã®å ´åˆã§ã‚‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦æ•´åˆæ€§ã‚’ä¿ã¤
            self._save_and_upload("name", name_history, defer=True)

        # å…¨çŠ¶æ…‹ã®ä¸­ã‹ã‚‰ã€code ã”ã¨ã«æå‡ºæ—¥æ™‚ãŒæœ€æ–°ã®ã‚‚ã®ã‚’æŠ½å‡º
        sorted_all = all_states.sort_values("last_submitted_at", ascending=False)

        # ã‚»ã‚¯ã‚¿ãƒ¼ã¨å¸‚å ´æƒ…å ±ã®ã€Œå±æ€§ç¶™æ‰¿ï¼ˆInheritanceï¼‰ã€
        # æœ€æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒ NULL ã‚„ "ãã®ä»–" ã®å ´åˆã€éå»ã®æœ‰åŠ¹ãªãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆJPXç­‰ï¼‰ã‹ã‚‰å¼•ãç¶™ã
        def resolve_attr(group, col):
            # æå‡ºæ—¥ã«é–¢ã‚ã‚‰ãšã€ãã®ã‚³ãƒ¼ãƒ‰ã«ãŠã‘ã‚‹ NULL ä»¥å¤–ã®æœ€ã‚‚ç¢ºã‹ãªå€¤ã‚’æ¢ã™
            # (JPXã¯1970å¹´ã ãŒã‚»ã‚¯ã‚¿ãƒ¼æƒ…å ±ã¯ã€Œæ­£ã€ã§ã‚ã‚‹ãŸã‚ã€å…¨ä½“ã‹ã‚‰æ¤œç´¢ã—ã¦è‰¯ã„)
            valid = group[col][~group[col].isin(["ãã®ä»–", None, "nan", ""])]
            return valid.iloc[0] if not valid.empty else None

        # å„ã‚³ãƒ¼ãƒ‰ã®æœ€æ–°çŠ¶æ…‹ã‚’ç‰¹å®šã—ã¤ã¤ã€å±æ€§ã‚’è£œå®Œ
        best_records = []
        for _, group in sorted_all.groupby("code", sort=False):
            # 1. ç‰©ç†çš„ãªæœ€æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾— (ç¤¾åã¨æå‡ºæ—¥æ™‚ã®æ±ºå®šç”¨)
            latest_rec = group.iloc[0].copy()

            # 2. JPXãƒ¬ã‚³ãƒ¼ãƒ‰(æ—¥ä»˜ãªã—)ã‚’ç‰¹å®š (å±æ€§ã®æ­£è§£ãƒ‡ãƒ¼ã‚¿)
            jpx_entries = group[group["last_submitted_at"].isna()]

            if not jpx_entries.empty:
                # JPXãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ä¸»è¦å±æ€§ã‚’JPXã‹ã‚‰å¼·åˆ¶å–å¾—ï¼ˆEDINETå±æ€§ã‚’æ‹’çµ¶ï¼‰
                jpx_rec = jpx_entries.iloc[0]
                latest_rec["sector"] = jpx_rec["sector"]
                latest_rec["market"] = jpx_rec["market"]
                latest_rec["is_active"] = jpx_rec["is_active"]
                # ä¸‡ãŒä¸€ JPX ã®ã‚»ã‚¯ã‚¿ãƒ¼ãŒä¸å…¨ãªå ´åˆã¯ã€éå»ã®æœ‰åŠ¹ãªå±æ€§ã‹ã‚‰æ‹¾ã†ï¼ˆãŸã ã—å„ªå…ˆåº¦ã¯JPXï¼‰
                if latest_rec["sector"] in ["ãã®ä»–", None, "nan", ""]:
                    latest_rec["sector"] = resolve_attr(group, "sector")
            else:
                # JPXã«ä¸€åº¦ã‚‚ç™»éŒ²ã•ã‚ŒãŸã“ã¨ãŒãªã„(å®Œå…¨æ–°è¦ä¸Šå ´ç­‰)ã®å ´åˆ
                # JPXã«ã‚ˆã‚‹æ‰¿èª(åŒæœŸ)ãŒã‚ã‚‹ã¾ã§ã¯ã€Unknown (None) çŠ¶æ…‹ã§éš”é›¢ã™ã‚‹
                latest_rec["is_active"] = None
                latest_rec["sector"] = None
                latest_rec["market"] = None

            best_records.append(latest_rec)

        self.master_df = pd.DataFrame(best_records)

        # defer=True ã‚’æŒ‡å®šã—ã¦ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«ç©ã‚€
        return self._save_and_upload("master", self.master_df, defer=True)

    def get_last_index_list(self, index_name: str) -> pd.DataFrame:
        """æŒ‡å®šæŒ‡æ•°ã®æ§‹æˆéŠ˜æŸ„ã‚’å–å¾— (Phase 3ç”¨)"""
        return pd.DataFrame(columns=["code"])

    def get_sector(self, code: str) -> str:
        """è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æ¥­ç¨®å–å¾—"""
        if self.master_df.empty:
            return None
        row = self.master_df[self.master_df["code"] == code]
        if not row.empty:
            val = row.iloc[0]["sector"]
            return str(val) if val is not None else None
        return None

    def save_delta(
        self, key: str, df: pd.DataFrame, run_id: str, chunk_id: str, custom_filename: str = None, defer: bool = False
    ) -> bool:
        """ãƒ‡ãƒ«ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (Workerç”¨)"""
        if df.empty:
            return True

        if custom_filename:
            filename = custom_filename
        else:
            filename = f"{Path(self.paths[key]).stem}.parquet"

        delta_path = f"temp/deltas/{run_id}/{chunk_id}/{filename}"
        local_file = self.data_path / f"delta_{run_id}_{chunk_id}_{filename}"

        # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘ä¿å­˜ç›´å‰ã«æœ€çµ‚ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        df = self._clean_dataframe(key, df)

        df.to_parquet(local_file, index=False, compression="zstd")

        return self.upload_raw(local_file, delta_path, defer=defer)

    def mark_chunk_success(self, run_id: str, chunk_id: str, defer: bool = False) -> bool:
        """ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æˆåŠŸãƒ•ãƒ©ã‚° (_SUCCESS) ã‚’ä½œæˆ (Workerç”¨)"""
        success_path = f"temp/deltas/{run_id}/{chunk_id}/_SUCCESS"
        local_file = self.data_path / f"SUCCESS_{run_id}_{chunk_id}"
        local_file.touch()

        return self.upload_raw(local_file, success_path, defer=defer)

    def load_deltas(self, run_id: str) -> Dict[str, pd.DataFrame]:
        """å…¨ãƒ‡ãƒ«ã‚¿ã‚’åé›†ã—ã¦ãƒãƒ¼ã‚¸ (Mergerç”¨)"""
        if not self.api:
            logger.warning("APIåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ãƒ‡ãƒ«ã‚¿åé›†ä¸å¯")
            return {}

        deltas = {}

        try:
            # ã€æ•´åˆæ€§å¼·åŒ–ã€‘HF Hub ã®ãƒªã‚¹ãƒˆå–å¾—è‡ªä½“ã‚’ãƒªãƒˆãƒ©ã‚¤ã—ã€åæ˜ é…å»¶ã«å¯¾å‡¦
            folder = f"temp/deltas/{run_id}"
            files = []
            for attempt in range(3):
                files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type="dataset")
                target_files = [f for f in files if f.startswith(folder)]
                if target_files:
                    break
                logger.warning(f"ãƒ‡ãƒ«ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å†è©¦è¡Œä¸­... ({attempt + 1}/3)")
                time.sleep(10)

            # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            chunks = {}
            for f in target_files:
                parts = f.split("/")
                if len(parts) < 4:
                    continue
                chunk_id = parts[3]
                if chunk_id not in chunks:
                    chunks[chunk_id] = []
                chunks[chunk_id].append(f)

            # _SUCCESS ãŒã‚ã‚‹ãƒãƒ£ãƒ³ã‚¯ã®ã¿å‡¦ç†
            valid_chunks = 0
            for chunk_id, file_list in chunks.items():
                if not any(f.endswith("_SUCCESS") for f in file_list):
                    # ã€æ•´åˆæ€§å¼·åŒ–ã€‘HF Hubã®çµæœæ•´åˆæ€§ã‚’è€ƒæ…®ã—ã€1å›è¦‹ã¤ã‹ã‚‰ãªãã¦ã‚‚
                    # åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—ã‚’è©¦ã¿ã‚‹ã“ã¨ãŒæœ›ã¾ã—ã„ãŒã€ã“ã“ã§ã¯ä¸€æ—¦è­¦å‘Šã«ç•™ã‚ã‚‹
                    logger.warning(f"âš ï¸ æœªå®Œäº†ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—: {chunk_id}")
                    continue

                valid_chunks += 1
                for remote_path in file_list:
                    if remote_path.endswith("_SUCCESS"):
                        continue

                    # ã‚­ãƒ¼åˆ¤åˆ¥
                    fname = Path(remote_path).name
                    key = None
                    if fname == "documents_index.parquet":
                        key = "catalog"
                    elif fname == "stocks_master.parquet":
                        key = "master"
                    elif fname == "listing_history.parquet":
                        key = "listing"
                    elif fname == "index_history.parquet":
                        key = "index"
                    elif fname == "name_history.parquet":
                        key = "name"
                    elif fname.startswith("financial_values_bin"):
                        bin_id = fname.replace("financial_values_bin", "").replace(".parquet", "")
                        key = f"financial_bin{bin_id}"
                    elif fname.startswith("qualitative_text_bin"):
                        bin_id = fname.replace("qualitative_text_bin", "").replace(".parquet", "")
                        key = f"text_bin{bin_id}"
                    elif fname.startswith("financial_values_"):
                        sector = fname.replace("financial_values_", "").replace(".parquet", "")
                        key = f"financial_{sector}"
                    elif fname.startswith("qualitative_text_"):
                        sector = fname.replace("qualitative_text_", "").replace(".parquet", "")
                        key = f"text_{sector}"

                    if key:
                        attempts = 2
                        for att in range(attempts):
                            try:
                                local_path = hf_hub_download(
                                    repo_id=self.hf_repo, filename=remote_path, repo_type="dataset", token=self.hf_token
                                )
                                df = pd.read_parquet(local_path)
                                if key not in deltas:
                                    deltas[key] = []
                                deltas[key].append(df)
                                break
                            except Exception as e:
                                if att == attempts - 1:
                                    logger.error(f"âŒ ãƒ‡ãƒ«ã‚¿èª­ã¿è¾¼ã¿å¤±æ•— ({remote_path}): {e}")
                                    raise
                                logger.warning(f"ãƒ‡ãƒ«ã‚¿èª­ã¿è¾¼ã¿å†è©¦è¡Œä¸­... ({att + 1}) {remote_path}")
                                time.sleep(5)

            logger.info(f"æœ‰åŠ¹ãªãƒãƒ£ãƒ³ã‚¯æ•°: {valid_chunks} / {len(chunks)}")

            # ãƒãƒ¼ã‚¸çµæœã‚’è¿”ã™
            merged = {}
            for key, df_list in deltas.items():
                if df_list:
                    # å…¨ã¦ã®DFã®ã‚«ãƒ©ãƒ ã‚’å…±é€šåŒ–ï¼ˆå‹ä¸æ•´åˆå¯¾ç­–ï¼‰
                    merged[key] = pd.concat(df_list, ignore_index=True)
                else:
                    merged[key] = pd.DataFrame()
            return merged

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ«ã‚¿åé›†å¤±æ•—: {e}")
            return {}

    def push_commit(self, message: str = "Batch update from ARIA") -> bool:
        """
        ãƒãƒƒãƒ•ã‚¡ã«æºœã¾ã£ãŸæ“ä½œã‚’ã‚³ãƒŸãƒƒãƒˆå®Ÿè¡Œã€‚
        ã€ç©¶æ¥µã®å®‰å®šåŒ–ã€‘æ“ä½œæ•°ãŒå¤šã„å ´åˆã¯ã€HFå´ã®è² è·ã¨429ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã€è‡ªå‹•çš„ã«åˆ†å‰²ã—ã¦ã‚³ãƒŸãƒƒãƒˆã™ã‚‹ã€‚
        """
        if not self.api or not self._commit_operations:
            return True

        ops_list = list(self._commit_operations.values())
        total_ops = len(ops_list)

        # 1ã‚³ãƒŸãƒƒãƒˆã‚ãŸã‚Šã®æœ€å¤§æ“ä½œæ•°
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ (128å›/æ™‚) ã‚’å›é¿ã™ã‚‹ãŸã‚ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ‹¡å¤§ã—ã¦ã‚³ãƒŸãƒƒãƒˆå›æ•°ã‚’å‰Šæ¸›ã™ã‚‹
        # HFå´ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãªã„ã‚®ãƒªã‚®ãƒªã®ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ 500ä»¶ç¨‹åº¦ãŒæœ€é©
        batch_size = 500
        batches = [ops_list[i : i + batch_size] for i in range(0, total_ops, batch_size)]

        logger.info(f"ğŸš€ ã‚³ãƒŸãƒƒãƒˆé€ä¿¡é–‹å§‹: åˆè¨ˆ {total_ops} æ“ä½œã‚’ {len(batches)} ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦å®Ÿè¡Œã—ã¾ã™")

        for i, batch in enumerate(batches):
            batch_msg = f"{message} (part {i + 1}/{len(batches)})"
            max_retries = 12
            success = False

            for attempt in range(max_retries):
                try:
                    self.api.create_commit(
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        operations=batch,
                        commit_message=batch_msg,
                        token=self.hf_token,
                    )
                    success = True
                    break
                except Exception as e:
                    status_code = getattr(getattr(e, "response", None), "status_code", None)

                    # 429 ãƒ¬ãƒ¼ãƒˆåˆ¶é™ ã¾ãŸã¯ 500 ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
                    if status_code in [429, 500]:
                        # 429ã®å ´åˆã¯ã‚ˆã‚Šé•·ãå¾…æ©Ÿ (HFã®å›å¾©ã‚’å¾…ã¤)
                        wait_time = int(getattr(e.response.headers, "get", lambda x, y: y)("Retry-After", 60))
                        wait_time = max(wait_time, 60) + (attempt * 30) + random.uniform(5, 15)
                        logger.warning(
                            f"HF Server Error ({status_code}). Waiting {wait_time:.1f}s... "
                            f"(Batch {i + 1}, Attempt {attempt + 1}/{max_retries})"
                        )
                        time.sleep(wait_time)
                        continue

                    # 409 ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ ã¾ãŸã¯ 412 å‰ææ¡ä»¶å¤±æ•—
                    if status_code in [409, 412]:
                        wait_time = (2 ** (attempt + 2)) + (random.uniform(10, 30))
                        logger.warning(
                            f"Commit Conflict ({status_code}). Retrying in {wait_time:.2f}s... "
                            f"(Batch {i + 1}, Attempt {attempt + 1}/{max_retries})"
                        )
                        time.sleep(wait_time)
                        continue

                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç­‰ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¾‹å¤–
                    logger.warning(
                        f"é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e} - Retrying... (Batch {i + 1}, Attempt {attempt + 1}/{max_retries})"
                    )
                    time.sleep(30 * (attempt + 1))

            if not success:
                logger.error(f"âŒ ãƒãƒƒãƒ {i + 1} ã®é€ä¿¡ã«æœ€çµ‚çš„ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                return False

            # ãƒãƒƒãƒé–“ã«çŸ­ã„ä¼‘æ†©ã‚’æŒŸã‚“ã§HFå´ã®è² è·ã‚’é€ƒãŒã™
            if i < len(batches) - 1:
                time.sleep(random.uniform(3, 7))

        logger.success(f"âœ… å…¨ {total_ops} æ“ä½œã®ãƒãƒƒãƒã‚³ãƒŸãƒƒãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        self._commit_operations = {}  # ã‚¯ãƒªã‚¢
        return True

    def cleanup_deltas(self, run_id: str, cleanup_old: bool = True):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (Mergerç”¨)"""
        if not self.api:
            return

        try:
            files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type="dataset")
            delta_root = "temp/deltas"

            # å¤ã„ãƒ•ã‚©ãƒ«ãƒ€ã®å‰Šé™¤ (24æ™‚é–“ä»¥ä¸ŠçµŒéã—ãŸã‚‚ã®ã‚’å¯¾è±¡ã¨ã™ã‚‹)
            if cleanup_old:
                now = time.time()
                delete_ops = []
                expired_runs = set()

                for f in files:
                    if not f.startswith(delta_root):
                        continue
                    parts = f.split("/")
                    if len(parts) < 3:
                        continue
                    r_id = parts[2]

                    # run_id ãŒæ•°å€¤ï¼ˆtimestampï¼‰ã§ã‚ã‚‹å‰æã§å¤ã„ã‚‚ã®ã‚’åˆ¤å®š
                    try:
                        timestamp = int(r_id)
                        if (now - timestamp) > 86400:  # 24æ™‚é–“ä»¥ä¸Š
                            delete_ops.append(f)
                            expired_runs.add(r_id)
                    except ValueError:
                        # æ•°å€¤ã§ãªã„ãƒ•ã‚©ãƒ«ãƒ€ã¯ç„¡è¦–ã™ã‚‹ã‹ã€åˆ¥ã®åŸºæº–ã§æ¶ˆã™
                        pass

                if delete_ops:
                    logger.info(f"å¤ã„ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¸…æƒä¸­... (24æ™‚é–“ä»¥ä¸ŠçµŒé: {len(expired_runs)} runs)")
                    for i in range(0, len(delete_ops), 50):
                        batch = delete_ops[i : i + 50]
                        # å‰Šé™¤æ“ä½œã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                        del_ops = [CommitOperationDelete(path_in_repo=p) for p in batch]
                        self.api.create_commit(
                            repo_id=self.hf_repo,
                            repo_type="dataset",
                            operations=del_ops,
                            commit_message="Automatic garbage collection of old deltas",
                        )

            # ä»Šå›ã®ãƒ•ã‚©ãƒ«ãƒ€å‰Šé™¤ï¼ˆå…¨å®Œäº†å¾Œç”¨ï¼‰
            else:
                target_prefix = f"{delta_root}/{run_id}"
                delete_ops = [f for f in files if f.startswith(target_prefix)]

                if delete_ops:
                    logger.info(f"ä»Šå›ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ä¸­... {run_id} ({len(delete_ops)} files)")
                    for i in range(0, len(delete_ops), 50):
                        batch = delete_ops[i : i + 50]
                        del_ops = [CommitOperationDelete(path_in_repo=p) for p in batch]
                        self.api.create_commit(
                            repo_id=self.hf_repo,
                            repo_type="dataset",
                            operations=del_ops,
                            commit_message=f"Cleanup successfully merged deltas: {run_id}",
                        )
                    logger.success(f"Cleanup completed: {run_id}")

        except Exception as e:
            logger.error(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
