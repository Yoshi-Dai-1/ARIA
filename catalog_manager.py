import random
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List

import pandas as pd
from huggingface_hub import CommitOperationAdd, CommitOperationDelete, HfApi, hf_hub_download
from huggingface_hub.utils import EntryNotFoundError, HfHubHTTPError, RepositoryNotFoundError
from loguru import logger

from models import CatalogRecord, StockMasterRecord


class CatalogManager:
    def __init__(self, hf_repo: str, hf_token: str, data_path: Path):
        self.hf_repo = hf_repo
        self.hf_token = hf_token
        self.data_path = data_path
        self.data_path.mkdir(parents=True, exist_ok=True)
        self.api = HfApi() if hf_repo and hf_token else None

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å®šç¾©
        self.paths = {
            "catalog": "catalog/documents_index.parquet",
            "master": "meta/stocks_master.parquet",
            "listing": "meta/listing_history.parquet",
            "index": "meta/index_history.parquet",
            "name": "meta/name_history.parquet",
        }

        self.catalog_df = self._load_parquet("catalog")
        self.master_df = self._load_parquet("master")

        # ã€è¿½åŠ ã€‘ãƒãƒƒãƒã‚³ãƒŸãƒƒãƒˆç”¨ãƒãƒƒãƒ•ã‚¡
        self._commit_operations = {}
        logger.info("CatalogManager ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")

        # ã€ç©¶æ¥µã®é¡åŠã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã€‘å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸ç´”ç‰©(rec)ã‚’ä¸€æƒã—ã€æœ€æ–°ã‚¹ã‚­ãƒ¼ãƒã¸å¼·åˆ¶ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
        self._retrospective_cleanse()

    def _retrospective_cleanse(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èµ°æŸ»ã—ã€ä¸å‚™ãŒã‚ã‚Œã°è‡ªå‹•ä¿®æ­£ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        if not self.api:
            return

        logger.info("ğŸ•µï¸ å…¨Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã‚’é–‹å§‹ã—ã¾ã™...")
        updated_count = 0

        # 1. å®šç¾©æ¸ˆã¿ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯
        for key in self.paths.keys():
            try:
                # æ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã® catalog_df, master_df ã¯ _load_parquet ã§ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿
                df = self.catalog_df if key == "catalog" else (self.master_df if key == "master" else None)
                if df is None:
                    df = self._load_parquet(key)

                # ã‚«ã‚¿ãƒ­ã‚°ã®å ´åˆã€18ã‚«ãƒ©ãƒ æœªæº€ãªã‚‰å¼·åˆ¶ä¿å­˜ã—ã¦ã‚¹ã‚­ãƒ¼ãƒæ‹¡å¼µ
                needs_update = False
                if key == "catalog" and len(df.columns) < 18:
                    needs_update = True

                # _clean_dataframe ã§æ—¢ã«æ¶ˆãˆã¦ã„ã‚‹ã¯ãšã ãŒã€ãƒªãƒã‚¸ãƒˆãƒªã«åæ˜ ã•ã›ã‚‹ãŸã‚ã«ä¿å­˜ã‚’äºˆç´„
                if needs_update or "rec" in df.columns:  # å®Ÿéš›ã«ã¯ _load_parquet ã§æ¶ˆãˆã¦ã„ã‚‹ãŒå¿µã®ãŸã‚
                    self._save_and_upload(key, df, defer=True)
                    updated_count += 1
            except Exception:
                continue

        # 2. ãƒã‚¹ã‚¿ãƒ¼ã®å…¨Binãƒ•ã‚¡ã‚¤ãƒ«ã‚’èµ°æŸ»
        try:
            files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type="dataset")
            bin_files = [f for f in files if "master/bin/" in f and f.endswith(".parquet")]

            for b_file in bin_files:
                local_tmp = self.data_path / "temp_cleanse.parquet"
                self.api.hf_hub_download(
                    repo_id=self.hf_repo,
                    filename=b_file,
                    repo_type="dataset",
                    token=self.hf_token,
                    local_dir=str(self.data_path),
                    local_dir_use_symlinks=False,
                )
                df_bin = pd.read_parquet(self.data_path / b_file)

                # rec ã‚«ãƒ©ãƒ ãŒã‚ã‚Œã°å³æ­»
                if "rec" in df_bin.columns or df_bin.index.name == "rec":
                    logger.info(f"ğŸ§¹ Binãƒ•ã‚¡ã‚¤ãƒ«ã®æ±šæŸ“ã‚’æ¤œçŸ¥: {b_file}")
                    df_clean = self._clean_dataframe("master", df_bin)
                    df_clean.to_parquet(local_tmp, index=False, compression="zstd")
                    self.add_commit_operation(b_file, local_tmp)
                    updated_count += 1
        except Exception:
            pass

        if updated_count > 0:
            logger.success(f"âœ… {updated_count} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®å¾©ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ ã—ã¾ã—ãŸã€‚")
            self.push_commit("Structural Integrity Upgrade: Unified 18-column schema and 'rec' elimination")

    def _clean_dataframe(self, key: str, df: pd.DataFrame) -> pd.DataFrame:
        """å…¨ã¦ã®DataFrameã«å¯¾ã—ã¦å…±é€šã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚’é©ç”¨"""
        if df.empty:
            return df

        # 0. ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–ï¼ˆç©ºç™½é™¤å»ï¼‰
        df.columns = df.columns.astype(str).str.strip()

        # 1. 'rec' ãŠã‚ˆã³ä¸è¦ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”±æ¥ã‚«ãƒ©ãƒ ã®å®Œå…¨é™¤å»
        # å®Œå…¨ä¸€è‡´ã ã‘ã§ãªãã€éƒ¨åˆ†ä¸€è‡´ã‚‚è­¦æˆ’ã™ã¹ãã ãŒã€ã¾ãšã¯æ˜ç¢ºãªã‚´ãƒŸã‚’é™¤å»
        drop_targets = ["rec", "index", "level_0", "Unnamed: 0"]
        cols_to_drop = [c for c in drop_targets if c in df.columns]

        if cols_to_drop:
            logger.debug(f"ğŸ§¹ {key}: ä¸è¦ã‚«ãƒ©ãƒ ã‚’é™¤å»ã—ã¾ã—ãŸ: {cols_to_drop}")
            df = df.drop(columns=cols_to_drop)

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åãŒ 'rec' ã®å ´åˆã‚‚å¯¾å‡¦
        if df.index.name == "rec":
            df.index.name = None

        # 2. ã‚«ã‚¿ãƒ­ã‚°ã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«å®šç¾©ã®ã‚«ãƒ©ãƒ æ§‹æˆã‚’å¼·åˆ¶ (18ã‚«ãƒ©ãƒ åŒ–)
        if key == "catalog":
            # NaN ã‚’ None ã«ç½®æ›
            df = df.replace({pd.NA: None, float("nan"): None})

            # ãƒ¢ãƒ‡ãƒ«å®šç¾©ã®å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—
            expected_cols = list(CatalogRecord.model_fields.keys())

            # æ—¢å­˜ã®ã‚«ãƒ©ãƒ ã®ã¿ã§Pydanticãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šã—ã€ä¸è¶³åˆ†ã‚’Noneã§è£œå®Œ
            validated = []
            for rec_dict in df.to_dict("records"):
                try:
                    # æ¬ è½ã—ã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã£ã¦ã‚‚ Pydantic ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è£œå®Œ
                    validated.append(CatalogRecord(**rec_dict).model_dump())
                except Exception as e:
                    # å¿…é ˆé …ç›®(doc_idç­‰)ãŒæ¬ ã‘ã¦ã„ã‚‹å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼
                    logger.warning(f"ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ä¸­ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ä¸å‚™ (doc_id: {rec_dict.get('doc_id')}): {e}")
                    # æ§‹é€ ã ã‘ã§ã‚‚ç¶­æŒã™ã‚‹ãŸã‚ã€è¾æ›¸ã¨ã—ã¦å¯èƒ½ãªé™ã‚Šä¿æŒ
                    row = {col: rec_dict.get(col) for col in expected_cols}
                    validated.append(row)

            df = pd.DataFrame(validated)
            # ã‚«ãƒ©ãƒ é †ã‚’ãƒ¢ãƒ‡ãƒ«å®šç¾©ã«åˆã‚ã›ã‚‹
            df = df[expected_cols]

        # 3. ãƒã‚¹ã‚¿ã®å ´åˆã€codeã‚’ç¢ºå®Ÿã«æ–‡å­—åˆ—åŒ–
        if key == "master" and "code" in df.columns:
            df["code"] = df["code"].astype(str).str.strip()

        # 4. Objectå‹ã®å®‰å®šåŒ– (None ã‚’ä¿æŒã—ã¤ã¤æ–‡å­—åˆ—åŒ–)
        for col in df.columns:
            if df[col].dtype == "object":
                df[col] = df[col].apply(lambda x: str(x) if (x is not None and not pd.isna(x)) else None)

        return df

    def add_commit_operation(self, repo_path: str, local_path: Path):
        """ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«æ“ä½œã‚’è¿½åŠ ï¼ˆé‡è¤‡ã¯æœ€æ–°ã§ä¸Šæ›¸ãï¼‰"""
        self._commit_operations[repo_path] = CommitOperationAdd(path_in_repo=repo_path, path_or_fileobj=str(local_path))
        logger.debug(f"ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {repo_path}")

    def _load_parquet(self, key: str) -> pd.DataFrame:
        filename = self.paths[key]
        try:
            local_path = hf_hub_download(
                repo_id=self.hf_repo, filename=filename, repo_type="dataset", token=self.hf_token
            )
            df = pd.read_parquet(local_path)
            # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘èª­ã¿è¾¼ã¿ç›´å¾Œã«ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
            df = self._clean_dataframe(key, df)
            logger.debug(f"ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {filename} ({len(df)} rows)")
            return df
        except RepositoryNotFoundError:
            logger.error(f"âŒ ãƒªãƒã‚¸ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.hf_repo}")
            logger.error("ç’°å¢ƒå¤‰æ•° HF_REPO ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            raise
        except EntryNotFoundError:
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚æ–°è¦ä½œæˆã—ã¾ã™: {filename}")
            if key == "catalog":
                cols = list(CatalogRecord.model_fields.keys())
                return pd.DataFrame(columns=cols)
            elif key == "master":
                cols = list(StockMasterRecord.model_fields.keys())
                return pd.DataFrame(columns=cols)
            elif key == "listing":
                return pd.DataFrame(columns=["code", "type", "event_date"])
            elif key == "index":
                return pd.DataFrame(columns=["index_name", "code", "type", "event_date"])
            elif key == "name":
                return pd.DataFrame(columns=["code", "old_name", "new_name", "change_date"])
            return pd.DataFrame()
        except HfHubHTTPError as e:
            logger.error(f"âŒ HF API ã‚¨ãƒ©ãƒ¼ ({e.response.status_code}): {filename}")
            logger.error(f"è©³ç´°: {e}")
            if e.response.status_code == 401:
                logger.error("èªè¨¼ã‚¨ãƒ©ãƒ¼: HF_TOKEN ãŒç„¡åŠ¹ã¾ãŸã¯æœŸé™åˆ‡ã‚Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            elif e.response.status_code == 403:
                logger.error("ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦: ãƒªãƒã‚¸ãƒˆãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“")
            raise
        except Exception as e:
            logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {filename} - {type(e).__name__}: {e}")
            raise

    def is_processed(self, doc_id: str) -> bool:
        if self.catalog_df.empty:
            return False
        return doc_id in self.catalog_df["doc_id"].values

    def update_catalog(self, new_records: List[Dict]) -> bool:
        """ã‚«ã‚¿ãƒ­ã‚°ã‚’æ›´æ–° (Pydanticãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿæ–½)"""
        if not new_records:
            return True

        validated = []
        for rec in new_records:
            try:
                validated.append(CatalogRecord(**rec).model_dump())
            except Exception as e:
                logger.error(f"ã‚«ã‚¿ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•— (doc_id: {rec.get('doc_id')}): {e}")

        if not validated:
            return False

        new_df = pd.DataFrame(validated)

        # ã€ä¿®æ­£ã€‘ä¸€æ™‚çš„ã«çµåˆã—ãŸDataFrameã‚’ä½œæˆï¼ˆãƒ¡ãƒ¢ãƒªä¸Šã®çŠ¶æ…‹ã¯å¤‰æ›´ã—ãªã„ï¼‰
        temp_catalog = pd.concat([self.catalog_df, new_df], ignore_index=True).drop_duplicates(
            subset=["doc_id"], keep="last"
        )

        # ã€ä¿®æ­£ã€‘ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸæ™‚ã®ã¿ã€ãƒ¡ãƒ¢ãƒªä¸Šã®ã‚«ã‚¿ãƒ­ã‚°ã‚’æ›´æ–°
        if self._save_and_upload("catalog", temp_catalog):
            self.catalog_df = temp_catalog
            logger.success(f"âœ… ã‚«ã‚¿ãƒ­ã‚°æ›´æ–°æˆåŠŸ: {len(validated)} ä»¶")
            return True
        else:
            logger.error("âŒ ã‚«ã‚¿ãƒ­ã‚°ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ¡ãƒ¢ãƒªä¸Šã®çŠ¶æ…‹ã‚’ä¿æŒã—ã¾ã™")
            return False

    def _save_and_upload(self, key: str, df: pd.DataFrame, defer: bool = False) -> bool:
        filename = self.paths[key]
        local_file = self.data_path / Path(filename).name

        # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘ä¿å­˜ç›´å‰ã«æœ€çµ‚ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        df = self._clean_dataframe(key, df)

        df.to_parquet(local_file, index=False, compression="zstd")

        if self.api:
            if defer:
                # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ ã—ã¦çµ‚äº† (ãƒ‘ã‚¹ã‚’ã‚­ãƒ¼ã«ã—ã¦æœ€æ–°ã®ã‚‚ã®ã§ä¸Šæ›¸ã)
                self.add_commit_operation(filename, local_file)
                return True

            max_retries = 5  # å¼·åŒ–
            for attempt in range(max_retries):
                try:
                    self.api.upload_file(
                        path_or_fileobj=str(local_file),
                        path_in_repo=filename,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.success(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {filename}")
                    return True
                except Exception as e:
                    # HfHubHTTPErrorã®å‹ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã€429ã®å ´åˆã®ã¿ãƒªãƒˆãƒ©ã‚¤
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(f"Rate limit exceeded. Waiting {wait_time}s before retry ({attempt + 1}/5)...")
                        time.sleep(wait_time)
                        continue

                    # ãã®ä»–ã®HTTPã‚¨ãƒ©ãƒ¼ (5xxç­‰) ã‚‚ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã«ã™ã‚‹
                    if isinstance(e, HfHubHTTPError) and e.response.status_code >= 500:
                        wait_time = 15 * (attempt + 1)
                        logger.warning(
                            f"Master HF Server Error ({e.response.status_code}). "
                            f"Waiting {wait_time}s... ({attempt + 1}/5)"
                        )
                        time.sleep(wait_time)
                        continue

                    logger.warning(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {filename} - {e} - Retrying ({attempt + 1}/5)...")
                    time.sleep(10 * (attempt + 1))
            return False
        return True

    def upload_raw(self, local_path: Path, repo_path: str, defer: bool = False) -> bool:
        """ãƒ­ãƒ¼ã‚«ãƒ«ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ Hugging Face ã® raw/ ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        if not local_path.exists():
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“: {local_path}")
            return False

        if self.api:
            if defer:
                self.add_commit_operation(repo_path, local_path)
                logger.debug(f"RAWã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {repo_path}")
                return True

            max_retries = 5  # å¼·åŒ–
            for attempt in range(max_retries):
                try:
                    self.api.upload_file(
                        path_or_fileobj=str(local_path),
                        path_in_repo=repo_path,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.debug(f"RAWã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {repo_path}")
                    return True
                except Exception as e:
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(f"Rate limit exceeded for RAW. Waiting {wait_time}s... ({attempt + 1}/5)")
                        time.sleep(wait_time)
                        continue

                    logger.warning(f"RAWã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {repo_path} - {e} - Retrying ({attempt + 1}/5)...")
                    time.sleep(10 * (attempt + 1))
            return False
        return True

    def upload_raw_folder(self, folder_path: Path, path_in_repo: str, defer: bool = False) -> bool:
        """ãƒ•ã‚©ãƒ«ãƒ€å˜ä½ã§ã®ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ãƒªãƒˆãƒ©ã‚¤ä»˜)"""
        if not folder_path.exists():
            return True  # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãªã—ã¯æˆåŠŸã¨ã¿ãªã™

        if self.api:
            if defer:
                # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                for f in folder_path.glob("**/*"):
                    if f.is_file():
                        r_path = f"{path_in_repo}/{f.relative_to(folder_path)}"
                        self._commit_operations[r_path] = CommitOperationAdd(
                            path_in_repo=r_path, path_or_fileobj=str(f)
                        )
                logger.debug(f"RAWãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚³ãƒŸãƒƒãƒˆãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ : {path_in_repo}")
                return True

            max_retries = 5  # 3å›ã‹ã‚‰5å›ã«å¼·åŒ–
            for attempt in range(max_retries):
                try:
                    self.api.upload_folder(
                        folder_path=str(folder_path),
                        path_in_repo=path_in_repo,
                        repo_id=self.hf_repo,
                        repo_type="dataset",
                        token=self.hf_token,
                    )
                    logger.success(f"ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {path_in_repo} (from {folder_path})")
                    return True
                except Exception as e:
                    if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                        wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                        logger.warning(
                            f"Folder Upload Rate limit exceeded. Waiting {wait_time}s... ({attempt + 1}/{max_retries})"
                        )
                        time.sleep(wait_time)
                        continue

                    logger.warning(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸€æ™‚ã‚¨ãƒ©ãƒ¼: {e} - Retrying ({attempt + 1}/{max_retries})...")
                    time.sleep(10)

            logger.error(f"ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•— (Give up): {path_in_repo}")
            return False
        return True

    def update_listing_history(self, new_events: pd.DataFrame) -> bool:
        history = self._load_parquet("listing")

        # åˆå›å®Ÿè¡Œæ™‚ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã›ãšã€ã‚¤ãƒ™ãƒ³ãƒˆã‚‚ç©ºï¼‰ã®å ´åˆã§ã‚‚ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if new_events.empty:
            if history.empty:
                # ç©ºã®å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ä¿å­˜
                return self._save_and_upload("listing", history)
            return True

        history = pd.concat([history, new_events], ignore_index=True).drop_duplicates()
        return self._save_and_upload("listing", history)

    def update_index_history(self, new_events: pd.DataFrame) -> bool:
        history = self._load_parquet("index")

        # åˆå›å®Ÿè¡Œæ™‚ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã›ãšã€ã‚¤ãƒ™ãƒ³ãƒˆã‚‚ç©ºï¼‰ã®å ´åˆã§ã‚‚ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if new_events.empty:
            if history.empty:
                # ç©ºã®å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ä¿å­˜
                return self._save_and_upload("index", history)
            return True

        history = pd.concat([history, new_events], ignore_index=True).drop_duplicates()
        return self._save_and_upload("index", history)

    def get_listing_history(self) -> pd.DataFrame:
        """ç¾åœ¨ã®ä¸Šå ´å±¥æ­´ãƒã‚¹ã‚¿ã‚’å–å¾—"""
        return self._load_parquet("listing")

    def get_index_history(self) -> pd.DataFrame:
        """ç¾åœ¨ã®æŒ‡æ•°æ¡ç”¨å±¥æ­´ãƒã‚¹ã‚¿ã‚’å–å¾—"""
        return self._load_parquet("index")

    def update_stocks_master(self, new_master: pd.DataFrame):
        """ãƒã‚¹ã‚¿æ›´æ–° (Pydantic ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿæ–½)"""
        if new_master.empty:
            return

        records = new_master.to_dict("records")
        validated = []
        for rec in records:
            try:
                validated.append(StockMasterRecord(**rec).model_dump())
            except Exception as e:
                logger.error(f"éŠ˜æŸ„ãƒã‚¹ã‚¿ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•— (code: {rec.get('code')}): {e}")

        if not validated:
            return
        valid_df = pd.DataFrame(validated)

        # ç¤¾åå¤‰æ›´ãƒã‚§ãƒƒã‚¯
        if not self.master_df.empty:
            merged = pd.merge(
                self.master_df[["code", "company_name"]],
                valid_df[["code", "company_name"]],
                on="code",
                suffixes=("_old", "_new"),
            )
            changed = merged[merged["company_name_old"] != merged["company_name_new"]]
            if not changed.empty:
                today = datetime.now().strftime("%Y-%m-%d")
                name_history = self._load_parquet("name")
                for _, row in changed.iterrows():
                    name_history = pd.concat(
                        [
                            name_history,
                            pd.DataFrame(
                                [
                                    {
                                        "code": row["code"],
                                        "old_name": row["company_name_old"],
                                        "new_name": row["company_name_new"],
                                        "change_date": today,
                                    }
                                ]
                            ),
                        ],
                        ignore_index=True,
                    )
                self._save_and_upload("name", name_history.drop_duplicates())

        self.master_df = valid_df
        return self._save_and_upload("master", self.master_df)  # ã€ä¿®æ­£ã€‘æˆ»ã‚Šå€¤ã‚’è¿”ã™

    def get_last_index_list(self, index_name: str) -> pd.DataFrame:
        """æŒ‡å®šæŒ‡æ•°ã®æ§‹æˆéŠ˜æŸ„ã‚’å–å¾— (Phase 3ç”¨)"""
        return pd.DataFrame(columns=["code"])

    def get_sector(self, code: str) -> str:
        """è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æ¥­ç¨®å–å¾—"""
        if self.master_df.empty:
            return "ãã®ä»–"
        row = self.master_df[self.master_df["code"] == code]
        if not row.empty:
            return str(row.iloc[0]["sector"])
        return "ãã®ä»–"

    def save_delta(
        self, key: str, df: pd.DataFrame, run_id: str, chunk_id: str, custom_filename: str = None, defer: bool = False
    ) -> bool:
        """ãƒ‡ãƒ«ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (Workerç”¨)"""
        if df.empty:
            return True

        if custom_filename:
            filename = custom_filename
        else:
            filename = f"{Path(self.paths[key]).stem}.parquet"

        delta_path = f"temp/deltas/{run_id}/{chunk_id}/{filename}"
        local_file = self.data_path / f"delta_{run_id}_{chunk_id}_{filename}"

        # ã€çµ¶å¯¾ã‚¬ãƒ¼ãƒ‰ã€‘ä¿å­˜ç›´å‰ã«æœ€çµ‚ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        df = self._clean_dataframe(key, df)

        df.to_parquet(local_file, index=False, compression="zstd")

        return self.upload_raw(local_file, delta_path, defer=defer)

    def mark_chunk_success(self, run_id: str, chunk_id: str, defer: bool = False) -> bool:
        """ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æˆåŠŸãƒ•ãƒ©ã‚° (_SUCCESS) ã‚’ä½œæˆ (Workerç”¨)"""
        success_path = f"temp/deltas/{run_id}/{chunk_id}/_SUCCESS"
        local_file = self.data_path / f"SUCCESS_{run_id}_{chunk_id}"
        local_file.touch()

        return self.upload_raw(local_file, success_path, defer=defer)

    def load_deltas(self, run_id: str) -> Dict[str, pd.DataFrame]:
        """å…¨ãƒ‡ãƒ«ã‚¿ã‚’åé›†ã—ã¦ãƒãƒ¼ã‚¸ (Mergerç”¨)"""
        if not self.api:
            logger.warning("APIåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ãƒ‡ãƒ«ã‚¿åé›†ä¸å¯")
            return {}

        deltas = {}

        try:
            # ã€æ•´åˆæ€§å¼·åŒ–ã€‘HF Hub ã®ãƒªã‚¹ãƒˆå–å¾—è‡ªä½“ã‚’ãƒªãƒˆãƒ©ã‚¤ã—ã€åæ˜ é…å»¶ã«å¯¾å‡¦
            folder = f"temp/deltas/{run_id}"
            files = []
            for attempt in range(3):
                files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type="dataset")
                target_files = [f for f in files if f.startswith(folder)]
                if target_files:
                    break
                logger.warning(f"ãƒ‡ãƒ«ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å†è©¦è¡Œä¸­... ({attempt + 1}/3)")
                time.sleep(10)

            # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            chunks = {}
            for f in target_files:
                parts = f.split("/")
                if len(parts) < 4:
                    continue
                chunk_id = parts[3]
                if chunk_id not in chunks:
                    chunks[chunk_id] = []
                chunks[chunk_id].append(f)

            # _SUCCESS ãŒã‚ã‚‹ãƒãƒ£ãƒ³ã‚¯ã®ã¿å‡¦ç†
            valid_chunks = 0
            for chunk_id, file_list in chunks.items():
                if not any(f.endswith("_SUCCESS") for f in file_list):
                    # ã€æ•´åˆæ€§å¼·åŒ–ã€‘HF Hubã®çµæœæ•´åˆæ€§ã‚’è€ƒæ…®ã—ã€1å›è¦‹ã¤ã‹ã‚‰ãªãã¦ã‚‚
                    # åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—ã‚’è©¦ã¿ã‚‹ã“ã¨ãŒæœ›ã¾ã—ã„ãŒã€ã“ã“ã§ã¯ä¸€æ—¦è­¦å‘Šã«ç•™ã‚ã‚‹
                    logger.warning(f"âš ï¸ æœªå®Œäº†ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—: {chunk_id}")
                    continue

                valid_chunks += 1
                for remote_path in file_list:
                    if remote_path.endswith("_SUCCESS"):
                        continue

                    # ã‚­ãƒ¼åˆ¤åˆ¥
                    fname = Path(remote_path).name
                    key = None
                    if fname == "documents_index.parquet":
                        key = "catalog"
                    elif fname == "stocks_master.parquet":
                        key = "master"
                    elif fname == "listing_history.parquet":
                        key = "listing"
                    elif fname == "index_history.parquet":
                        key = "index"
                    elif fname == "name_history.parquet":
                        key = "name"
                    elif fname.startswith("financial_values_bin"):
                        bin_id = fname.replace("financial_values_bin", "").replace(".parquet", "")
                        key = f"financial_bin{bin_id}"
                    elif fname.startswith("qualitative_text_bin"):
                        bin_id = fname.replace("qualitative_text_bin", "").replace(".parquet", "")
                        key = f"text_bin{bin_id}"
                    elif fname.startswith("financial_values_"):
                        sector = fname.replace("financial_values_", "").replace(".parquet", "")
                        key = f"financial_{sector}"
                    elif fname.startswith("qualitative_text_"):
                        sector = fname.replace("qualitative_text_", "").replace(".parquet", "")
                        key = f"text_{sector}"

                    if key:
                        attempts = 2
                        for att in range(attempts):
                            try:
                                local_path = hf_hub_download(
                                    repo_id=self.hf_repo, filename=remote_path, repo_type="dataset", token=self.hf_token
                                )
                                df = pd.read_parquet(local_path)
                                # ã€é‡è¦ã€‘ãƒ‡ãƒ«ã‚¿èª­ã¿è¾¼ã¿æ™‚ã« rec ã‚«ãƒ©ãƒ ã‚’æ’é™¤
                                if "rec" in df.columns:
                                    df = df.drop(columns=["rec"])
                                if key not in deltas:
                                    deltas[key] = []
                                deltas[key].append(df)
                                break
                            except Exception as e:
                                if att == attempts - 1:
                                    logger.error(f"âŒ ãƒ‡ãƒ«ã‚¿èª­ã¿è¾¼ã¿å¤±æ•— ({remote_path}): {e}")
                                    raise
                                logger.warning(f"ãƒ‡ãƒ«ã‚¿èª­ã¿è¾¼ã¿å†è©¦è¡Œä¸­... ({att + 1}) {remote_path}")
                                time.sleep(5)

            logger.info(f"æœ‰åŠ¹ãªãƒãƒ£ãƒ³ã‚¯æ•°: {valid_chunks} / {len(chunks)}")

            # ãƒãƒ¼ã‚¸çµæœã‚’è¿”ã™
            merged = {}
            for key, df_list in deltas.items():
                if df_list:
                    # å…¨ã¦ã®DFã®ã‚«ãƒ©ãƒ ã‚’å…±é€šåŒ–ï¼ˆå‹ä¸æ•´åˆå¯¾ç­–ï¼‰
                    merged[key] = pd.concat(df_list, ignore_index=True)
                else:
                    merged[key] = pd.DataFrame()
            return merged

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ«ã‚¿åé›†å¤±æ•—: {e}")
            return {}

    def push_commit(self, message: str = "Batch update from ARIA") -> bool:
        """ãƒãƒƒãƒ•ã‚¡ã«æºœã¾ã£ãŸæ“ä½œã‚’ä¸€æ‹¬ã§ã‚³ãƒŸãƒƒãƒˆå®Ÿè¡Œ"""
        if not self.api or not self._commit_operations:
            return True

        max_retries = 8  # ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’å¢—ã‚„ã—ã¦ç«¶åˆã«å‚™ãˆã‚‹
        ops_list = list(self._commit_operations.values())

        for attempt in range(max_retries):
            try:
                self.api.create_commit(
                    repo_id=self.hf_repo,
                    repo_type="dataset",
                    operations=ops_list,
                    commit_message=message,
                    token=self.hf_token,
                )
                logger.success(f"âœ… ãƒãƒƒãƒã‚³ãƒŸãƒƒãƒˆæˆåŠŸ: {len(ops_list)} æ“ä½œ")
                self._commit_operations = {}  # ã‚¯ãƒªã‚¢
                return True
            except Exception as e:
                # 429 ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                if isinstance(e, HfHubHTTPError) and e.response.status_code == 429:
                    wait_time = int(e.response.headers.get("Retry-After", 60)) + 5
                    logger.warning(f"Commit Rate limit exceeded. Waiting {wait_time}s... ({attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue

                # 409 ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ ã¾ãŸã¯ 412 å‰ææ¡ä»¶å¤±æ•— (ä»–ã®ã‚¸ãƒ§ãƒ–ãŒåŒæ™‚ã«ã‚³ãƒŸãƒƒãƒˆã—ãŸ)
                if isinstance(e, HfHubHTTPError) and e.response.status_code in [409, 412]:
                    # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ• + ã‚¸ãƒƒã‚¿ãƒ¼
                    wait_time = (2**attempt) + (random.uniform(5, 15))
                    logger.warning(
                        f"Commit Conflict ({e.response.status_code}). "
                        f"Retrying in {wait_time:.2f}s... ({attempt + 1}/{max_retries})"
                    )
                    time.sleep(wait_time)
                    continue

                logger.warning(f"ã‚³ãƒŸãƒƒãƒˆå¤±æ•—: {e} - Retrying ({attempt + 1}/{max_retries})...")
                time.sleep(10 * (attempt + 1))

        logger.error("âŒ ãƒãƒƒãƒã‚³ãƒŸãƒƒãƒˆã«æœ€çµ‚çš„ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return False

    def cleanup_deltas(self, run_id: str, cleanup_old: bool = True):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (Mergerç”¨)"""
        if not self.api:
            return

        try:
            files = self.api.list_repo_files(repo_id=self.hf_repo, repo_type="dataset")
            delta_root = "temp/deltas"

            # å¤ã„ãƒ•ã‚©ãƒ«ãƒ€ã®å‰Šé™¤ (24æ™‚é–“ä»¥ä¸ŠçµŒéã—ãŸã‚‚ã®ã‚’å¯¾è±¡ã¨ã™ã‚‹)
            if cleanup_old:
                now = time.time()
                delete_ops = []
                expired_runs = set()

                for f in files:
                    if not f.startswith(delta_root):
                        continue
                    parts = f.split("/")
                    if len(parts) < 3:
                        continue
                    r_id = parts[2]

                    # run_id ãŒæ•°å€¤ï¼ˆtimestampï¼‰ã§ã‚ã‚‹å‰æã§å¤ã„ã‚‚ã®ã‚’åˆ¤å®š
                    try:
                        timestamp = int(r_id)
                        if (now - timestamp) > 86400:  # 24æ™‚é–“ä»¥ä¸Š
                            delete_ops.append(f)
                            expired_runs.add(r_id)
                    except ValueError:
                        # æ•°å€¤ã§ãªã„ãƒ•ã‚©ãƒ«ãƒ€ã¯ç„¡è¦–ã™ã‚‹ã‹ã€åˆ¥ã®åŸºæº–ã§æ¶ˆã™
                        pass

                if delete_ops:
                    logger.info(f"å¤ã„ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¸…æƒä¸­... (24æ™‚é–“ä»¥ä¸ŠçµŒé: {len(expired_runs)} runs)")
                    for i in range(0, len(delete_ops), 50):
                        batch = delete_ops[i : i + 50]
                        # å‰Šé™¤æ“ä½œã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                        del_ops = [CommitOperationDelete(path_in_repo=p) for p in batch]
                        self.api.create_commit(
                            repo_id=self.hf_repo,
                            repo_type="dataset",
                            operations=del_ops,
                            commit_message="Automatic garbage collection of old deltas",
                        )

            # ä»Šå›ã®ãƒ•ã‚©ãƒ«ãƒ€å‰Šé™¤ï¼ˆå…¨å®Œäº†å¾Œç”¨ï¼‰
            else:
                target_prefix = f"{delta_root}/{run_id}"
                delete_ops = [f for f in files if f.startswith(target_prefix)]

                if delete_ops:
                    logger.info(f"ä»Šå›ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ä¸­... {run_id} ({len(delete_ops)} files)")
                    for i in range(0, len(delete_ops), 50):
                        batch = delete_ops[i : i + 50]
                        del_ops = [CommitOperationDelete(path_in_repo=p) for p in batch]
                        self.api.create_commit(
                            repo_id=self.hf_repo,
                            repo_type="dataset",
                            operations=del_ops,
                            commit_message=f"Cleanup successfully merged deltas: {run_id}",
                        )
                    logger.success(f"Cleanup completed: {run_id}")

        except Exception as e:
            logger.error(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
