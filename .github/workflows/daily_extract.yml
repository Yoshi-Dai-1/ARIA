name: Daily EDINET Extract

on:
  schedule:
    - cron: '0 15 * * *'  # 毎日日本時間 0:00 (UTC 15:00)
  workflow_dispatch:
    inputs:
      start_date:
        description: 'Start Date (YYYY-MM-DD)'
        required: true
        default: '2024-06-01'
      end_date:
        description: 'End Date (YYYY-MM-DD)'
        required: true
        default: '2024-06-30'

jobs:
  discovery:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      start_date: ${{ steps.set-dates.outputs.start_date }}
      end_date: ${{ steps.set-dates.outputs.end_date }}
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      - name: Lint and Format with Ruff
        run: |
          ruff format .
          ruff check --fix .
      - name: Prep and Cache Data Lakehouse Masters
        run: mkdir -p data/meta
      - name: Cache Data Lakehouse Masters
        uses: actions/cache@v4
        with:
          path: data/meta
          key: ${{ runner.os }}-masters-${{ hashFiles('**/requirements.txt') }}-${{ github.run_id }}
          restore-keys: |
            ${{ runner.os }}-masters-
      - id: set-dates
        run: |
          START_DATE=$(date -d "30 days ago" +%Y-%m-%d)
          END_DATE=$(date +%Y-%m-%d)
          [[ -n "${{ github.event.inputs.start_date }}" ]] && START_DATE="${{ github.event.inputs.start_date }}"
          [[ -n "${{ github.event.inputs.end_date }}" ]] && END_DATE="${{ github.event.inputs.end_date }}"
          echo "start_date=$START_DATE" >> $GITHUB_OUTPUT
          echo "end_date=$END_DATE" >> $GITHUB_OUTPUT
      - name: Update Market Master (to ensure sectors for discovery)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO: ${{ vars.HF_REPO }}
          PYTHONPATH: data_engine
        run: |
          # マトリックス分割に必要な業種情報を最新にするため、Market Pipeline を実行 (Masterのみ)
          python data_engine/market_main.py --mode master

      - id: set-matrix
        env:
          EDINET_API_KEY: ${{ secrets.EDINET_API_KEY }}
          HF_REPO: ${{ vars.HF_REPO }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          EXTRACT_START: ${{ steps.set-dates.outputs.start_date }}
          EXTRACT_END: ${{ steps.set-dates.outputs.end_date }}
          DISABLE_PANDERA_IMPORT_WARNING: True
        run: |
          RAW_JSON=$(python data_engine/main.py --list-only --start "${{ steps.set-dates.outputs.start_date }}" --end "${{ steps.set-dates.outputs.end_date }}" | grep "^JSON_MATRIX_DATA:" | cut -d':' -f2-)
          DATA=${RAW_JSON:-"[]"}

          echo "$DATA" | python -c "
          import json, math, sys
          try:
              data_raw = sys.stdin.read()
              data = json.loads(data_raw)
              if not data:
                  print('matrix=[]')
                  sys.exit(0)

              # 【修正】全書類（824件など）を対象にする。
              # 以前は 'その他' を除外していたため56件の漏れが発生していた。
              filtered_data = data

              if not filtered_data:
                  sys.stderr.write('処理対象の書類が0件でした。\n')
                  print('matrix=[]')
                  sys.exit(0)

              # 【高度化】銘柄単位(code)でグループ化し、XBRL解析の重みを加味して分配
              # 1. 会社ごとの書類リストと重みを計算
              company_map = {}
              for item in filtered_data:
                  code = item.get('code', '9999')
                  company_map.setdefault(code, []).append(item)

              company_tasks = []
              for code, items in company_map.items():
                  # 【再最適化】有報(120)/一般(010)/標準(030000) のみを重い(50)と判定
                  # その他は DLオーバーヘッド(10) + 従量(1)
                  company_weight = 10
                  for x in items:
                      is_parsed = x.get('type') == '120' and x.get('ord') == '010' and x.get('form') == '030000'
                      company_weight += 50 if is_parsed else 1
                  
                  company_tasks.append({
                      'code': code,
                      'ids': [x['id'] for x in items],
                      'weight': company_weight
                  })

              # 2. 重い順にソートして貪欲法(Greedy)で20バケットに分配
              company_tasks.sort(key=lambda x: x['weight'], reverse=True)
              
              n_jobs = 20
              buckets = [[] for _ in range(n_jobs)]
              bucket_weights = [0] * n_jobs

              for task in company_tasks:
                  # 現在最も軽いバケットを選択
                  idx = bucket_weights.index(min(bucket_weights))
                  buckets[idx].extend(task['ids'])
                  bucket_weights[idx] += task['weight']

              res = []
              for i, b in enumerate(buckets):
                  if not b: continue
                  res.append({
                      'ids': ','.join(b),
                      'idx': i,
                      'count': len(b),
                      'weight': bucket_weights[i]
                  })

              sys.stderr.write(f'総書類数: {len(filtered_data)} | 平均重み: {sum(bucket_weights)/n_jobs:.1f}\n')
              for i, r in enumerate(res):
                  sys.stderr.write(f'  Job {i}: weight={r[\"weight\"]}, docs={r[\"count\"]}\n')
              
              print(f'matrix={json.dumps(res)}')
          except Exception as e:
              import traceback
              sys.stderr.write(f'Error: {traceback.format_exc()}\n')
              print('matrix=[]')
          " >> $GITHUB_OUTPUT

      - name: Upload Metadata Artifact
        uses: actions/upload-artifact@v4
        with:
          name: discovery-metadata
          path: data/meta/discovery_metadata.json
          retention-days: 1

  extract:
    needs: discovery
    if: needs.discovery.outputs.matrix != '[]' && needs.discovery.outputs.matrix != ''
    runs-on: ubuntu-latest
    permissions:
      contents: write
    strategy:
      fail-fast: false
      matrix:
        chunk: ${{ fromJson(needs.discovery.outputs.matrix) }}
    timeout-minutes: 360
    name: extract (Job ${{ matrix.chunk.idx }}/20, Weight ${{ matrix.chunk.weight }})
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      - name: Download Metadata Artifact
        uses: actions/download-artifact@v4
        with:
          name: discovery-metadata
          path: data/meta/
      - name: Lint and Format with Ruff
        run: |
          ruff format .
          ruff check --fix .
      - name: Prep and Cache Data Lakehouse Masters
        run: mkdir -p data/meta
      - name: Cache Data Lakehouse Masters
        uses: actions/cache@v4
        with:
          path: data/meta
          key: ${{ runner.os }}-masters-${{ hashFiles('**/requirements.txt') }}-${{ github.run_id }}
          restore-keys: |
            ${{ runner.os }}-masters-
      - name: Run Extraction
        env:
          EDINET_API_KEY: ${{ secrets.EDINET_API_KEY }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO: ${{ vars.HF_REPO }}
        run: |
          # main.py の引数名は --id_list
          # Worker Mode: Run ID と Chunk ID を指定してデルタファイルを作成
          python data_engine/main.py \
            --start "${{ needs.discovery.outputs.start_date }}" \
            --end "${{ needs.discovery.outputs.end_date }}" \
            --id_list "${{ matrix.chunk.ids }}" \
            --mode worker \
            --run-id "${{ github.run_id }}" \
            --chunk-id "${{ matrix.chunk.idx }}"

  finalize:
    needs: extract
    if: always() && !cancelled()
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      - name: Download Metadata Artifact
        uses: actions/download-artifact@v4
        with:
          name: discovery-metadata
          path: data/meta/
      - name: Run Finalize (Merger)
        env:
          EDINET_API_KEY: ${{ secrets.EDINET_API_KEY }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO: ${{ vars.HF_REPO }}
        run: |
          mkdir -p data
          # Merger Mode: 全デルタを集約・マージしてGlobal更新
          python data_engine/main.py --mode merger --run-id "${{ github.run_id }}"
