name: Hourly Historical Backfill (2016-Present)

on:
  schedule:
    # JST 01:15 - 05:15, 07:15 - 23:15
    # JST 06:15 (UTC 21:15) は Daily Indices (JST 06:30) と重なるリスクがあるためスキップ
    - cron: '15 0-14,16-20,22-23 * * *'
  workflow_dispatch:
    inputs:
      debug_mode:
        description: 'Run valid check only'
        type: boolean
        default: false

jobs:
  discovery:
    runs-on: ubuntu-latest
    outputs:
      matrix_primary: ${{ steps.set-matrix.outputs.matrix_primary }}
      matrix_retry: ${{ steps.set-matrix.outputs.matrix_retry }}
      start_date: ${{ steps.set-dates.outputs.start_date }}
      end_date: ${{ steps.set-dates.outputs.end_date }}
      retry_start: ${{ steps.set-dates.outputs.retry_start }}
      retry_end: ${{ steps.set-dates.outputs.retry_end }}
      finished: ${{ steps.set-dates.outputs.finished }}
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Calculate Target Period
        id: set-dates
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO: ${{ vars.HF_REPO }}
          PYTHONPATH: data_engine
        run: |
          OUTPUT=$(python data_engine/backfill_manager.py)
          
          if [[ "$OUTPUT" == *"FINISHED"* ]]; then
            echo "Backfill completed or limit reached."
            echo "finished=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          START=$(echo "$OUTPUT" | grep "^START=" | cut -d'=' -f2)
          END=$(echo "$OUTPUT" | grep "^END=" | cut -d'=' -f2)
          RETRY_START=$(echo "$OUTPUT" | grep "^RETRY_START=" | cut -d'=' -f2 || echo "")
          RETRY_END=$(echo "$OUTPUT" | grep "^RETRY_END=" | cut -d'=' -f2 || echo "")
          
          echo "Target Period: $START ~ $END"
          echo "Retry Period: $RETRY_START ~ $RETRY_END"
          echo "start_date=$START" >> $GITHUB_OUTPUT
          echo "end_date=$END" >> $GITHUB_OUTPUT
          echo "retry_start=$RETRY_START" >> $GITHUB_OUTPUT
          echo "retry_end=$RETRY_END" >> $GITHUB_OUTPUT
          echo "finished=false" >> $GITHUB_OUTPUT

      - name: Generate Matrix
        id: set-matrix
        if: steps.set-dates.outputs.finished == 'false'
        env:
          EDINET_API_KEY: ${{ secrets.EDINET_API_KEY }}
          HF_REPO: ${{ vars.HF_REPO }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          PYTHONPATH: data_engine
        run: |
          # 1. 通常取得期間のリストアップ (Artifact/Cache保存)
          python data_engine/main.py --list-only --start "${{ steps.set-dates.outputs.start_date }}" --end "${{ steps.set-dates.outputs.end_date }}" | grep "^JSON_MATRIX_DATA:" | cut -d':' -f2- > data/meta/matrix1.json
          mv data/meta/discovery_metadata.json data/meta/disc1.json

          # 2. 再試行期間 (過去2ヶ月) のリストアップ (Artifact/Cache保存)
          RETRY_START="${{ steps.set-dates.outputs.retry_start }}"
          if [[ -n "$RETRY_START" ]]; then
            python data_engine/main.py --list-only --start "$RETRY_START" --end "${{ steps.set-dates.outputs.retry_end }}" | grep "^JSON_MATRIX_DATA:" | cut -d':' -f2- > data/meta/matrix2.json
            mv data/meta/discovery_metadata.json data/meta/disc2.json
          else
            echo "[]" > data/meta/matrix2.json
            echo "[]" > data/meta/disc2.json
          fi

          # 3. マージとマトリックス生成 & メタデータキャッシュの統合
          python -c "
          import json, sys, os
          try:
              with open('data/meta/matrix1.json', 'r') as f1, open('data/meta/matrix2.json', 'r') as f2:
                  data1 = json.load(f1)
                  data2 = json.load(f2)
              
              # 処理対象件数の可視化
              sys.stderr.write(f'--- Document Count Summary ---\n')
              sys.stderr.write(f'  Primary Period: {len(data1)} docs\n')
              sys.stderr.write(f'  Retry Period  : {len(data2)} docs\n')
              sys.stderr.write(f'------------------------------\n')

              # メタデータキャッシュの統合 (Drift防止)
              with open('data/meta/disc1.json', 'r') as f1, open('data/meta/disc2.json', 'r') as f2:
                  m1 = json.load(f1)
                  m2 = json.load(f2)
                  with open('data/meta/discovery_metadata.json', 'w') as out:
                      json.dump(m1 + m2, out, ensure_ascii=False, indent=2)

              def build_matrix(data):
                  if not data: return '[]'
                  company_map = {}
                  for item in data:
                      code = item.get('code', '9999')
                      company_map.setdefault(code, []).append(item)
                  company_tasks = []
                  for code, items in company_map.items():
                      weight = 10
                      for x in items:
                          is_yuho = x.get('type') == '120' and x.get('ord') == '010' and x.get('form') == '030000'
                          weight += 50 if is_yuho else 2
                      company_tasks.append({'ids': [x['id'] for x in items], 'weight': weight})
                  company_tasks.sort(key=lambda x: x['weight'], reverse=True)
                  n_jobs = 15
                  buckets = [[] for _ in range(n_jobs)]
                  bucket_weights = [0] * n_jobs
                  for task in company_tasks:
                      idx = bucket_weights.index(min(bucket_weights))
                      buckets[idx].extend(task['ids'])
                      bucket_weights[idx] += task['weight']
                  res = [{'ids': ','.join(b), 'idx': i, 'weight': bucket_weights[i]} for i, b in enumerate(buckets) if b]
                  return json.dumps(res)

              sys.stdout.write(f'matrix_primary={build_matrix(data1)}\n')
              sys.stdout.write(f'matrix_retry={build_matrix(data2)}\n')
          except Exception as e:
              sys.stderr.write(f'Error during matrix generation: {e}\n')
              print('matrix_primary=[]\nmatrix_retry=[]')
          " >> $GITHUB_OUTPUT

      - name: Upload Metadata Artifact
        uses: actions/upload-artifact@v4
        with:
          name: discovery-metadata
          path: data/meta/discovery_metadata.json
          retention-days: 1

  extract-primary:
    needs: discovery
    if: needs.discovery.outputs.matrix_primary != '[]' && needs.discovery.outputs.matrix_primary != ''
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.discovery.outputs.matrix_primary) }}
    timeout-minutes: 60
    name: backfill-P (${{ matrix.idx }})
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      - name: Download Metadata Artifact
        uses: actions/download-artifact@v4
        with:
          name: discovery-metadata
          path: data/meta/
      - name: Run Extraction Worker (Primary)
        env:
          EDINET_API_KEY: ${{ secrets.EDINET_API_KEY }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO: ${{ vars.HF_REPO }}
          PYTHONPATH: data_engine
        run: |
          RUN_ID="backfill-${{ needs.discovery.outputs.start_date }}"
          python data_engine/main.py --mode worker --run-id "$RUN_ID" --chunk-id "primary-${{ matrix.idx }}" --id_list "${{ matrix.ids }}"

  extract-retry:
    needs: discovery
    if: needs.discovery.outputs.matrix_retry != '[]' && needs.discovery.outputs.matrix_retry != ''
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.discovery.outputs.matrix_retry) }}
    timeout-minutes: 60
    name: backfill-R (${{ matrix.idx }})
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      - name: Download Metadata Artifact
        uses: actions/download-artifact@v4
        with:
          name: discovery-metadata
          path: data/meta/
      - name: Run Extraction Worker (Retry)
        continue-on-error: true # 歴史的救済の失敗でワークフロー全体を赤くしない
        env:
          EDINET_API_KEY: ${{ secrets.EDINET_API_KEY }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO: ${{ vars.HF_REPO }}
          PYTHONPATH: data_engine
        run: |
          RUN_ID="backfill-${{ needs.discovery.outputs.start_date }}"
          python data_engine/main.py --mode worker --run-id "$RUN_ID" --chunk-id "retry-${{ matrix.idx }}" --id_list "${{ matrix.ids }}"

  finalize:
    needs: [discovery, extract-primary, extract-retry]
    if: always() && needs.discovery.outputs.finished == 'false' && !cancelled()
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run Merger
        if: needs.discovery.result == 'success'
        env:
          EDINET_API_KEY: ${{ secrets.EDINET_API_KEY }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO: ${{ vars.HF_REPO }}
          PYTHONPATH: data_engine
        run: |
          RUN_ID="backfill-${{ needs.discovery.outputs.start_date }}"
          python data_engine/main.py --mode merger --run-id "$RUN_ID"

      - name: Update Cursor
        # [不退転の進行] 進行分（Primary）が成功または対象なしの場合のみカーソルを進める。
        # Retryジョブの成否は進行を妨げない。
        if: |
          needs.discovery.result == 'success' && 
          (needs.extract-primary.result == 'success' || needs.extract-primary.result == 'skipped') && 
          github.event.inputs.debug_mode != 'true'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO: ${{ vars.HF_REPO }}
          PYTHONPATH: data_engine
        run: |
          python data_engine/backfill_manager.py --update-cursor "${{ needs.discovery.outputs.end_date }}"
